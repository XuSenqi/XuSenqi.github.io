<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Go语言 - 01 工作区和GOPATH</title>
    <url>/2018/08/12/Go%E8%AF%AD%E8%A8%80-01-%E5%B7%A5%E4%BD%9C%E5%8C%BA%E5%92%8CGOPATH/</url>
    <content><![CDATA[<h2 id="Go语言的安装"><a href="#Go语言的安装" class="headerlink" title="Go语言的安装"></a>Go语言的安装</h2><p>参照官网文档 <a href="https://golang.org/doc/install">https://golang.org/doc/install</a><br>可以在命令行中输入命令来验证是否安装成功：</p>
<ul>
<li>go version：查看安装的版本</li>
<li>go env：当前go的配置环境</li>
</ul>
<p>2个环境变量，即 GOROOT、GOPATH和GOBIN:</p>
<ul>
<li>GOROOT：go的安装路径</li>
<li>GOPATH：go的工作路径<span id="more"></span></li>
</ul>
<h2 id="问题-：设置-GOPATH-有什么意义？"><a href="#问题-：设置-GOPATH-有什么意义？" class="headerlink" title="问题 ：设置 GOPATH 有什么意义？"></a>问题 ：设置 GOPATH 有什么意义？</h2><p>典型回答<br>环境变量 GOPATH 的值可以是一个目录的路径，也可以包含多个目录路径，每个目录都代表 Go 语言的一个工作区（workspace）。<br>这些工作区用于放置 Go 语言的源码文件，以及安装（install）后的归档文件和可执行文件。<br>一个典型的workspace如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bin/</span><br><span class="line">    hello                 # command executable </span><br><span class="line">pkg/</span><br><span class="line">    linux_amd64/          # this will reflect your OS and architecture</span><br><span class="line">        github.com/user/</span><br><span class="line">            stringutil.a  # package object</span><br><span class="line">src/</span><br><span class="line">    github.com/user/</span><br><span class="line">        hello/</span><br><span class="line">            hello.go      # command source</span><br><span class="line">        stringutil/</span><br><span class="line">            reverse.go    # package source</span><br></pre></td></tr></table></figure>

<h2 id="知识扩展"><a href="#知识扩展" class="headerlink" title="知识扩展"></a>知识扩展</h2><h3 id="Go-语言源码的组织方式"><a href="#Go-语言源码的组织方式" class="headerlink" title="Go 语言源码的组织方式"></a>Go 语言源码的组织方式</h3><p>以代码包(package)为基本组织单位的。代码包其实是与目录一一对应的。目录可以有子目录，所以代码包也可以有子包。</p>
<p>一个代码包中可以包含任意个以.go 为扩展名的源码文件，这些源码文件都需要被声明为属于同一个代码包。代码包的名称一般会与这些源码文件所在的目录同名。如果不同名，那么在构建、安装的过程中会以代码包名称为准。</p>
<p>代码包的导入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import &quot;github.com/labstack/echo&quot;</span><br></pre></td></tr></table></figure>

<p>在工作区中，一个代码包的导入路径实际上就是从 src 子目录，到该包的实际存储位置的相对路径。下面我马上就会谈到 src子目录。<br>插播：Go语言的源码文件有三大类，即：命令源码文件、库源码文件和测试源码文件。</p>
<h3 id="构建-go-build-和安装-go-install"><a href="#构建-go-build-和安装-go-install" class="headerlink" title="构建(go build)和安装(go install)"></a>构建(go build)和安装(go install)</h3><p>go build: 用于编译我们指定的源码文件或代码包以及它们的依赖包。<br>对于只包含库源码文件的代码包, go build命令在编译时，只会做检查性的编译，操作的结果文件只会存在于临时目录中，立刻自动删除，而不会输出任何结果文件。<br>如果想保留在临时工作目录，不删除掉：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  stringutil go build -work</span><br><span class="line">WORK=/var/folders/5s/25ck5pv16rg55hr2kfdvtszc0000gp/T/go-build412712988</span><br></pre></td></tr></table></figure>
<p>如果构建的是命令源码文件，那么操作的结果文件会被搬运到那个源码文件所在的目录中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  hello git:(master) ✗ ls</span><br><span class="line">hello.go</span><br><span class="line">➜  hello git:(master) ✗ go build</span><br><span class="line">➜  hello git:(master) ✗ ls</span><br><span class="line">hello    hello.go</span><br></pre></td></tr></table></figure>
<p>go install: 安装操作会先执行构建(go build)，然后还会进行链接操作，并且把结果文件搬运到指定目录。<br>进一步说，如果安装的是库源码文件，那么结果文件会被搬运到它所在工作区的 pkg 目录下的某个子目录中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  XuSenqi ls /Users/patrick.xu/go/pkg/darwin_amd64/github.com/XuSenqi</span><br><span class="line">➜  XuSenqi go install github.com/XuSenqi/stringutil</span><br><span class="line">➜  XuSenqi ls /Users/patrick.xu/go/pkg/darwin_amd64/github.com/XuSenqi</span><br><span class="line">stringutil.a</span><br></pre></td></tr></table></figure>

<p>如果安装的是命令源码文件，那么结果文件会被搬运到它所在工作区的 bin 目录中，或者环境变量GOBIN指向的目录中。同时，依赖的代码包也会安装到pgk下对应的目录中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  XuSenqi ls /Users/patrick.xu/go/pkg/darwin_amd64/github.com/XuSenqi</span><br><span class="line">➜  XuSenqi ls /Users/patrick.xu/go/bin/hello</span><br><span class="line">ls: /Users/patrick.xu/go/bin/hello: No such file or directory</span><br><span class="line">➜  XuSenqi go install github.com/XuSenqi/hello/</span><br><span class="line">➜  XuSenqi ls /Users/patrick.xu/go/pkg/darwin_amd64/github.com/XuSenqi</span><br><span class="line">stringutil.a</span><br><span class="line">➜  XuSenqi ls /Users/patrick.xu/go/bin/hello</span><br><span class="line">/Users/patrick.xu/go/bin/hello</span><br></pre></td></tr></table></figure>
<h3 id="go-build-命令一些可选项的用途和用法"><a href="#go-build-命令一些可选项的用途和用法" class="headerlink" title="go build 命令一些可选项的用途和用法"></a>go build 命令一些可选项的用途和用法</h3><p>在运行go build命令的时候，默认不会编译目标代码包所依赖的那些代码包。当然，如果被依赖的代码包的归档文件不存在，或者源码文件有了变化，那它还是会被编译。</p>
<p>如果要强制编译它们，可以在执行命令的时候加入标记-a。此时，不但目标代码包总是会被编译，它依赖的代码包也总会被编译，即使依赖的是标准库中的代码包也是如此。</p>
<p>表1 go build命令的常用标记说明</p>
<table>
<thead>
<tr>
<th>标记名称</th>
<th align="center">标记描述</th>
</tr>
</thead>
<tbody><tr>
<td>-a</td>
<td align="center">强行对所有涉及到的代码包（包含标准库中的代码包）进行重新构建，即使它们已经是最新的了。</td>
</tr>
<tr>
<td>-n</td>
<td align="center">打印编译期间所用到的其它命令，但是并不真正执行它们。</td>
</tr>
<tr>
<td>-p n</td>
<td align="center">指定编译过程中执行各任务的并行数量（确切地说应该是并发数量）。在默认情况下，该数量等于CPU的逻辑核数。但是在darwin/arm平台（即iPhone和iPad所用的平台）下，该数量默认是1。</td>
</tr>
<tr>
<td>-race</td>
<td align="center">开启竞态条件的检测。不过此标记目前仅在linux/amd64、freebsd/amd64、darwin/amd64和windows/amd64平台下受到支持。</td>
</tr>
<tr>
<td>-v</td>
<td align="center">打印出那些被编译的代码包的名字。</td>
</tr>
<tr>
<td>-work</td>
<td align="center">打印出编译时生成的临时工作目录的路径，并在编译结束时保留它。在默认情况下，编译结束时会删除该目录。</td>
</tr>
<tr>
<td>-x</td>
<td align="center">打印编译期间所用到的其它命令。注意它与-n标记的区别。</td>
</tr>
</tbody></table>
<h3 id="go-get"><a href="#go-get" class="headerlink" title="go get"></a>go get</h3><p>go get可以根据要求和实际情况从互联网上下载或更新指定的代码包及其依赖包，并对它们进行go build和go install。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  XuSenqi go get github.com/hyper-carrot/go_lib/logging</span><br><span class="line">➜  XuSenqi ls $GOPATH/src/github.com/hyper-carrot/go_lib/logging</span><br><span class="line">base.go           console_logger.go log_manager.go    logger_test.go    tag.go</span><br><span class="line">➜  XuSenqi ls /Users/patrick.xu/go/pkg/darwin_amd64/github.com/hyper-carrot/go_lib/logging.a</span><br><span class="line">/Users/patrick.xu/go/pkg/darwin_amd64/github.com/hyper-carrot/go_lib/logging.a</span><br></pre></td></tr></table></figure>
<p>表2 go get命令的常用标记说明</p>
<table>
<thead>
<tr>
<th>标记名称</th>
<th align="center">标记描述</th>
</tr>
</thead>
<tbody><tr>
<td>-d</td>
<td align="center">让命令程序只执行下载动作，而不执行安装动作。</td>
</tr>
<tr>
<td>-u</td>
<td align="center">让命令利用网络来更新已有代码包及其依赖包。默认情况下，该命令只会从网络上下载本地不存在的代码包，而不会更新已有的代码包。</td>
</tr>
</tbody></table>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>go build<br><a href="http://wiki.jikexueyuan.com/project/go-command-tutorial/0.1.html">http://wiki.jikexueyuan.com/project/go-command-tutorial/0.1.html</a><br>go get<br><a href="http://wiki.jikexueyuan.com/project/go-command-tutorial/0.3.html">http://wiki.jikexueyuan.com/project/go-command-tutorial/0.3.html</a><br><a href="https://github.com/hyper0x/go_command_tutorial/blob/master/0.3.md">https://github.com/hyper0x/go_command_tutorial/blob/master/0.3.md</a></p>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>c++ profile的大杀器-gperftools的使用</title>
    <url>/2020/12/06/C++Profile%E7%9A%84%E5%A4%A7%E6%9D%80%E5%99%A8_gperftools%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>熟悉golang的同学，一定很熟悉用pprof来作为性能分析和可视化的工具，包括 cpu profile, memery profile等。这么方便且炫的功能，在C++里也一样能实现。所需要的工具就是gperftools。</p>
<span id="more"></span>

<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="安装libunwind"><a href="#安装libunwind" class="headerlink" title="安装libunwind"></a>安装libunwind</h2><p>64位操作系统需要安装libunwind，gperftools推荐版本是libunwind-0.99-beta，详见gperftools/INSTALL里的说明。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget http://download.savannah.gnu.org/releases/libunwind/libunwind-0.99-beta.tar.gz</span><br><span class="line">tar -zxvf libunwind-0.99-beta.tar.gz</span><br><span class="line">cd libunwind-0.99-beta/</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>

<p>因为默认的libunwind安装在/usr/local/lib目录下，需要将这个目录添加到系统动态库缓存中。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">echo &quot;/usr/local/lib&quot; &gt; /etc/ld.so.conf.d/usr_local_lib.conf</span><br><span class="line">/sbin/ldconfig</span><br></pre></td></tr></table></figure>

<h2 id="安装graphviz"><a href="#安装graphviz" class="headerlink" title="安装graphviz"></a>安装graphviz</h2><p>Graphviz是一个由AT&amp;T实验室启动的开源工具包，用于绘制DOT语言脚本描述的图形，gperftools依靠此工具生成图形分析结果。<br />安装命令：yum install graphviz</p>
<p>生成图像时依赖ps2pdf<br />安装命令：yum -y install ghostscript<br /></p>
<h2 id="安装perftools"><a href="#安装perftools" class="headerlink" title="安装perftools"></a>安装perftools</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/gperftools/gperftools.git</span><br><span class="line">cd gperftools/</span><br><span class="line">./autogen.sh</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>

<p>遇到的问题1：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-152-53 gperftools]# ./autogen.sh</span><br><span class="line">configure.ac:174: error: possibly undefined macro: AC_PROG_LIBTOOL</span><br><span class="line">      If this token and others are legitimate, please use m4_pattern_allow.</span><br><span class="line">      See the Autoconf documentation.</span><br><span class="line">      autoreconf: /usr/bin/autoconf failed with exit status: </span><br></pre></td></tr></table></figure>

<p>解决方法：yum -y install libtool</p>
<p>遇到的问题2：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hb06-ufile-132-199 gperftools]# ./autogen.sh</span><br><span class="line">libtoolize: putting macros in AC_CONFIG_MACRO_DIR, `m4&#x27;.</span><br><span class="line">libtoolize: copying file `m4/libtool.m4&#x27;</span><br><span class="line">libtoolize: copying file `m4/ltoptions.m4&#x27;</span><br><span class="line">libtoolize: copying file `m4/ltsugar.m4&#x27;</span><br><span class="line">libtoolize: copying file `m4/ltversion.m4&#x27;</span><br><span class="line">libtoolize: copying file `m4/lt~obsolete.m4&#x27;</span><br><span class="line">configure.ac:159: installing &#x27;./compile&#x27;</span><br><span class="line">configure.ac:22: installing &#x27;./config.guess&#x27;</span><br><span class="line">configure.ac:22: installing &#x27;./config.sub&#x27;</span><br><span class="line">configure.ac:23: installing &#x27;./install-sh&#x27;</span><br><span class="line">configure.ac:174: error: required file &#x27;./ltmain.sh&#x27; not found</span><br><span class="line">configure.ac:23: installing &#x27;./missing&#x27;</span><br></pre></td></tr></table></figure>

<p>解决方法：aclocal &amp;&amp; autoheader &amp;&amp; autoconf &amp;&amp; automake –add-missing<br>参考了<a href="https://stackoverflow.com/questions/22603163/automake-error-ltmain-sh-not-found">https://stackoverflow.com/questions/22603163/automake-error-ltmain-sh-not-found</a></p>
<h1 id="使用举例"><a href="#使用举例" class="headerlink" title="使用举例"></a>使用举例</h1><p>一共有5种使用方式：</p>
<ul>
<li><p> TC Malloc</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gcc [...] -ltcmalloc</span><br></pre></td></tr></table></figure></li>
<li><p>Heap Checker</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gcc [...] -o myprogram -ltcmalloc</span><br><span class="line">HEAPCHECK=normal ./myprogram</span><br></pre></td></tr></table></figure></li>
<li><p>Heap Profiler</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gcc [...] -o myprogram -ltcmalloc</span><br><span class="line">HEAPPROFILE=/tmp/netheap ./myprogram</span><br></pre></td></tr></table></figure></li>
<li><p>Cpu Profiler</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gcc [...] -o myprogram -lprofiler</span><br><span class="line">CPUPROFILE=/tmp/profile ./myprogram</span><br></pre></td></tr></table></figure></li>
<li><p>pprof and Remote Servers</p>
</li>
</ul>
<p>接下来以Cpu Profiler来详细说明使用方式。</p>
<h2 id="Cpu-Profiler"><a href="#Cpu-Profiler" class="headerlink" title="Cpu Profiler"></a>Cpu Profiler</h2><h3 id="修改启动方式的运行"><a href="#修改启动方式的运行" class="headerlink" title="修改启动方式的运行"></a>修改启动方式的运行</h3><p>示例代码test.cpp如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;gperftools/profiler.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">while</span> (i &lt; <span class="number">100000</span>) &#123;</span><br><span class="line">       ++i;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">while</span> (i &lt; <span class="number">200000</span>) &#123;</span><br><span class="line">       ++i;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func3</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; ++i) &#123;</span><br><span class="line">       <span class="built_in">func1</span>();</span><br><span class="line">       <span class="built_in">func2</span>();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="built_in">func3</span>();</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-152-53 cpuprofilertest]# g++ test.cpp -lprofiler</span><br><span class="line">[root@10-8-152-53 cpuprofilertest]# CPUPROFILE=./test.prof ./a.out</span><br><span class="line">PROFILE: interrupts/evictions/bytes = 52/4/512</span><br></pre></td></tr></table></figure>


<p>运行后会生成test.prof文件，然后用pprof就可以生成text的分析报告，具体如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-152-53 cpuprofilertest]# pprof --text a.out test.prof</span><br><span class="line">Using local file a.out.</span><br><span class="line">Using local file test.prof.</span><br><span class="line">Total: 52 samples</span><br><span class="line">      40  76.9%  76.9%       40  76.9% func2</span><br><span class="line">      12  23.1% 100.0%       12  23.1% func1</span><br><span class="line">       0   0.0% 100.0%       52 100.0% __libc_start_main</span><br><span class="line">       0   0.0% 100.0%       52 100.0% _start</span><br><span class="line">       0   0.0% 100.0%       52 100.0% func3</span><br><span class="line">       0   0.0% 100.0%       52 100.0% main</span><br></pre></td></tr></table></figure>

<p>输出数据解析：每行包含6列数据，依次为:</p>
<ol>
<li>分析样本数量（不包含其他函数调用）</li>
<li>分析样本百分比（不包含其他函数调用）</li>
<li>目前为止的分析样本百分比（不包含其他函数调用）</li>
<li>分析样本数量（包含其他函数调用）</li>
<li>分析样本百分比（包含其他函数调用）</li>
<li>函数名</li>
</ol>
<p>运行命令生成函数调用树形式的pdf分析报告：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pprof --pdf a.out test.prof &gt; test.pdf</span><br></pre></td></tr></table></figure>

<img src="/2020/12/06/C++Profile%E7%9A%84%E5%A4%A7%E6%9D%80%E5%99%A8_gperftools%E7%9A%84%E4%BD%BF%E7%94%A8/gperftoos_cpuprofile.png" class="" title="image.png"><br />

<p>树上的每个节点代表一个函数，节点数据格式：</p>
<ol>
<li>函数名 或者 类名+方法名</li>
<li>不包含内部函数调用的样本数 (百分比)</li>
<li>包含内部函数调用的样本数 (百分比) #如果没有内部调用函数则这一项数据不显示</li>
</ol>
<h3 id="不修改启动方式，但修改代码方式的运行"><a href="#不修改启动方式，但修改代码方式的运行" class="headerlink" title="不修改启动方式，但修改代码方式的运行"></a>不修改启动方式，但修改代码方式的运行</h3><h4 id="运行一段时间会正常退出的程序的性能分析"><a href="#运行一段时间会正常退出的程序的性能分析" class="headerlink" title="运行一段时间会正常退出的程序的性能分析"></a>运行一段时间会正常退出的程序的性能分析</h4><p>这种情况，我们可以直接在代码中插入性能分析函数。示例代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;gperftools/profiler.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">while</span> (i &lt; <span class="number">100000</span>) &#123;</span><br><span class="line">       ++i;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">while</span> (i &lt; <span class="number">200000</span>) &#123;</span><br><span class="line">       ++i;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func3</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; ++i) &#123;</span><br><span class="line">       <span class="built_in">func1</span>();</span><br><span class="line">       <span class="built_in">func2</span>();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="built_in">ProfilerStart</span>(<span class="string">&quot;test.prof&quot;</span>); <span class="comment">// 指定所生成的profile文件名</span></span><br><span class="line">   <span class="built_in">func3</span>();</span><br><span class="line">   <span class="built_in">ProfilerStop</span>(); <span class="comment">// 结束profiling</span></span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编译运行，注意编译时需要连接tcmalloc和profiler库。运行后会生成test.prof文件，然后用pprof就可以生成text的分析报告，具体如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@10-8-152-53 cpuprofilertest]# g++ not_run_always.cpp -lprofiler -ltcmalloc</span><br><span class="line">[root@10-8-152-53 cpuprofilertest]# ./a.out</span><br><span class="line">PROFILE: interrupts/evictions/bytes = 52/7/680</span><br><span class="line">[root@10-8-152-53 cpuprofilertest]# pprof --text a.out test.prof</span><br><span class="line">Using local file a.out.</span><br><span class="line">Using local file test.prof.</span><br><span class="line">Total: 52 samples</span><br><span class="line">      32  61.5%  61.5%       32  61.5% func2</span><br><span class="line">      20  38.5% 100.0%       20  38.5% func1</span><br><span class="line">       0   0.0% 100.0%       52 100.0% __libc_start_main</span><br><span class="line">       0   0.0% 100.0%       52 100.0% _start</span><br><span class="line">       0   0.0% 100.0%       52 100.0% func3</span><br><span class="line">       0   0.0% 100.0%       52 100.0% main</span><br></pre></td></tr></table></figure>


<h4 id="一直运行的程序的性能分析"><a href="#一直运行的程序的性能分析" class="headerlink" title="一直运行的程序的性能分析"></a>一直运行的程序的性能分析</h4><p>一直运行的程序由于不能正常退出，所以不能采用上面的方法。我们可以用信号量来开启/关闭性能分析，具体代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;gperftools/profiler.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;signal.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">gprofStartAndStop</span><span class="params">(<span class="keyword">int</span> signum)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> isStarted = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (signum != SIGUSR1) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//通过isStarted标记未来控制第一次收到信号量开启性能分析，第二次收到关闭性能分析。</span></span><br><span class="line">    <span class="keyword">if</span> (!isStarted)&#123;</span><br><span class="line">        isStarted = <span class="number">1</span>;</span><br><span class="line">        <span class="built_in">ProfilerStart</span>(<span class="string">&quot;test.prof&quot;</span>);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;ProfilerStart success\n&quot;</span>);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">ProfilerStop</span>();</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;ProfilerStop success\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">while</span> (i &lt; <span class="number">100000</span>) &#123;</span><br><span class="line">       ++i;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">while</span> (i &lt; <span class="number">200000</span>) &#123;</span><br><span class="line">       ++i;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func3</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; ++i) &#123;</span><br><span class="line">       <span class="built_in">func1</span>();</span><br><span class="line">       <span class="built_in">func2</span>();</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="built_in">signal</span>(SIGUSR1, gprofStartAndStop);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">&quot;call f\n&quot;</span>);</span><br><span class="line">     <span class="built_in">func3</span>();</span><br><span class="line">     <span class="built_in">sleep</span>(<span class="number">1</span>);<span class="comment">//为了防止死循环，导致信号处理函数得不到调度</span></span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编译运行如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">[root@<span class="number">10</span><span class="number">-8</span><span class="number">-152</span><span class="number">-53</span> cpuprofilertest]<span class="meta"># g++ run_always.cpp -lprofiler -ltcmalloc</span></span><br><span class="line">[root@<span class="number">10</span><span class="number">-8</span><span class="number">-152</span><span class="number">-53</span> cpuprofilertest]# ./a.out</span><br><span class="line">call f</span><br><span class="line">call f</span><br><span class="line">...</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p><strong>通过kill命令发送信号给进程来开启/关闭性能分析：</strong><br />用top命令查看进程的PID<br />kill -s SIGUSR1 PID //第一次运行命令启动性能分析<br />kill -s SIGUSR1 PID //再次运行命令关闭性能分析，产生test.prof</p>
<p>后续查看分析报告和之前一样。<br />这种方式适合灵活关闭profile，不用重启启动服务，适合在线上临时查看。<br /></p>
<h2 id="Heap-Profiler"><a href="#Heap-Profiler" class="headerlink" title="Heap Profiler"></a>Heap Profiler</h2><h3 id="示例1-修改启动方式"><a href="#示例1-修改启动方式" class="headerlink" title="示例1 - 修改启动方式"></a>示例1 - 修改启动方式</h3><p>示例代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;gperftools/profiler.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f1</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;<span class="number">1024</span>*<span class="number">1024</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span>* p2 = <span class="keyword">new</span> <span class="keyword">int</span>;</span><br><span class="line">        <span class="comment">//delete[] p2;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f2</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;<span class="number">1024</span>*<span class="number">1024</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span>* p2 = <span class="keyword">new</span> <span class="keyword">int</span>;</span><br><span class="line">        <span class="comment">//delete[] p2;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="built_in">f1</span>();</span><br><span class="line">   <span class="built_in">f2</span>();</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>编译运行如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">[root@<span class="number">10</span><span class="number">-8</span><span class="number">-152</span><span class="number">-53</span> heapprofilertest]<span class="meta"># g++ test.cpp -ltcmalloc</span></span><br><span class="line">[root@<span class="number">10</span><span class="number">-8</span><span class="number">-152</span><span class="number">-53</span> heapprofilertest]<span class="meta"># env HEAPPROFILE=./test_heap.prof ./a.out</span></span><br><span class="line">Starting tracking the heap</span><br><span class="line">Dumping heap profile to ./test_heap.prof<span class="number">.0001</span>.<span class="built_in">heap</span> (Exiting, <span class="number">8</span> MB in use)</span><br><span class="line">[root@<span class="number">10</span><span class="number">-8</span><span class="number">-152</span><span class="number">-53</span> heapprofilertest]<span class="meta"># pprof --text ./a.out ./test_heap.prof.0001.heap</span></span><br><span class="line">Using local file ./a.out.</span><br><span class="line">Using local file ./test_heap.prof<span class="number">.0001</span>.heap.</span><br><span class="line">Total: <span class="number">8.0</span> MB</span><br><span class="line">     <span class="number">4.0</span>  <span class="number">50.0</span>%  <span class="number">50.0</span>%      <span class="number">4.0</span>  <span class="number">50.0</span>% f1</span><br><span class="line">     <span class="number">4.0</span>  <span class="number">50.0</span>% <span class="number">100.0</span>%      <span class="number">4.0</span>  <span class="number">50.0</span>% f2</span><br><span class="line">     <span class="number">0.0</span>   <span class="number">0.0</span>% <span class="number">100.0</span>%      <span class="number">8.0</span> <span class="number">100.0</span>% __libc_start_main</span><br><span class="line">     <span class="number">0.0</span>   <span class="number">0.0</span>% <span class="number">100.0</span>%      <span class="number">8.0</span> <span class="number">100.0</span>% _start</span><br><span class="line">     <span class="number">0.0</span>   <span class="number">0.0</span>% <span class="number">100.0</span>%      <span class="number">8.0</span> <span class="number">100.0</span>% main</span><br><span class="line">[root@<span class="number">10</span><span class="number">-8</span><span class="number">-152</span><span class="number">-53</span> heapprofilertest]<span class="meta"># pprof --pdf ./a.out ./test_heap.prof.0001.heap &gt; ./test_heap.pdf</span></span><br><span class="line">Using local file ./a.out.</span><br><span class="line">Using local file ./test_heap.prof<span class="number">.0001</span>.heap.</span><br><span class="line">Dropping nodes with &lt;= <span class="number">0.0</span> MB; edges with &lt;= <span class="number">0.0</span> <span class="built_in">abs</span>(MB)</span><br><span class="line">[root@<span class="number">10</span><span class="number">-8</span><span class="number">-152</span><span class="number">-53</span> heapprofilertest]<span class="meta"># ls</span></span><br><span class="line">a.out  test.cpp  test_heap.pdf  test_heap.prof<span class="number">.0001</span>.heap</span><br></pre></td></tr></table></figure>

<img src="/2020/12/06/C++Profile%E7%9A%84%E5%A4%A7%E6%9D%80%E5%99%A8_gperftools%E7%9A%84%E4%BD%BF%E7%94%A8/gperftools_heapprofile.png" class="" title="image.png">

<p>在生成heap的过程中，还有另外的一些属性可以设置[2]：</p>
<table>
<thead>
<tr>
<th><code>HEAP_PROFILE_ALLOCATION_INTERVAL</code></th>
<th>default: 1073741824 (1 Gb)</th>
<th>Dump heap profiling information each time the specified number of bytes has been allocated by the program.</th>
</tr>
</thead>
<tbody><tr>
<td><code>HEAP_PROFILE_INUSE_INTERVAL</code></td>
<td>default: 104857600 (100 Mb)</td>
<td>Dump heap profiling information whenever the high-water memory usage mark increases by the specified number of bytes.</td>
</tr>
<tr>
<td><code>HEAP_PROFILE_TIME_INTERVAL</code></td>
<td>default: 0</td>
<td>Dump heap profiling information each time the specified number of seconds has elapsed.</td>
</tr>
<tr>
<td><code>HEAPPROFILESIGNAL</code></td>
<td>default: disabled</td>
<td>Dump heap profiling information whenever the specified signal is sent to the process.</td>
</tr>
<tr>
<td><code>HEAP_PROFILE_MMAP</code></td>
<td>default: false</td>
<td>Profile <code>mmap</code>, <code>mremap</code> and <code>sbrk</code> calls in addition to <code>malloc</code>, <code>calloc</code>, <code>realloc</code>, and <code>new</code>. NOTE: this causes the profiler to profile calls internal to tcmalloc, since tcmalloc and friends use mmap and sbrk internally for allocations. One partial solution is to filter these allocations out when running <code>pprof</code>, with something like `pprof –ignore=’DoAllocWithArena</td>
</tr>
<tr>
<td><code>HEAP_PROFILE_ONLY_MMAP</code></td>
<td>default: false</td>
<td>Only profile <code>mmap</code>, <code>mremap</code>, and <code>sbrk</code> calls; do not profile <code>malloc</code>, <code>calloc</code>, <code>realloc</code>, or <code>new</code>.</td>
</tr>
<tr>
<td><code>HEAP_PROFILE_MMAP_LOG</code></td>
<td>default: false</td>
<td>Log <code>mmap</code>/<code>munmap</code> calls.</td>
</tr>
</tbody></table>
<h3 id="线上例子-修改启动方式"><a href="#线上例子-修改启动方式" class="headerlink" title="线上例子-修改启动方式"></a>线上例子-修改启动方式</h3><h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><p>加上-ltcmalloc<br><code>set(CMAKE_CXX_FLAGS &quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11 -g3 -ggdb3 -O3 -mavx2 -Wall -DDEBUG_RING -ltcmalloc&quot;)</code></p>
<h4 id="启动命令"><a href="#启动命令" class="headerlink" title="启动命令"></a>启动命令</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">env HEAPPROFILE=/tmp/broker_1003_xxx_heap.prof HEAP_PROFILE_ALLOCATION_INTERVAL=107374182400 HEAP_PROFILE_INUSE_INTERVAL=1073741824000 /root/ufile/UFileBroker-set1003/UFileBroker-set1003 -c /root/ufile/UFileBroker-set1003/config-set1003.ini</span><br></pre></td></tr></table></figure>

<ul>
<li>HEAP_PROFILE_ALLOCATION_INTERVAL=107374182400 每次分配了100GB内存，进行dump</li>
<li>HEAP_PROFILE_INUSE_INTERVAL=1073741824000   每次内存的最高使用量超过1000GB，进行dump，看下面日志没有达到触发条件。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Dumping heap profile to /tmp/broker_1003_xxx_heap.prof.0001.heap (10240 MB allocated cumulatively, 1118 MB currently in use)</span><br><span class="line">Dumping heap profile to /tmp/broker_1003_xxx_heap.prof.0002.heap (20480 MB allocated cumulatively, 2005 MB currently in use)</span><br><span class="line">Dumping heap profile to /tmp/broker_1003_xxx_heap.prof.0003.heap (30720 MB allocated cumulatively, 2383 MB currently in use)</span><br><span class="line">Dumping heap profile to /tmp/broker_1003_xxx_heap.prof.0004.heap (40960 MB allocated cumulatively, 2482 MB currently in use)</span><br><span class="line">......</span><br><span class="line">Dumping heap profile to /tmp/broker_1003_xxx_heap.prof.0126.heap (1290366 MB allocated cumulatively, 2876 MB currently in use)</span><br><span class="line">Dumping heap profile to /tmp/broker_1003_xxx_heap.prof.0127.heap (1300606 MB allocated cumulatively, 2871 MB currently in use)</span><br><span class="line">Dumping heap profile to /tmp/broker_1003_xxx_heap.prof.0128.heap (1310849 MB allocated cumulatively, 2870 MB currently in use)</span><br><span class="line">Dumping heap profile to /tmp/broker_1003_xxx_heap.prof.0129.heap (1321090 MB allocated cumulatively, 2878 MB currently in use)</span><br></pre></td></tr></table></figure>


<h4 id="分析heap"><a href="#分析heap" class="headerlink" title="分析heap"></a>分析heap</h4><img src="/2020/12/06/C++Profile%E7%9A%84%E5%A4%A7%E6%9D%80%E5%99%A8_gperftools%E7%9A%84%E4%BD%BF%E7%94%A8/broker_1003_3GB.jpeg" class="" title="image.png">
<p>可以看到95%的heap都是从同一个地方分配出来的，是需要优化的方向。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://github.com/gperftools/gperftools/wiki">https://github.com/gperftools/gperftools/wiki</a></li>
<li><a href="https://gperftools.github.io/gperftools/heapprofile.html">https://gperftools.github.io/gperftools/heapprofile.html</a></li>
<li><a href="https://github.com/gperftools/gperftools/wiki">https://github.com/gperftools/gperftools/wiki</a></li>
<li><a href="https://www.cnblogs.com/gary-guo/p/10607514.html">gperftools对程序进行分析</a></li>
</ol>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>profile</tag>
        <tag>gperftools</tag>
      </tags>
  </entry>
  <entry>
    <title>HTTP Keepalive</title>
    <url>/2018/10/08/HTTP-Keepalive/</url>
    <content><![CDATA[<p>参考这里，已经很详细了 <a href="http://www.nowamagic.net/academy/detail/23350305">HTTP Keep-Alive是什么？如何工作？</a></p>
]]></content>
  </entry>
  <entry>
    <title>Remote Debugging Go Code with Visual Studio Code</title>
    <url>/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/</url>
    <content><![CDATA[<p>当了解一些大型项目的代码逻辑时，如kubernetes，跟着调试器跟踪代码学习，是很好的方式。当项目部署在远程环境时，在本机进行开发，很自然对Remote Debug有迫切需求，这也是和调试一般golang程序最主要的诉求区别点，本文将通过两个实际例子，来介绍如何Remote Debug。</p>
<span id="more"></span>

<h1 id="dlv远程调试的原理"><a href="#dlv远程调试的原理" class="headerlink" title="dlv远程调试的原理"></a>dlv远程调试的原理</h1><p>vscode支持丰富的插件，dlv插件可以帮助我们调试golang代码，基本使用方法详见 <a href="https://github.com/go-delve/delve">delve官网</a>，以及我之前的一篇文档<a href="https://xusenqi.github.io/2019/08/25/%E9%80%9A%E8%BF%87delve-dlv-%E8%B0%83%E8%AF%95golang%E7%A8%8B%E5%BA%8F/#more">通过delve(dlv)调试golang程序</a>。dlv远程调试的基本原理是，在服务端启动dlv server，客户端vscode去连接dlv server</p>
<img src="/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/dlv_server.jpeg" class="" title="image">

<h1 id="远程调试的步骤"><a href="#远程调试的步骤" class="headerlink" title="远程调试的步骤"></a>远程调试的步骤</h1><p>主要参考了<a href="https://github.com/Microsoft/vscode-go/wiki/Debugging-Go-code-using-VS-Code">Debugging Go code using VS Code</a>，现将步骤整理如下：</p>
<h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><h3 id="Local安装Golang"><a href="#Local安装Golang" class="headerlink" title="Local安装Golang"></a>Local安装Golang</h3><p>参考Golang官网安装步骤<a href="https://golang.org/doc/install">Getting Started</a></p>
<h3 id="Local和Remote端安装dlv工具"><a href="#Local和Remote端安装dlv工具" class="headerlink" title="Local和Remote端安装dlv工具"></a>Local和Remote端安装dlv工具</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">go get -u github.com/go-delve/delve/cmd/dlv</span><br><span class="line"></span><br><span class="line">验证dlv版本</span><br><span class="line">dlv version</span><br></pre></td></tr></table></figure>
<h2 id="本地代码版本切换到和目标端一致"><a href="#本地代码版本切换到和目标端一致" class="headerlink" title="本地代码版本切换到和目标端一致"></a>本地代码版本切换到和目标端一致</h2><p>以一个服务uhost-scheduler为例, 线上和本地都保持为1.0.10版本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@yg-man-uhost-set9-01 bin]# /data/uframework/uhost_server -v</span><br><span class="line">option = a  value =</span><br><span class="line">option = i  value = 0</span><br><span class="line">option = s  value =</span><br><span class="line">option = v  value = true</span><br><span class="line">option = z  value =</span><br><span class="line"></span><br><span class="line">Version: 1.0.10</span><br><span class="line">Git branch: from_pxu_bugfix</span><br><span class="line">Git commit: 3f997fe</span><br><span class="line">Git user.name    patrick</span><br><span class="line">Git user.email     xusen_qi@163.com</span><br><span class="line">Go version: go_version_go1.8.1_darwin/amd64</span><br><span class="line">Build time: 2019-09-02T20:26:58+0800</span><br></pre></td></tr></table></figure>

<p>##在Remote端启动dlv server<br>启动命令如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dlv --listen=&quot;:50000&quot; --headless=true --log --api-version=2 exec /data/uframework/uhost_server \</span><br><span class="line">&gt; -- --z=172.23.0.77:2181,172.23.0.78:2181,172.23.0.79:2181,172.23.0.80:2181,172.23.0.81:2181 \</span><br><span class="line">&gt; --a=uhost-scheduler --s=set9 --i=0</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li>-l, –listen string        Debugging server listen address. (default “127.0.0.1:0”)</li>
<li>–headless             Run debug server only, in headless mode.</li>
<li>–log                  Enable debugging server logging.</li>
</ul>
<p>更多参数说明详见<a href="https://github.com/go-delve/delve/blob/master/Documentation/usage/dlv.md">dlv</a></p>
<h2 id="在Local端的调试"><a href="#在Local端的调试" class="headerlink" title="在Local端的调试"></a>在Local端的调试</h2><h3 id="dlv命令行的调试"><a href="#dlv命令行的调试" class="headerlink" title="dlv命令行的调试"></a>dlv命令行的调试</h3><p>访问dlv server的IP:Port，可以调试，类似于gdb</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  ~ dlv connect 172.23.0.159:50000</span><br><span class="line">Type &#x27;help&#x27; for list of commands.</span><br><span class="line">(dlv) break main.main</span><br><span class="line">Breakpoint 1 set at 0x823deb for main.main() ./go/src/uhost-go/uhost_server.go:31</span><br></pre></td></tr></table></figure>

<h3 id="在VS-Code里的调试"><a href="#在VS-Code里的调试" class="headerlink" title="在VS Code里的调试"></a>在VS Code里的调试</h3><p>但命令行里，没有图形界面直观和方便，所以更推荐在VS Code里的调试</p>
<ol>
<li>保持VS Code打开的代码，和Remote端的代码或二进制包一致</li>
<li>在本地主机中配置远程调试的参数，会在当前项目下的.vscode目录产生一个launch.json文件</li>
</ol>
<img src="/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/Open_Configurations.jpeg" class="" title="image">

<ol start="3">
<li>配置Remote端的IP和Port</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;name&quot;: &quot;Remote uhost-scheduler&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;go&quot;,</span><br><span class="line">    &quot;request&quot;: &quot;attach&quot;,</span><br><span class="line">    &quot;mode&quot;: &quot;remote&quot;,</span><br><span class="line">    &quot;remotePath&quot;: &quot;/Users/patrick.xu/go/src/uhost-go&quot;,</span><br><span class="line">    &quot;port&quot;: 50000,</span><br><span class="line">    &quot;host&quot;: &quot;172.23.0.159&quot;,</span><br><span class="line">    &quot;showLog&quot;: true,</span><br><span class="line">    &quot;trace&quot;: &quot;log&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>运行Debug，可以像本地一样调试，如加断点、条件断点、单步调试等。</li>
</ol>
<p>本地VS Code界面</p>
<img src="/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/VSCode_debug_scheduler%E7%95%8C%E9%9D%A2.jpeg" class="" title="image">

<p>Remote端页面</p>
<img src="/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/dlv_server_scheduler_%E8%B0%83%E8%AF%95.jpeg" class="" title="image">

<p>Remote-Debug-Go-Code-with-Visual-Studio-Code.md</p>
<p>条件断点</p>
<img src="/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/%E6%9D%A1%E4%BB%B6%E6%96%AD%E7%82%B9.png" class="" title="image">

<h1 id="k8s-scheduler的远程调试"><a href="#k8s-scheduler的远程调试" class="headerlink" title="k8s scheduler的远程调试"></a>k8s scheduler的远程调试</h1><p>k8s里有很多单独的服务，如kube-apiserver、kube-proxy、kube-scheduler等，每个都可以作为单独的调试代码。最近在看kube-scheduler的代码，所以这里以它为例。</p>
<img src="/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/kubernetes_components_architecture.png" class="" title="image">
<center>High level Kubernetes architecture diagram showing a cluster with a master and two worker nodes</center>
<center>Source: https://x-team.com/blog/introduction-kubernetes-architecture</center>

<h2 id="k8s环境的搭建"><a href="#k8s环境的搭建" class="headerlink" title="k8s环境的搭建"></a>k8s环境的搭建</h2><p>k8s环境通过kubeadm搭建在远程的4台虚机上，1台master和3台node，具体的搭建步骤见<a href="https://xusenqi.github.io/2019/12/11/kubeadm%E9%83%A8%E7%BD%B2single-control-plane-k8s%E9%9B%86%E7%BE%A4/">kubeadm部署single control-plane k8s集群</a>。</p>
<h2 id="停止kube-scheduler服务"><a href="#停止kube-scheduler服务" class="headerlink" title="停止kube-scheduler服务"></a>停止kube-scheduler服务</h2><p>我们需要dlv拉起kube-scheduler服务，但是kubeadm部署的kube-scheduler被kill掉后<br>，会自动拉起来，占用10251的端口，影响调试。</p>
<img src="/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/kubeadm_kube_scheduler.jpeg" class="" title="image">

<p>这里对对配置文件改名，防止自动拉起kube-scheduler<br><code>mv /etc/kubernetes/scheduler.conf /etc/kubernetes/scheduler_191223.conf</code></p>
<h2 id="dlv-server"><a href="#dlv-server" class="headerlink" title="dlv server"></a>dlv server</h2><p>dlv server启动后，监听40000端口</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dlv --listen=&quot;:40000&quot; --headless=true --log --api-version=2 exec /data/k8s/kube-scheduler \</span><br><span class="line">-- --authentication-kubeconfig=/etc/kubernetes/scheduler_191223.conf \</span><br><span class="line">--authorization-kubeconfig=/etc/kubernetes/scheduler_191223.conf \</span><br><span class="line">--bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler_191223.conf \</span><br><span class="line">--leader-elect=true</span><br></pre></td></tr></table></figure>

<p>注意：本地是从公网访问虚机的40000端口，如果防火墙有打开的话，要关掉。</p>
<h2 id="本地调试kube-scheduler"><a href="#本地调试kube-scheduler" class="headerlink" title="本地调试kube-scheduler"></a>本地调试kube-scheduler</h2><p>与之前步骤一样</p>
<ol>
<li>代码与线上一致；如果不一致，重新编译新的版本推到线上，重新拉起服务</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  kube-scheduler git:(b3cbbae) ✗ pwd</span><br><span class="line">/Users/patrick.xu/go/src/k8s.io/kubernetes/cmd/kube-scheduler</span><br><span class="line">➜  kube-scheduler git:(b3cbbae) CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build</span><br><span class="line">➜  kube-scheduler git:(b3cbbae) ✗ ll</span><br><span class="line">total 106208</span><br><span class="line">-rw-r--r--  1 patrick.xu  staff   1.1K Dec 11 20:11 BUILD</span><br><span class="line">-rw-r--r--  1 patrick.xu  staff   144B Dec 11 20:11 OWNERS</span><br><span class="line">drwxr-xr-x  7 patrick.xu  staff   224B Dec 11 20:11 app</span><br><span class="line">-rwxr-xr-x  1 patrick.xu  staff    52M Dec 18 01:31 kube-scheduler</span><br><span class="line">-rw-r--r--  1 patrick.xu  staff   1.3K Dec 11 20:11 scheduler.go</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>配置文件launch.json的内容</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;name&quot;: &quot;Remote k8s server&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;go&quot;,</span><br><span class="line">    &quot;request&quot;: &quot;attach&quot;,</span><br><span class="line">    &quot;showLog&quot;: true,</span><br><span class="line">    &quot;mode&quot;: &quot;remote&quot;,</span><br><span class="line">    &quot;remotePath&quot;: &quot;/Users/patrick.xu/go/src/k8s.io&quot;,</span><br><span class="line">    &quot;port&quot;: 40000,</span><br><span class="line">    &quot;host&quot;: &quot;152.32.173.26&quot;,</span><br><span class="line">    &quot;trace&quot;: &quot;log&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>3.运行Debug，这时可以像可以像本地一样调试，如加断点、单步调试等。</p>
<p>本地VS Code远程调试kube-scheduler界面</p>
<img src="/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/VSCode_debug_kube_scheduler%E7%95%8C%E9%9D%A2.jpeg" class="" title="image">

<p>Remote端kube-scheduler输出，看到打断点和调试的日志</p>
<img src="/2019/12/23/Remote-Debug-Go-Code-with-Visual-Studio-Code/dlv_server_kube_scheduler_%E8%B0%83%E8%AF%95.jpeg" class="" title="image">

<p>后面就是结合创建pod，去分析具体的调度逻辑了。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://github.com/go-delve/delve">delve官网</a></li>
<li><a href="https://github.com/Microsoft/vscode-go/wiki/Debugging-Go-code-using-VS-Code">Debugging Go code using VS Code</a></li>
<li><a href="http://zhongpan.tech/2019/10/21/016.remote-debug-golang-with-vscode/#more">使用vscode远程调试golang</a></li>
<li><a href="https://xusenqi.github.io/2019/08/25/%E9%80%9A%E8%BF%87delve-dlv-%E8%B0%83%E8%AF%95golang%E7%A8%8B%E5%BA%8F/#more">通过delve(dlv)调试golang程序</a></li>
</ol>
]]></content>
      <categories>
        <category>vscode</category>
      </categories>
      <tags>
        <tag>vscode</tag>
        <tag>remote debug</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP Keepalive</title>
    <url>/2018/10/08/TCP-Keepalive/</url>
    <content><![CDATA[<h2 id="TCP-Keepalive的作用"><a href="#TCP-Keepalive的作用" class="headerlink" title="TCP Keepalive的作用"></a>TCP Keepalive的作用</h2><p>链接建立之后，如果应用程序或者上层协议一直不发送数据，或者隔很长时间才发送一次数据，当链接很久没有数据报文传输时如何去确定对方还在线，到底是掉线了还是确实没有数据传输，链接还需不需要保持，这种情况在TCP协议设计中是需要考虑到的。 </p>
<p>TCP协议通过一种巧妙的方式去解决这个问题，当超过一段时间之后，TCP自动发送一个数据为空的报文给对方，如果对方回应了这个报文，说明对方还在线，链接可以继续保持，如果对方没有报文返回，并且重试了多次之后则认为链接丢失，没有必要保持链接。 </p>
<span id="more"></span>
<h2 id="TCP-Keepalive的具体处理"><a href="#TCP-Keepalive的具体处理" class="headerlink" title="TCP Keepalive的具体处理"></a>TCP Keepalive的具体处理</h2><p>对于面向连接的TCP socket,在实际应用中通常都要检测对端是否处于连接中,连接端口分两种情况:</p>
<p>1、连接正常关闭,调用close() shutdown()连接优雅关闭,send与recv立马返回错误,select返回SOCK_ERR;</p>
<p>2、连接的对端异常关闭,比如网络断掉,突然断电.</p>
<p>对于第二种情况,判断连接是否断开的方法有一下几种:</p>
<p>1、自己编写心跳包程序,简单的说就是自己的程序加入一条线程,定时向对端发送数据包,查看是否有ACK,根据ACK的返回情况来管理连接。此方法比较通用,一般使用业务层心跳处理,灵活可控,但改变了现有的协议;</p>
<p>2、使用TCP的keepalive机制,UNIX网络编程不推荐使用SO_KEEPALIVE来做心跳检测(为什么??)。<br>keepalive原理:TCP内嵌有心跳包,以服务端为例,当server检测到超过一定时间(/proc/sys/net/ipv4/tcp_keepalive_time 7200 即2小时)没有数据传输,那么会向client端发送一个keepalive packet,此时client端有三种反应:</p>
<p>1、client端连接正常,返回一个ACK.server端收到ACK后重置计时器,在2小时后在发送探测.如果2小时内连接上有数据传输,那么在该时间的基础上向后推延2小时发送探测包;</p>
<p>2、客户端异常关闭,或网络断开。client无响应,server收不到ACK,在一定时间(/proc/sys/net/ipv4/tcp_keepalive_intvl 75 即75秒)后重发keepalive packet, 并且重发一定次数(/proc/sys/net/ipv4/tcp_keepalive_probes 9 即9次);</p>
<p>3、客户端曾经崩溃,但已经重启.server收到的探测响应是一个复位,server端终止连接。</p>
<h2 id="注意和-HTTP的Keep-Alive区别"><a href="#注意和-HTTP的Keep-Alive区别" class="headerlink" title="注意和 HTTP的Keep-Alive区别"></a>注意和 HTTP的Keep-Alive区别</h2><p>http keep-alive与tcp keep-alive，不是同一回事，意图不一样。http keep-alive是为了让tcp活得更久一点，以便在同一个连接上传送多个http，提高socket的效率。<br>http-keepalive的具体作用查看这里 <a href="http://www.nowamagic.net/academy/detail/23350305">HTTP Keep-Alive是什么？如何工作？</a></p>
<p>TCP的keepalive机制意图在于保活、心跳，检测连接错误。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/callinglove/article/details/38380673">Linux SO_KEEPALIVE属性,心跳</a></p>
<p><a href="http://www.blogjava.net/yongboy/archive/2015/04/14/424413.html">随手记之TCP Keepalive笔记</a></p>
]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>Keepalive</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP三次握手</title>
    <url>/2018/05/15/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/</url>
    <content><![CDATA[<p>TCP三次握手/四次挥手的文章看过太多，但有些不太正确。还是决定自己写一篇，也算是对自己知识和碰到的问题的总结。</p>
<h2 id="tcp的主要特点"><a href="#tcp的主要特点" class="headerlink" title="tcp的主要特点"></a>tcp的主要特点</h2><ul>
<li>面向连接</li>
<li>点到点</li>
<li>可靠交付</li>
<li>全双工通信</li>
<li>面向字节流</li>
</ul>
<p>其中核心的特点：tcp是可以可靠传输协议，它的其他所有特点都为这个可靠传输服务。</p>
<h2 id="如何保证可靠传输"><a href="#如何保证可靠传输" class="headerlink" title="如何保证可靠传输"></a>如何保证可靠传输</h2><p>tcp在传输过程中都有一个ack，接收方通过ack告诉发送方收到那些包了。这样发送方能知道有没有丢包，进而确定重传。</p>
<h2 id="tcp三次握手"><a href="#tcp三次握手" class="headerlink" title="tcp三次握手"></a>tcp三次握手</h2><img src="/2018/05/15/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.png" class="" title="image">

<ul>
<li><p>第一步：client发送连接请求报文段，将SYN位置为1，Seq(Sequence Number)为X(由操作系统动态随机选取一个32位长的序列号)。然后，客户端进入SYN_SEND状态，等待服务器的确认。</p>
</li>
<li><p>第二步：server收到client的SYN报文段。需要对这个SYN报文段进行确认，设置Ack(Acknowledgment Number)设置为X(第一次握手中的Seq的值)+1。同时，自己还要发送SYN请求信息，将SYN位置为1，Seq(Sequence Number)为Y(由操作系统动态随机选取一个32位长的序列号)。服务器端将上述所有信息一并发送给客户端，此时服务器进入SYN_RECV状态。</p>
</li>
<li><p>第三步：client收到sever的SYN+ACK后，回复server一个ACK。将Ack(Acknowledgment Number)设置为Y(第二次握手中的Seq的值)+1，Seq(Sequence Number)设置为X+1(第二次握手中的Ack(Acknowledgment Number)值)，向服务器发送ACK报文段，这个报文段发送完毕以后，客户端和服务器端都进入ESTABLISHED状态，完成TCP三次握手。</p>
</li>
</ul>
<h2 id="tcp首部简介"><a href="#tcp首部简介" class="headerlink" title="tcp首部简介"></a>tcp首部简介</h2><p>这里先介绍下TCP首部的相关知识。三次握手相关的是主要是ACK和SYN这两个标志比特位。</p>
<img src="/2018/05/15/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/TCP%E9%A6%96%E9%83%A8.png" class="" title="image">

<p>（1）源端口和目的端口：各占2字节。</p>
<p>（2）序号(Seq, Sequence Number)：占4个字节。用来标识从TCP发端向TCP收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节在数据流中的序号。主要用来解决网络报乱序的问题。序号范围是[0, 2^32 - 1],共2^32个序号。</p>
<p>（3）确认号(Ack, Acknowledgment Number): 占4个字节。期望收到对方下一个报文段的第一个数据字节的序号，因此，确认序号应当是上次已成功收到数据字节序号加1。不过，只有当标志位中的ACK标志（下面介绍）为1时该确认序列号的字段才有效。主要用来解决不丢包的问题。</p>
<p>（4）数据偏移(Offset): 占4位。TCP报文段的数据起始处距离TCP报文段的起始处有多远。实际上指出TCP报文段的首部长度。但注意，“数据偏移”的单位是32位字（即以4字节长度为单位）。最多能表示15个32bit的的字，即4*15=60个字节的首部长度。因此TCP最多有60字节的首部。然而，没有选项字段，正常的长度是20字节。</p>
<p>（5）保留: 占6位。保留为今后使用，目前置为0。</p>
<p>（6）标志比特(TCP Flags): TCP首部中有6个标志比特，它们中的多个可同时被设置为1，主要是用于操控TCP的状态机的，依次为URG，ACK，PSH，RST，SYN，FIN。每个标志位的意思如下:</p>
<ul>
<li>URG（URGent）: 此标志表示TCP包的紧急指针域（后面马上就要说到）有效，用来保证TCP连接不被中断，并且督促中间层设备要尽快处理这些数据。</li>
<li>ACK(ACKnowlegment): 仅当ACK=1时Ack字段有效。在连接建立后所有传送的报文段都必须把ACK置1。</li>
<li>PSH(PuSH): 这个标志位表示Push操作。所谓Push操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队。</li>
<li>RST(ReSeT): 这个标志表示连接复位请求。用来复位那些产生错误的连接，也被用来拒绝错误和非法的数据包。</li>
<li>SYN(SYNchoronization): 表示同步序号，用来建立连接。SYN标志位和ACK标志位搭配使用，当连接请求的时候，SYN=1，ACK=0。连接被响应的时候，SYN=1，ACK=1。</li>
<li>FIN(FINis): 表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送FIN标志位的TCP数据包后，连接将被断开。</li>
</ul>
<p>（7）窗口: 占2个字节，[0, 2^16 - 1]。指的是发送本报文段的一方的接收窗口。明确指出了现在允许对方发送的数据量。窗口值是经常动态变化的。</p>
<p>（8）校验和: 占2字节。该字段检验的范围包括首部和数据这两部分。由发端计算和存储，并由收端进行验证。</p>
<p>（9）紧急指针: 占2个字节，紧急指针仅在URG=1时才有意义，它指出本报文段中的紧急数据的字节数。当所有紧急数据处理完毕时，TCP就告诉应用程序恢复到正常操作。值得注意的是，即使窗口为0时也可发送紧急数据。</p>
<p>（10）选项: 长度可变，最长可达40字节，当没有选项时，TCP的首部长度是20字节。最初只规定了一种选项，即最大报文段长度MSS(Maximum Segment Size)，MSS是指每一个TCP报文段中的数据字段的最大长度。</p>
<h2 id="抓包实践"><a href="#抓包实践" class="headerlink" title="抓包实践"></a>抓包实践</h2><p>来看一个pb请求的三次握手过程</p>
<img src="/2018/05/15/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E6%8A%93%E5%8C%85.jpg" class="" title="image">

<p>client: 172.17.1.217  server: 172.27.247.168</p>
<p>握手的核心目的是告知对方seq（client的初始seq，server的初始seq），对方回复ack（收到的seq+包的大小），这样发送端就知道有没有丢包了。</p>
<p>握手的次要目的是告知和协商一些信息。</p>
<ul>
<li>MSS–最大传输包</li>
<li>SACK_PERM–是否支持Selective ack(用户优化重传效率）</li>
<li>WS–窗口计算指数（有点复杂的话先不用管）</li>
</ul>
<p><strong>这就是tcp为什么要握手建立连接，就是为了解决tcp的可靠传输。</strong></p>
<h3 id="查看第一次握手的详情"><a href="#查看第一次握手的详情" class="headerlink" title="查看第一次握手的详情"></a>查看第一次握手的详情</h3><img src="/2018/05/15/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8F%A1%E6%89%8B%E6%8A%93%E5%8C%85.jpg" class="" title="image">
<h3 id="查看第二次握手的详情"><a href="#查看第二次握手的详情" class="headerlink" title="查看第二次握手的详情"></a>查看第二次握手的详情</h3><img src="/2018/05/15/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8F%A1%E6%89%8B%E6%8A%93%E5%8C%85.jpg" class="" title="image">
<h3 id="查看第三次握手的详情"><a href="#查看第三次握手的详情" class="headerlink" title="查看第三次握手的详情"></a>查看第三次握手的详情</h3><img src="/2018/05/15/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B/%E7%AC%AC%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E6%8A%93%E5%8C%85.jpg" class="" title="image">


<h3 id="为什么要三次握手？"><a href="#为什么要三次握手？" class="headerlink" title="为什么要三次握手？"></a>为什么要三次握手？</h3><ul>
<li><p>如果只有一次握手，Client不能确定与Server的单向连接，更加不能确定Server与Client的单向连接；</p>
</li>
<li><p>如果只有两次握手，Client确定与Server的单向连接，但是Server不能确定与Client的单向连接；</p>
</li>
<li><p>只有三次握手，Client与Server才能相互确认双向连接，实现双工数据传输。</p>
</li>
<li><p>谢希仁版《计算机网络》中的例子是这样的，“已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。</p>
</li>
</ul>
<h3 id="握手中断"><a href="#握手中断" class="headerlink" title="握手中断"></a>握手中断</h3><ul>
<li>第一次握手中断: A发送给B的SYN中断，A会周期性超时重传，直到A收到B的确认响应。</li>
<li>第二次握手中断: B发送给A的SYN、ACK中断，B会周期性超时重传，直到B收到A的确认响应。</li>
<li>第三次握手中断: A发送给B的ACK中断，A不会重传。超时后，B会重传SYN信号(即回到第二次握手)，直到B收到A的确认响应。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://www.jianshu.com/p/448f37ed29fe">https://www.jianshu.com/p/448f37ed29fe</a></li>
<li><a href="https://www.jianshu.com/p/a4beee06220c">https://www.jianshu.com/p/a4beee06220c</a></li>
<li><a href="http://jm.taobao.org/2017/06/08/20170608/">http://jm.taobao.org/2017/06/08/20170608/</a></li>
</ul>
]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP四次挥手</title>
    <url>/2018/05/15/TCP%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/</url>
    <content><![CDATA[<h2 id="tcp四次挥手"><a href="#tcp四次挥手" class="headerlink" title="tcp四次挥手"></a>tcp四次挥手</h2><p>TCP四次挥手的过程以及握手两端状态的变化。</p>
<img src="/2018/05/15/TCP%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/tcp%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B.png" class="" title="image">

<p>挥手过程</p>
<ul>
<li><p>第一次挥手: 主动关闭方(可以使客户端，也可以是服务器端，这里标记为：A)，将FIN置为1，ACK置为1，Seq(Sequence Number)设置为X为上一次对方传送过来的Ack(Acknowledgment Number)值，Ack(Acknowledgment Number)设置为Y为上一次对方传过来的Seq(Sequence Number)值+1。设置好以上值后，将数据发送至被动关闭方(这里标记为：B)。然后A进入FIN_WAIT_1状态。</p>
</li>
<li><p>第二次挥手：B收到了A发送的FIN报文段，向A回复，将ACK置为1，Ack(Acknowledgment Number)设置为X第一次挥手中的Seq(Sequence Number)值+1，Seq(Sequence Number)设置为Y第一次挥手中的Ack(Acknowledgment Number)值。然后B进入CLOSE_WAIT状态，A收到B的回复后，进入FIN_WAIT_2状态。</p>
</li>
<li><p>第三次挥手：B再次向A发送报文，将FIN置为1，ACK置为1，Ack(Acknowledgment Number)设置为X+1第二次挥手中的Ack(Acknowledgment Number)值，Seq(Sequence Number)设置为Y第二次挥手中的Seq(Sequence Number)值。然后B进入LAST_ACK状态，A收到B的报文后，进入TIME_WAIT状态。</p>
</li>
<li><p>第四次挥手：A收到B发送的FIN报文段，像B回复，将ACK置为1，Ack(Acknowledgment Number)设置为Y第三次挥手中的Seq(Sequence Number)值+1，Seq(Sequence Number)设置为X+1第三次挥手中的Ack(Acknowledgment Number)值。然后A进入TIME_WAIT状态，B在收到报文后进入CLOSED状态，A在发送完报文等待了2MSL时间后进入CLOSED状态。</p>
</li>
</ul>
<h2 id="为什么-TIME-WAIT-状态要等待-2MSL-之后才关闭连接"><a href="#为什么-TIME-WAIT-状态要等待-2MSL-之后才关闭连接" class="headerlink" title="为什么 TIME_WAIT 状态要等待 2MSL 之后才关闭连接"></a>为什么 TIME_WAIT 状态要等待 2MSL 之后才关闭连接</h2><ul>
<li><p>2MSL表示两个MSL的时长，MSL全称为Maximum Segment Life，表示TCP 对TCP Segment 生存时间的限制。</p>
</li>
<li><p>为了保证A发送的最后一个ACK报文段能够到达B。这个ACK报文段有可能丢失，因而使处在LAST_ACK状态的B收不到对自己已发送的FIN+ACK报文段的确认。B会超时重传这个FIN+ACK报文段。而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。接着A重传一次确认，重新启动2MSL计时器。最后A和B都正常进入到CLOSED状态。如果A在TIME_WAIT状态不等待一段时间，而是在发送完ACK报文段后立即释放连接，那么就无法收到B重传的FIN+ACK报文段，因而也不会在发送一次确认报文段。这样，B就无法按照正常步骤进入CLOSED状态。</p>
</li>
<li><p>防止已失效的连接请求报文段出现在本连接中。A在发送完最后一个ACK报文段后，在经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。</p>
</li>
</ul>
<h2 id="抓包实践"><a href="#抓包实践" class="headerlink" title="抓包实践"></a>抓包实践</h2><img src="/2018/05/15/TCP%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E6%8A%93%E5%8C%85.jpg" class="" title="image">

<p>抓包看，挥手的过程却只有三次。这是因为数据传输中的延迟确认策略。</p>
<p>何谓延迟确认策略？</p>
<p>WIKI:TCP delayed acknowledgment is a technique used by some implementations of the Transmission Control Protocol in an effort to improve network performance. In essence, several ACK responses may be combined together into a single response, reducing protocol overhead. However, in some circumstances, the technique can reduce application performance.即接收方收到包后，如果暂时没有内容回复给发送方，则延迟一段时间再确认，假如在这个时间范围内刚好有数据需要传输，则和确认包一起回复。这种也被称为数据捎带。延迟确认只是减轻网络负担，未必可以提升网络性能，有些情况下反而会影响性能。</p>
<p>正是这个策略，让图中缺少了一次断开连接的包，仔细看可以发现8和9之间时间也是差了200ms左右，为什么是200ms？根据TCP/IP详解卷一里面的描述是绝不大部分平台的时延是按200ms来实现的，但是不允许超过500ms。</p>
<p>谈到延迟确认就必须再谈谈Nagle算法，两者的作用都是减轻网络负担，Nagle算法起源于John Nagle在RFC896的提议，所以命名为Nagle算法。Nagle在描述The small-packet problem时提到TCP在传输1字节有用信息时必须传输41字节的数据，其中20字节的TCP头，20字节的IP头，4000%的开销在低负载网络是可以容忍的，但是在重负载网络，这种开销是会导致网络拥塞，进而导致重传和数据丢失。这里暂时先不用关注拥塞、重传等，后面再讨论。重点关注下Nagle算法的实现原理，即在发送的数据在未被确认前，如果有新的小数据生成，那就把小数据收集起来，等凑足一个MSS或者收到确认后再发送。</p>
<p>说到这，我们就清楚了提到的延迟确认和Nagle算法的作用都是降低网络负担，提高传输效率，但是未必能提升网络性能。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="http://www.cnblogs.com/iou123lg/p/8727598.html?utm_source=tuicool&amp;utm_medium=referral">http://www.cnblogs.com/iou123lg/p/8727598.html?utm_source=tuicool&amp;utm_medium=referral</a></li>
<li><a href="https://www.jianshu.com/p/448f37ed29fe">https://www.jianshu.com/p/448f37ed29fe</a></li>
<li><a href="https://www.jianshu.com/p/a4beee06220c">https://www.jianshu.com/p/a4beee06220c</a></li>
<li><a href="http://jm.taobao.org/2017/06/08/20170608/">http://jm.taobao.org/2017/06/08/20170608/</a></li>
</ul>
]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual Studio Code的使用总结</title>
    <url>/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="Visual-Studio-Code介绍"><a href="#Visual-Studio-Code介绍" class="headerlink" title="Visual Studio Code介绍"></a>Visual Studio Code介绍</h1><p>Visual Studio Code是很轻量但功能强大的代码编辑器，可以在Windows、macOS和 Linux上运行。</p>
<span id="more"></span>

<h1 id="User-and-Workspace-Settings"><a href="#User-and-Workspace-Settings" class="headerlink" title="User and Workspace Settings"></a>User and Workspace Settings</h1><p>通过settings，可以很容易地配置自己的Visual Studio Code。它提供了<br>two different scopes for settings:</p>
<ul>
<li>User Settings - Settings that apply globally to any instance of VS Code you open.</li>
<li>Workspace Settings - Settings stored inside your workspace and only apply when the workspace is opened.</li>
</ul>
<p>Workspace settings override user settings.</p>
<h2 id="打开方式"><a href="#打开方式" class="headerlink" title="打开方式"></a>打开方式</h2><p>页面打开：Code –&gt; Preferences –&gt; Settings，这种方式可以很直观地配置</p>
<img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/Settings.jpg" class="" title="image">

<p>也可以在json文件里直接修改：</p>
<ul>
<li>User Settings:   </li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  User cat settings.json</span><br><span class="line">&#123;</span><br><span class="line">    window.title: $&#123;activeEditorLong&#125;$&#123;separator&#125;$&#123;rootName&#125;,</span><br><span class="line">    workbench.statusBar.visible: true,</span><br><span class="line">    workbench.colorTheme: Visual Studio Dark,</span><br><span class="line">    workbench.iconTheme: vs-minimal, // panel</span><br><span class="line">    window.zoomLevel: 0,</span><br><span class="line">    editor.tabSize: 4,</span><br><span class="line">    editor.detectIndentation: false,</span><br><span class="line">    editor.renderControlCharacters: true,</span><br><span class="line">    editor.minimap.enabled: false,</span><br><span class="line">    leetcode.endpoint: leetcode,</span><br><span class="line">    leetcode.defaultLanguage: golang,</span><br><span class="line">    breadcrumbs.enabled: false,</span><br><span class="line">    editor.renderWhitespace: none,</span><br><span class="line">    terminal.integrated.shell.osx: zsh,</span><br><span class="line">    terminal.external.osxExec: iTerm.app,  // 默认打开iterm2</span><br><span class="line">    // golang</span><br><span class="line">    go.autocompleteUnimportedPackages: true,</span><br><span class="line">    go.formatTool: goimports, //使用 goimports 工具进行代码格式化，或者使用 goreturns 和 gofmt</span><br><span class="line">    [go]: &#123;</span><br><span class="line">        editor.formatOnSave: true,</span><br><span class="line">    &#125;,</span><br><span class="line">    // vim</span><br><span class="line">    vim.useSystemClipboard: true,</span><br><span class="line">    diffEditor.ignoreTrimWhitespace: false,</span><br><span class="line">    diffEditor.renderSideBySide: true,</span><br><span class="line">    C_Cpp.updateChannel: Insiders,</span><br><span class="line">    editor.suggestSelection: first,</span><br><span class="line">    vsintellicode.modify.editor.suggestSelection: automaticallyOverrodeDefaultValue,</span><br><span class="line">    editor.wordWrap: wordWrapColumn,</span><br><span class="line">    editor.wordWrapColumn: 120,</span><br><span class="line">    // 每行输入字符长度提示线</span><br><span class="line">    editor.rulers: [</span><br><span class="line">        80,</span><br><span class="line">        120</span><br><span class="line">    ],</span><br><span class="line">    editor.renderLineHighlight: all,</span><br><span class="line">    go.useLanguageServer: true,</span><br><span class="line">    &quot;window.openFilesInNewWindow&quot;: &quot;on&quot;, //启用后，将在新窗口中打开文件，而不是重复使用现有实例。</span><br><span class="line">       &quot;workbench.editor.enablePreview&quot;: false,   // 打开的文件显示在多个tab&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>Wrokspace: …</li>
</ul>
<h1 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h1><h2 id="默认常用快捷键"><a href="#默认常用快捷键" class="headerlink" title="默认常用快捷键"></a>默认常用快捷键</h2><ul>
<li>(Command Palette | Show All Commands)  shift + command + p 或者 F1<img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/CommandPalette.jpg" class="" title="image"></li>
<li>搜索文件 command + p</li>
<li>搜索内容(Find in Files) shift + command + f</li>
<li>回到上一个地方(Go Back) ctrl + -</li>
<li>跳到侧边栏(Focus into Side Bar) command + 0</li>
<li>切换侧边栏(Toggle Side Bar Visibility) command + b</li>
<li>打开原生的console(OpenNativeConsole) shift + command +c</li>
<li>切换Panel(Toggle Panel) command + j</li>
<li>调整Panel大小(Resize Panel) ctrl + command + &lt;– | –&gt;</li>
</ul>
<h3 id="Code-navigation"><a href="#Code-navigation" class="headerlink" title="Code navigation"></a>Code navigation</h3><p>![image](Visual-Studio-Code的使用总结/Code navigation.jpg)</p>
<ul>
<li>跳转到定义(Go to Definition) F12</li>
<li>(showAllSymbols) command + t</li>
</ul>
<h2 id="自定义快捷键"><a href="#自定义快捷键" class="headerlink" title="自定义快捷键"></a>自定义快捷键</h2><h3 id="更改方式"><a href="#更改方式" class="headerlink" title="更改方式"></a>更改方式</h3><p>打开键盘快捷键</p>
<img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/OpenKeyboardShotcuts.jpg" class="" title="image">

<p>Keyboard Shotcuts里显示当前支持哪些快捷键，包括默认的和自定义的；也可以在里面直接更改</p>
<img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/KeyboardShotcuts.jpg" class="" title="image">

<p>Keyboard Shotcuts(json)里了自定义的快捷键；前面在Keyboard Shotcuts修改的会展示在这里，也可以在这里直接修改</p>
<img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/KeyboardShotcuts_json.jpg" class="" title="image">

<h3 id="常用自定义快捷键"><a href="#常用自定义快捷键" class="headerlink" title="常用自定义快捷键"></a>常用自定义快捷键</h3><p>根绝个人喜好，常用的如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> &#123;</span><br><span class="line">     &quot;key&quot;: &quot;cmd+enter&quot;,</span><br><span class="line">     &quot;command&quot;: &quot;workbench.action.toggleFullScreen&quot;  //切换全屏</span><br><span class="line"> &#125;,</span><br><span class="line"> &#123;</span><br><span class="line">     &quot;key&quot;: &quot;cmd+right&quot;,</span><br><span class="line">     &quot;command&quot;: &quot;workbench.action.nextEditor&quot;   //下个编辑器</span><br><span class="line"> &#125;,</span><br><span class="line"> &#123;</span><br><span class="line">     &quot;key&quot;: &quot;cmd+left&quot;,</span><br><span class="line">     &quot;command&quot;: &quot;workbench.action.previousEditor&quot; //上个编辑器</span><br><span class="line"> &#125;,</span><br><span class="line"> &#123;</span><br><span class="line">    &quot;key&quot;: &quot;alt+cmd+n&quot;,</span><br><span class="line">    &quot;command&quot;: &quot;openInTerminal&quot;  //打开Terminal,并跳到当前文件的目录</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">    &quot;key&quot;: &quot;cmd+s cmd+w&quot;,</span><br><span class="line">    &quot;command&quot;: &quot;workbench.action.quickSwitchWindow&quot; // 切换窗口</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">    &quot;key&quot;: &quot;cmd+2&quot;,</span><br><span class="line">    &quot;command&quot;: &quot;workbench.action.focusPanel&quot;   // 跳到Panel</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>


<h1 id="常用插件"><a href="#常用插件" class="headerlink" title="常用插件"></a>常用插件</h1><p>在左边Extensions里选择插件install。下面列出几种常用的插件：</p>
<h2 id="Vim"><a href="#Vim" class="headerlink" title="Vim"></a>Vim</h2><h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><ul>
<li>Vim方向键无法长按</li>
</ul>
<p>解决方案：终端下执行命令：</p>
<p>defaults write com.microsoft.VSCode ApplePressAndHoldEnabled -bool false</p>
<p>复原:</p>
<p>defaults write com.microsoft.VSCode ApplePressAndHoldEnabled -bool true</p>
<ul>
<li>让 VIM yank 至剪切板的内容通过 ⌘  V 粘贴出来</li>
</ul>
<p>配置如下 setting：<br>“vim.useSystemClipboard”: true,</p>
<h2 id="Go"><a href="#Go" class="headerlink" title="Go"></a>Go</h2><p><a href="https://code.visualstudio.com/docs/languages/go">https://code.visualstudio.com/docs/languages/go</a></p>
<h3 id="Auto-completions"><a href="#Auto-completions" class="headerlink" title="Auto completions"></a>Auto completions</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;go.autocompleteUnimportedPackages&quot;: true, // Auto completions</span><br><span class="line">&quot;go.formatTool&quot;: &quot;gofmt&quot;, //代码格式化, 可以用goimports，或者goreturns和gofmt; 必须把vscode的&quot;files.autoSave&quot;: off才能生效</span><br></pre></td></tr></table></figure>
<h3 id="Code-navigation-1"><a href="#Code-navigation-1" class="headerlink" title="Code navigation"></a>Code navigation</h3><p>Code navigation features are available in the context menu in the editor.</p>
<ul>
<li>Go To Definition F12 - Go to the source code of the type definition.</li>
<li>Peek Definition ⌥F12 - Bring up a Peek window with the type definition.</li>
<li>Peek References <strong>⇧F12</strong> - Show all references for the type. (显示该类型的所有引用,个人觉得这个功能很强大)<img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/PeekReferences.jpg" class="" title="image"></li>
</ul>
<p>You can navigate via symbol search using the Go to Symbol commands from the Command Palette (⇧⌘P).</p>
<p>Go to Symbol in File - ⇧⌘O<br>显示该文件的symbol,symbol: 指定义的函数，变量，类型等</p>
<img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/symbols.jpg" class="" title="image">

<p>Go to Symbol in Workspace - ⌘T</p>
<p>You can also navigate back and forth between a Go file and its test implementation using the Go: Toggle Test File command.</p>
<h3 id="Debugging-Go-code-using-VS-Code"><a href="#Debugging-Go-code-using-VS-Code" class="headerlink" title="Debugging Go code using VS Code"></a><a href="https://github.com/Microsoft/vscode-go/wiki/Debugging-Go-code-using-VS-Code">Debugging Go code using VS Code</a></h3><p>设置一个或者多个GOPATH</p>
<img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/GoDebugConfigurations.jpg" class="" title="image">

<h2 id="C-C"><a href="#C-C" class="headerlink" title="C/C++"></a>C/C++</h2><h3 id="Edit-Configurations"><a href="#Edit-Configurations" class="headerlink" title="Edit Configurations"></a>Edit Configurations</h3><p>F1打开Command Palette –&gt; C/C++: Edit configurations (JSON)</p>
<p>主要设置”includePath”, 便于代码跳转，参考 <a href="https://code.visualstudio.com/docs/cpp/c-cpp-properties-schema-reference">https://code.visualstudio.com/docs/cpp/c-cpp-properties-schema-reference</a></p>
<h1 id="在终端里直接打开vscode"><a href="#在终端里直接打开vscode" class="headerlink" title="在终端里直接打开vscode"></a>在终端里直接打开vscode</h1><img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/code.jpg" class="" title="image">

<p>在vscode 中快捷键 shift + command + p 输入 code ,选择安装code 命令。<br>本质是做了个软链/usr/local/bin/code</p>
<img src="/2019/08/23/Visual-Studio-Code%E7%9A%84%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/CodeLink.jpg" class="" title="image">

<p>用vscode打开当前目录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">code .</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>vscode</category>
      </categories>
      <tags>
        <tag>快捷键</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title>du、df和ls的区别</title>
    <url>/2018/11/10/du%E3%80%81df%E5%92%8Cls%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<p>待整理。。。。</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title>golang - json操作的注意点</title>
    <url>/2018/10/22/golang-json%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B3%A8%E6%84%8F%E7%82%B9/</url>
    <content><![CDATA[<h2 id="变量的大小写"><a href="#变量的大小写" class="headerlink" title="变量的大小写"></a>变量的大小写</h2><p>go中根据首字母的大小写来确定可以访问的权限。如果首字母大写，则可以被其他的包访问；如果首字母小写，则只能在本包中使用。包括接口、类型、函数和变量等。</p>
<p>可以简单的理解成，首字母大写是公有的，首字母小写是私有的</p>
<p>下面是一个排查该问题的例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">	&quot;encoding/json&quot;</span><br><span class="line">	&quot;fmt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">type Position struct &#123;</span><br><span class="line">	X int</span><br><span class="line">	Y int</span><br><span class="line">	Z int</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Student struct &#123;</span><br><span class="line">	Name     string</span><br><span class="line">	Sex      string</span><br><span class="line">	Age      int</span><br><span class="line">	position Position</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">	position1 := Position&#123;10, 20, 30&#125;</span><br><span class="line">	student1 := Student&#123;&quot;zhangsan&quot;, &quot;male&quot;, 20, position1&#125;</span><br><span class="line">	position2 := Position&#123;15, 10, 20&#125;</span><br><span class="line">	student2 := Student&#123;&quot;lisi&quot;, &quot;female&quot;, 18, position2&#125;</span><br><span class="line"></span><br><span class="line">	var srcSlice = make([]Student, 2)</span><br><span class="line">	srcSlice[0] = student1</span><br><span class="line">	srcSlice[1] = student2</span><br><span class="line">	fmt.Printf(&quot;Init:srcSlice is : %v\n&quot;, srcSlice)</span><br><span class="line">	data, err := json.Marshal(srcSlice)</span><br><span class="line">	if err != nil &#123;</span><br><span class="line">		fmt.Printf(&quot;Serialize:json.Marshal error! %v\n&quot;, err)</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line">	fmt.Println(&quot;After serialize, data : \n&quot;, string(data))</span><br><span class="line"></span><br><span class="line">	var dstSliece = make([]Student, 2)</span><br><span class="line">	err = json.Unmarshal(data, &amp;dstSliece)</span><br><span class="line">	if err != nil &#123;</span><br><span class="line">		fmt.Printf(&quot;Deserialize: json.Unmarshal error! %v\n&quot;, err)</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line">	fmt.Printf(&quot;Deserialize:dstSlice is : %v\n&quot;, dstSliece)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<img src="/2018/10/22/golang-json%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B3%A8%E6%84%8F%E7%82%B9/1_%E5%B0%8F%E5%86%99%E5%AF%BC%E8%87%B4%E5%BA%8F%E5%88%97%E5%8C%96%E5%90%8E%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1.png" class="" title="image">

<p>很意外的是，我们反序列化后获取的对象数据dstSliece是错误的，Position里的数据都变成了0。而json.Unmarshal没有返回任何异常。</p>
<p>观察将序列化后的json串，Position的数据丢了，这使得我们想到了可见性，即大写的符号在包外可见。通过走查代码，我们发现Student的定义中，Position的变量名是小写开始的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">type Student struct &#123;</span><br><span class="line">    Name string</span><br><span class="line">    Sex string</span><br><span class="line">    Age int</span><br><span class="line">    //position Position</span><br><span class="line">    Position Position</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>改成大写后再观察结果，可以正常序列化。</p>
<img src="/2018/10/22/golang-json%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B3%A8%E6%84%8F%E7%82%B9/2_%E5%BA%8F%E5%88%97%E5%8C%96%E6%AD%A3%E5%B8%B8%E7%9A%84%E7%BB%93%E6%9E%9C.png" class="" title="image">

<h2 id="序列化到json后改成小写"><a href="#序列化到json后改成小写" class="headerlink" title="序列化到json后改成小写"></a>序列化到json后改成小写</h2><p>对于json串，很多人喜欢全小写，对于大写开头的key感觉很刺眼，我们继续改进：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">type Position struct &#123;</span><br><span class="line">    X int `json:&quot;x&quot;`</span><br><span class="line">    Y int `json:&quot;y&quot;`</span><br><span class="line">    Z int `json:&quot;z&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type Student struct &#123;</span><br><span class="line">    Name string `json:&quot;name&quot;`</span><br><span class="line">    Sex string `json:&quot;sex&quot;`</span><br><span class="line">    Age int `json:&quot;age&quot;`</span><br><span class="line">    Posi Position `json:&quot;position&quot;`</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再次运行程序，结果是我们期望的，打印如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Init:srcSlice is : [&#123;zhangsan male 20 &#123;10 20 30&#125;&#125; &#123;lisi female 18 &#123;15 10 20&#125;&#125;]</span><br><span class="line">After serialize, data :</span><br><span class="line"> [&#123;&quot;name&quot;:&quot;zhangsan&quot;,&quot;sex&quot;:&quot;male&quot;,&quot;age&quot;:20,&quot;position&quot;:&#123;&quot;x&quot;:10,&quot;y&quot;:20,&quot;z&quot;:30&#125;&#125;,&#123;&quot;name&quot;:&quot;lisi&quot;,&quot;sex&quot;:&quot;female&quot;,&quot;age&quot;:18,&quot;position&quot;:&#123;&quot;x&quot;:15,&quot;y&quot;:10,&quot;z&quot;:20&#125;&#125;]</span><br><span class="line">Deserialize:dstSlice is : [&#123;zhangsan male 20 &#123;10 20 30&#125;&#125; &#123;lisi female 18 &#123;15 10 20&#125;&#125;]</span><br></pre></td></tr></table></figure>

<h2 id="omitempty"><a href="#omitempty" class="headerlink" title="omitempty"></a>omitempty</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">type Dog struct &#123;</span><br><span class="line">	Breed string</span><br><span class="line">	WeightKg int</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">	d := Dog&#123;</span><br><span class="line">		Breed:    &quot;dalmation&quot;,</span><br><span class="line">		WeightKg: 45,</span><br><span class="line">	&#125;</span><br><span class="line">	b, _ := json.Marshal(d)</span><br><span class="line">	fmt.Println(string(b))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>output:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&quot;Breed&quot;:&quot;dalmation&quot;,&quot;WeightKg&quot;:45&#125;</span><br></pre></td></tr></table></figure>

<p>如果对dog没有设置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">func main() &#123;</span><br><span class="line">	d := Dog&#123;</span><br><span class="line">		Breed:    &quot;pug&quot;,</span><br><span class="line">	&#125;</span><br><span class="line">	b, _ := json.Marshal(d)</span><br><span class="line">	fmt.Println(string(b))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这会输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&quot;Breed&quot;:&quot;pug&quot;,&quot;WeightKg&quot;:0&#125;</span><br></pre></td></tr></table></figure>

<p>即使WeightKg的值是未知的。</p>
<p>更好的方式是使“WeightKg”为null, 或者根本没有这个值。</p>
<p>omitempty tag 正好可以起到这个作用.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">type Dog struct &#123;</span><br><span class="line">	Breed    string</span><br><span class="line">	// The first comma below is to separate the name tag from the omitempty tag </span><br><span class="line">	WeightKg int `json:&quot;,omitempty&quot;`</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这会输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&quot;Breed&quot;:&quot;pug&quot;&#125;</span><br></pre></td></tr></table></figure>

<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://www.jianshu.com/p/e737cb26c141">Golang初学者易犯的三种错误</a></p>
<p><a href="https://www.sohamkamani.com/blog/golang/2018-07-19-golang-omitempty/">Go’s “omitempty” explained</a></p>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title>golang中的map并发读写问题</title>
    <url>/2020/03/08/golang%E4%B8%AD%E7%9A%84map%E5%B9%B6%E5%8F%91%E8%AF%BB%E5%86%99%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h1 id="map不是并发安全的"><a href="#map不是并发安全的" class="headerlink" title="map不是并发安全的"></a>map不是并发安全的</h1><p>官方的<a href="https://golang.org/doc/faq#atomic_maps">faq</a>里有说明，考虑到有性能损失，map没有设计成原子操作，在并发读写时会有问题。</p>
<blockquote>
<p>Map access is unsafe only when updates are occurring. As long as all goroutines are only reading—looking up elements in the map, including iterating through it using a for range loop—and not changing the map by assigning to elements or doing deletions, it is safe for them to access the map concurrently without synchronization.</p>
</blockquote>
<span id="more"></span>

<p>查看源码，进一步立即实现机制</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">const (</span><br><span class="line">  ...</span><br><span class="line">	hashWriting  = 4 // a goroutine is writing to the map</span><br><span class="line">	...</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">type hmap struct &#123;</span><br><span class="line">	...</span><br><span class="line">	flags     uint8</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">map是检查是否有另外线程修改h.flag来判断，是否有并发问题。</span><br><span class="line"></span><br><span class="line">// 在更新map的函数里检查并发写</span><br><span class="line">	if h.flags&amp;hashWriting == 0 &#123;</span><br><span class="line">		throw(&quot;concurrent map writes&quot;)</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">// 在读map的函数里检查是否有并发写</span><br><span class="line">	if h.flags&amp;hashWriting != 0 &#123;</span><br><span class="line">		throw(&quot;concurrent map read and map write&quot;)</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>测试并发问题的例子：一个goroutine不停地写，另一个goroutine不停地读</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">	&quot;fmt&quot;</span><br><span class="line">	&quot;time&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">	c := make(map[string]int)</span><br><span class="line">	go func() &#123; //开一个goroutine写map</span><br><span class="line">		for j := 0; j &lt; 1000000; j++ &#123;</span><br><span class="line">			c[fmt.Sprintf(&quot;%d&quot;, j)] = j</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	go func() &#123; //开一个goroutine读map</span><br><span class="line">		for j := 0; j &lt; 1000000; j++ &#123;</span><br><span class="line">			fmt.Println(c[fmt.Sprintf(&quot;%d&quot;, j)])</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	time.Sleep(time.Second * 20)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>立马产生错误</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0</span><br><span class="line">fatal error: concurrent map read and map write</span><br><span class="line"></span><br><span class="line">goroutine 19 [running]:</span><br><span class="line">runtime.throw(0x10d6ea4, 0x21)</span><br><span class="line">        /usr/local/go/src/runtime/panic.go:774 +0x72 fp=0xc00009aef0 sp=0xc00009aec0 pc=0x10299c2</span><br><span class="line">runtime.mapaccess1_faststr(0x10b41e0, 0xc000066180, 0x116ae71, 0x1, 0x1)</span><br><span class="line">        /usr/local/go/src/runtime/map_faststr.go:21 +0x44f fp=0xc00009af60 sp=0xc00009aef0 pc=0x100ffff</span><br><span class="line">main.main.func2(0xc000066180)</span><br></pre></td></tr></table></figure>

<h1 id="加sync-RWMutex来保护map"><a href="#加sync-RWMutex来保护map" class="headerlink" title="加sync.RWMutex来保护map"></a>加<a href="https://golang.org/pkg/sync/#RWMutex">sync.RWMutex</a>来保护map</h1><p>This statement declares a <code>counter</code> variable that is an anonymous struct containing a map and an embedded <code>sync.RWMutex</code>.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">var counter = struct&#123;</span><br><span class="line">    sync.RWMutex</span><br><span class="line">    m map[string]int</span><br><span class="line">&#125;&#123;m: make(map[string]int)&#125;</span><br></pre></td></tr></table></figure>

<p>To read from the counter, take the read lock:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">counter.RLock()</span><br><span class="line">n := counter.m[&quot;some_key&quot;]</span><br><span class="line">counter.RUnlock()</span><br><span class="line">fmt.Println(&quot;some_key:&quot;, n)</span><br></pre></td></tr></table></figure>

<p>To write to the counter, take the write lock:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">counter.Lock()</span><br><span class="line">counter.m[&quot;some_key&quot;]++</span><br><span class="line">counter.Unlock()</span><br></pre></td></tr></table></figure>

<p>针对上面有并发问题的测试例子，可以修改成以下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">	&quot;fmt&quot;</span><br><span class="line">	&quot;sync&quot;</span><br><span class="line">	&quot;time&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">	var c = struct &#123;</span><br><span class="line">		sync.RWMutex</span><br><span class="line">		m map[string]int</span><br><span class="line">	&#125;&#123;m: make(map[string]int)&#125;</span><br><span class="line"></span><br><span class="line">	go func() &#123; //开一个goroutine写map</span><br><span class="line">		for j := 0; j &lt; 1000000; j++ &#123;</span><br><span class="line">			c.Lock()</span><br><span class="line">			c.m[fmt.Sprintf(&quot;%d&quot;, j)] = j</span><br><span class="line">			c.Unlock()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	go func() &#123; //开一个goroutine读map</span><br><span class="line">		for j := 0; j &lt; 1000000; j++ &#123;</span><br><span class="line">			c.RLock()</span><br><span class="line">			fmt.Println(c.m[fmt.Sprintf(&quot;%d&quot;, j)])</span><br><span class="line">			c.RUnlock()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;()</span><br><span class="line">	time.Sleep(time.Second * 20)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="sync-Map"><a href="#sync-Map" class="headerlink" title="sync.Map"></a>sync.Map</h1><p>sync.Map 是官方出品的并发安全的 map，他在内部使用了大量的原子操作来存取键和值，并使用了 read 和 dirty 二个原生 map 作为存储介质，具体实现流程可阅读相关源码。</p>
<p>参考：</p>
<p><a href="https://learnku.com/articles/27691">https://learnku.com/articles/27691</a></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><p><a href="https://blog.golang.org/go-maps-in-action">The Go Blog - Go maps in action</a></p>
</li>
<li><p><a href="https://golang.org/doc/faq#atomic_maps">Why are map operations not defined to be atomic?</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>map</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>goroutine调度器</title>
    <url>/2019/07/28/goroutine%E8%B0%83%E5%BA%A6%E5%99%A8/</url>
    <content><![CDATA[<h1 id="goroutine调度器简介"><a href="#goroutine调度器简介" class="headerlink" title="goroutine调度器简介"></a>goroutine调度器简介</h1><p>G-P-M模型，下面这张图很直观。</p>
<img src="/2019/07/28/goroutine%E8%B0%83%E5%BA%A6%E5%99%A8/goroutine-scheduler-model.png" class="" title="image">
<span id="more"></span>
<p>具体内容，参考以下链接，已经说得很详细，不再重复。</p>
<ul>
<li><a href="http://morsmachine.dk/go-scheduler">The Go scheduler</a></li>
<li><a href="https://tonybai.com/2017/06/23/an-intro-about-goroutine-scheduler/">也谈goroutine调度器</a></li>
<li><a href="https://segmentfault.com/a/1190000015464889">Goroutine并发调度模型深度解析之手撸一个协程池</a></li>
<li>源码阅读的话，推荐国内的雨痕老师在其《Go语言学习笔记》</li>
</ul>
<h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line">import &quot;fmt&quot;</span><br><span class="line">import &quot;time&quot;</span><br><span class="line">import &quot;runtime&quot;</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    var x int</span><br><span class="line">    fmt.Println(&quot;GOMAXPROCS =&quot;, runtime.GOMAXPROCS(0) )</span><br><span class="line">    threads := runtime.GOMAXPROCS(0)</span><br><span class="line">    for i := 0; i &lt; threads; i++ &#123;</span><br><span class="line">        go func() &#123;</span><br><span class="line">            for &#123; x++ &#125;</span><br><span class="line">        &#125;()</span><br><span class="line">    &#125;</span><br><span class="line">    fmt.Println(&quot;Before time.Sleep&quot;)</span><br><span class="line">    time.Sleep(time.Second)</span><br><span class="line">    fmt.Println(&quot;After time.Sleep&quot;)</span><br><span class="line">    fmt.Println(&quot;x =&quot;, x)  //程序能运行到这里吗？</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行之后，发现main函数运行到Sleep之后，就卡住了，是什么原因呢？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[patrickxu@vm1 src]$ ./test</span><br><span class="line">GOMAXPROCS = 8</span><br><span class="line">Before time.Sleep</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>因为main函数在执行time.Sleep(time.Second)后，main函数本身占据的P(G-P-M模型中的P)也被for{ x++ }一直占着,抢占的机制。这下总共8个P，全部一直被for { x++ }占着，永远轮不到主函数了。</p>
<p>runtime.GOMAXPROCS(0)默认等于CPU核数8。每个P可以分配到一个M(即线程)，总共2 + runtime.GOMAXPROCS(0) = 10个。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[patrickxu@vm1 ~]$ ps -eLf | grep test</span><br><span class="line">519      26778 26325 26778  0   10 00:18 pts/42   00:00:00 ./test</span><br><span class="line">519      26778 26325 26779  0   10 00:18 pts/42   00:00:00 ./test</span><br><span class="line">519      26778 26325 26780 99   10 00:18 pts/42   00:02:28 ./test</span><br><span class="line">519      26778 26325 26781 99   10 00:18 pts/42   00:02:28 ./test</span><br><span class="line">519      26778 26325 26782 99   10 00:18 pts/42   00:02:28 ./test</span><br><span class="line">519      26778 26325 26783 99   10 00:18 pts/42   00:02:28 ./test</span><br><span class="line">519      26778 26325 26784 99   10 00:18 pts/42   00:02:28 ./test</span><br><span class="line">519      26778 26325 26785 99   10 00:18 pts/42   00:02:27 ./test</span><br><span class="line">519      26778 26325 26786 99   10 00:18 pts/42   00:02:28 ./test</span><br><span class="line">519      26778 26325 26787 99   10 00:18 pts/42   00:02:28 ./test</span><br></pre></td></tr></table></figure>


<p>如果把time.Sleep(time.Second)去掉后，马上全部执行结束。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[patrickxu@vm1 src]$ ./test</span><br><span class="line">GOMAXPROCS = 8</span><br><span class="line">x = 0</span><br></pre></td></tr></table></figure>
<p>当然也可以把GOMAXPROCS设打，比如threads := runtime.GOMAXPROCS(10)。有足够多的P，也可以运行完。</p>
]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>goroutine</tag>
        <tag>调度</tag>
      </tags>
  </entry>
  <entry>
    <title>nodejs获取客户端IP Address(转)</title>
    <url>/2018/12/14/nodejs%E8%8E%B7%E5%8F%96%E5%AE%A2%E6%88%B7%E7%AB%AFIP-Address/</url>
    <content><![CDATA[<p>在网上看见很多问node.js如何获取客户端IP,所以记录下来，以供大家参考。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">function getClientIp(req) &#123;</span><br><span class="line">        return req.headers[&#x27;x-forwarded-for&#x27;] ||</span><br><span class="line">        req.connection.remoteAddress ||</span><br><span class="line">        req.socket.remoteAddress ||</span><br><span class="line">        req.connection.socket.remoteAddress;</span><br><span class="line">    &#125;;</span><br></pre></td></tr></table></figure>

<p>代码，第一段判断是否有反向代理IP(头信息：x-forwarded-for)，再判断connection的远程IP，以及后端的socket的IP。</p>
<p><a href="https://www.cnblogs.com/whitewolf/p/3572724.html">nodejs获取客户端IP Address</a></p>
]]></content>
  </entry>
  <entry>
    <title>kubeadm部署single control-plane k8s集群</title>
    <url>/2019/12/11/kubeadm%E9%83%A8%E7%BD%B2single-control-plane-k8s%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1 id="kubeadm介绍"><a href="#kubeadm介绍" class="headerlink" title="kubeadm介绍"></a>kubeadm介绍</h1><blockquote>
<p>Kubeadm is a tool built to provide kubeadm init and kubeadm join as best-practice “fast paths” for creating Kubernetes clusters.</p>
</blockquote>
<ul>
<li>kubeadm init  创建一个 Master节点 </li>
<li>kubeadm join 将一个 Node 节点加入到当前集群中</li>
</ul>
<span id="more"></span>

<h1 id="Before-you-begin"><a href="#Before-you-begin" class="headerlink" title="Before you begin"></a>Before you begin</h1><p>在开始之前，部署Kubernetes集群机器需要满足以下几个条件：</p>
<ul>
<li>One or more machines running a deb/rpm-compatible OS, for example Ubuntu or CentOS</li>
<li>2 GB or more of RAM per machine. Any less leaves little room for your apps.</li>
<li>2 CPUs or more on the control-plane node</li>
<li>Full network connectivity among all machines in the cluster. A public or private network is fine.</li>
<li>Swap off</li>
</ul>
<p>实际的机器参数如下：</p>
<table>
<thead>
<tr>
<th align="center">节点主机名</th>
<th align="center">IP</th>
<th align="center">os</th>
<th align="center">类型</th>
</tr>
</thead>
<tbody><tr>
<td align="center">k8snode01</td>
<td align="center">10.8.141.79</td>
<td align="center">K8s node节点1</td>
<td align="center">centos 7.6</td>
</tr>
<tr>
<td align="center">k8snode02</td>
<td align="center">10.8.177.123</td>
<td align="center">K8s node节点2</td>
<td align="center">centos 7.6</td>
</tr>
<tr>
<td align="center">k8snode03</td>
<td align="center">10.8.177.192</td>
<td align="center">K8s node节点3</td>
<td align="center">centos 7.6</td>
</tr>
<tr>
<td align="center">k8smaster01</td>
<td align="center">10.8.112.141</td>
<td align="center">K8s master节点1</td>
<td align="center">centos 7.6</td>
</tr>
</tbody></table>
<h1 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 关闭swap：</span><br><span class="line"># swapoff -a  # 临时</span><br><span class="line"># vim /etc/fstab  # 永久</span><br><span class="line"></span><br><span class="line">2 set hostname</span><br><span class="line"># Add host domain name.</span><br><span class="line">cat &gt;&gt; /etc/hosts &lt;&lt; EOF</span><br><span class="line">10.8.112.141 k8smaster01</span><br><span class="line">10.8.141.79 k8snode01</span><br><span class="line">10.8.177.123 k8snode02</span><br><span class="line">10.8.177.192 k8snode03</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">3 互配信任</span><br><span class="line">ssh-keygen -f ~/.ssh/id_rsa -N &#x27;&#x27;</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub root@k8smaster01</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub root@k8snode01</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub root@k8snode02</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub root@k8snode03</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="所有节点安装Docker-kubeadm-kubelet"><a href="#所有节点安装Docker-kubeadm-kubelet" class="headerlink" title="所有节点安装Docker/kubeadm/kubelet"></a>所有节点安装Docker/kubeadm/kubelet</h1><h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><p>参考<a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">Container runtimes</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#/bin/bash</span><br><span class="line"></span><br><span class="line"># Install Docker CE</span><br><span class="line">## Set up the repository</span><br><span class="line">### Install required packages.</span><br><span class="line">yum install yum-utils device-mapper-persistent-data lvm2</span><br><span class="line"></span><br><span class="line">### Add Docker repository.</span><br><span class="line">yum-config-manager \</span><br><span class="line">  --add-repo \</span><br><span class="line">  https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line">## Install Docker CE.</span><br><span class="line">yum update &amp;&amp; yum install docker-ce-18.06.2.ce</span><br><span class="line"></span><br><span class="line">## Create /etc/docker directory.</span><br><span class="line">mkdir /etc/docker</span><br><span class="line"></span><br><span class="line"># Setup daemon.</span><br><span class="line">cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">mkdir -p /etc/systemd/system/docker.service.d</span><br><span class="line"></span><br><span class="line"># Restart Docker</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br><span class="line">systemctl enable docker.service</span><br></pre></td></tr></table></figure>

<h2 id="Installing-kubeadm"><a href="#Installing-kubeadm" class="headerlink" title="Installing kubeadm"></a>Installing kubeadm</h2><p>参考<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">Installing kubeadm</a></p>
<p>这里安装指定版本1.16.3版本的kubeadm / kubelet / kubectl</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#/bin/bash</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=permissive/&#x27; /etc/selinux/config</span><br><span class="line"></span><br><span class="line">yum install -y kubeadm-1.16.3-0.x86_64 kubelet-1.16.3-0.x86_64 kubectl-1.16.3-0.x86_64 --disableexcludes=kubernetes</span><br><span class="line"></span><br><span class="line">systemctl enable --now kubelet</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Some users on RHEL/CentOS 7 have reported issues with traffic being routed incorrectly due to iptables being bypassed. You should ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config, e.g.</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br></pre></td></tr></table></figure>

<p><em>提示：所有Master+Worker节点均需要如上操作。</em></p>
<h1 id="Initializing-your-control-plane-node"><a href="#Initializing-your-control-plane-node" class="headerlink" title="Initializing your control-plane node"></a>Initializing your control-plane node</h1><p>参考<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a single control-plane cluster with kubeadm</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#/bin/bash</span><br><span class="line"></span><br><span class="line">kubeadm init \</span><br><span class="line">--kubernetes-version v1.16.3 \</span><br><span class="line">--pod-network-cidr=10.244.0.0/16 \</span><br><span class="line">--service-cidr=10.1.0.0/16 \</span><br><span class="line">--apiserver-advertise-address=0.0.0.0</span><br></pre></td></tr></table></figure>

<p>各个参数的意义,参考<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/">kubeadm init</a>：</p>
<pre><code>--kubernetes-version 正在使用的Kubernetes程序组件的版本号，需要与kubelet的版本号相同。
--pod-network-cidr pod网络的IP地址范围，为CIDR格式；使用flannel网络插件时，默认地址为10.244.0.0/16。
--service-cidr Default: &quot;10.96.0.0/12&quot; Use alternative range of IP address for service VIPs。
--apiserver-advertise-address The IP address the API Server will advertise it&#39;s listening on. If not set the default network interface will be used.
</code></pre>
<p>以下图中是init执行的各个步骤：    </p>
<img src="/2019/12/11/kubeadm%E9%83%A8%E7%BD%B2single-control-plane-k8s%E9%9B%86%E7%BE%A4/kube_init_steps.png" class="" title="image">

<p>最后返回结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 10.8.112.141:6443 --token c3gynf.fvcwcwlla88jfyvc \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:c23a11863be652316092e4cda59d0c56267506b4636da0e747818fd864c133f6</span><br></pre></td></tr></table></figure>

<p>master上执行上面的步骤：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>
<p><em>提示：在Master需要如上操作。</em></p>
<h1 id="部署网络插件"><a href="#部署网络插件" class="headerlink" title="部署网络插件"></a>部署网络插件</h1><p>这里采用的是flannel网络插件，kube-flannel.yml文件可以从这里获取。<a href="https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml%E3%80%82%E7%84%B6%E5%90%8E%E6%89%A7%E8%A1%8Ckubectl">https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml。然后执行kubectl</a> apply,部署网络插件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Once a pod network has been installed, you can confirm that it is working by checking that the CoreDNS pod is Running in the output of kubectl get pods –all-namespaces.<br>And once the CoreDNS pod is up and running, you can continue by joining your nodes.</p>
</blockquote>
<p>部署成功后的结果</p>
<img src="/2019/12/11/kubeadm%E9%83%A8%E7%BD%B2single-control-plane-k8s%E9%9B%86%E7%BE%A4/flannel%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E9%83%A8%E7%BD%B2.png" class="" title="image">


<h1 id="添加Node至集群中"><a href="#添加Node至集群中" class="headerlink" title="添加Node至集群中"></a>添加Node至集群中</h1><p>参考<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/">kubeadm-join</a></p>
<p><em>提示：在Node节点操作。</em></p>
<p>在kubeadm init的返回结果中，返回了kubeadm join所需的参数，包括token和discovery-token-ca-cert-hash。</p>
<p>join后返回的结果：</p>
<img src="/2019/12/11/kubeadm%E9%83%A8%E7%BD%B2single-control-plane-k8s%E9%9B%86%E7%BE%A4/kube_join_steps.png" class="" title="image">

<p>如果忘记了token,可以通过“kubeadm token list”获取</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-112-141 ~]# kubeadm token list</span><br><span class="line">TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS</span><br><span class="line">c3gynf.fvcwcwlla88jfyvc   2h        2019-12-11T13:50:15+08:00   authentication,signing   The default bootstrap token generated by &#x27;kubeadm init&#x27;.   system:bootstrappers:kubeadm:default-node-token</span><br></pre></td></tr></table></figure>

<p>CA公钥的哈希值，可以通过以下命令获取：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-112-141 ~]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;</span><br><span class="line">c23a11863be652316092e4cda59d0c56267506b4636da0e747818fd864c133f6</span><br></pre></td></tr></table></figure>

<p>tocken默认在24 hours后会过期。可以重新生成：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubeadm token create --print-join-command</span><br></pre></td></tr></table></figure>

<img src="/2019/12/11/kubeadm%E9%83%A8%E7%BD%B2single-control-plane-k8s%E9%9B%86%E7%BE%A4/kubeadm_token_create.png" class="" title="image">

<h1 id="从集群中删除Node-Tear-down"><a href="#从集群中删除Node-Tear-down" class="headerlink" title="从集群中删除Node(Tear down)"></a>从集群中删除Node(Tear down)</h1><p>To undo what kubeadm did, you should first drain the node and make sure that the node is empty before shutting it down.</p>
<p>Talking to the control-plane node with the appropriate credentials, run:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets</span><br><span class="line">kubectl delete node &lt;node name&gt;</span><br></pre></td></tr></table></figure>
<p>Then, on the node being removed, reset all kubeadm installed state:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubeadm reset</span><br></pre></td></tr></table></figure>

<p>The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X</span><br></pre></td></tr></table></figure>

<p>If you want to reset the IPVS tables, you must run the following command:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ipvsadm -C</span><br></pre></td></tr></table></figure>

<h1 id="获取集群状态信息"><a href="#获取集群状态信息" class="headerlink" title="获取集群状态信息"></a>获取集群状态信息</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-112-141 ~]# kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https://10.8.112.141:6443</span><br><span class="line">KubeDNS is running at https://10.8.112.141:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;.</span><br></pre></td></tr></table></figure>

<p>kubectl get cs返回AGE unknown，原因未知；临时方案是用kubectl get cs -o yaml代替。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-112-141 ~]# kubectl get cs</span><br><span class="line">NAME                 AGE</span><br><span class="line">controller-manager   &lt;unknown&gt;</span><br><span class="line">scheduler            &lt;unknown&gt;</span><br><span class="line">etcd-0               &lt;unknown&gt;</span><br><span class="line">[root@10-8-112-141 ~]# kubectl get cs -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">items:</span><br><span class="line">- apiVersion: v1</span><br><span class="line">  conditions:</span><br><span class="line">  - message: ok</span><br><span class="line">    status: &quot;True&quot;</span><br><span class="line">    type: Healthy</span><br><span class="line">  kind: ComponentStatus</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: null</span><br><span class="line">    name: scheduler</span><br><span class="line">    selfLink: /api/v1/componentstatuses/scheduler</span><br><span class="line">- apiVersion: v1</span><br><span class="line">  conditions:</span><br><span class="line">  - message: ok</span><br><span class="line">    status: &quot;True&quot;</span><br><span class="line">    type: Healthy</span><br><span class="line">  kind: ComponentStatus</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: null</span><br><span class="line">    name: controller-manager</span><br><span class="line">    selfLink: /api/v1/componentstatuses/controller-manager</span><br><span class="line">- apiVersion: v1</span><br><span class="line">  conditions:</span><br><span class="line">  - message: &#x27;&#123;&quot;health&quot;:&quot;true&quot;&#125;&#x27;</span><br><span class="line">    status: &quot;True&quot;</span><br><span class="line">    type: Healthy</span><br><span class="line">  kind: ComponentStatus</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: null</span><br><span class="line">    name: etcd-0</span><br><span class="line">    selfLink: /api/v1/componentstatuses/etcd-0</span><br><span class="line">kind: List</span><br><span class="line">metadata:</span><br><span class="line">  resourceVersion: &quot;&quot;</span><br><span class="line">  selfLink: &quot;&quot;</span><br></pre></td></tr></table></figure>

<h1 id="测试kubernetes集群"><a href="#测试kubernetes集群" class="headerlink" title="测试kubernetes集群"></a>测试kubernetes集群</h1><p>在Kubernetes集群中创建一个pod，验证是否正常运行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># kubectl create deployment nginx --image=nginx</span><br><span class="line"># kubectl expose deployment nginx --port=80 --type=NodePort</span><br><span class="line">[root@10-8-112-141 ~]# kubectl get pod,svc -o wide</span><br><span class="line">NAME                         READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES</span><br><span class="line">pod/nginx-86c57db685-zbbnr   1/1     Running   0          12h   10.244.4.2   10-8-177-192   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE   SELECTOR</span><br><span class="line">service/kubernetes   ClusterIP   10.1.0.1     &lt;none&gt;        443/TCP        22h   &lt;none&gt;</span><br><span class="line">service/nginx        NodePort    10.1.41.49   &lt;none&gt;        80:31167/TCP   12h   app=nginx</span><br></pre></td></tr></table></figure>

<p>访问地址：<a href="http://ClusterIP:Port">http://ClusterIP:Port</a>, 如curl 10.1.41.49:80</p>
<p>或者 <a href="http://PodIP:Port">http://PodIP:Port</a>, 如curl 10.244.4.2:80    </p>
<p>或者 <a href="http://NodeIP:Port">http://NodeIP:Port</a>, curl 10.8.177.192:31167</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-112-141 ~]# curl 10.1.41.49:80</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>

<p>也可以进入部署的nginx</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl exec -it nginx-86c57db685-zbbnr sh</span><br></pre></td></tr></table></figure>


<h1 id="安装Dashboard"><a href="#安装Dashboard" class="headerlink" title="安装Dashboard"></a>安装Dashboard</h1><p>参考<a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Web UI (Dashboard)</a></p>
<h2 id="部署Dashboard"><a href="#部署Dashboard" class="headerlink" title="部署Dashboard"></a>部署Dashboard</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-112-141 k8s]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml</span><br></pre></td></tr></table></figure>

<p>部署完毕后, 执行kubectl get pods –all-namespaces查看pods状态</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@10-8-112-141 k8s]# kubectl get pods --all-namespaces | grep dashboard</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-fnwp4   1/1     Running   0          16s</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-b65488c4-l9kgm          1/1     Running   0          16s</span><br></pre></td></tr></table></figure>


<p>未完待续。。。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/">Overview of kubeadm</a></li>
<li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a single control-plane cluster with kubeadm</a></li>
<li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/">kubeadm init</a></li>
<li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/">kubeadm-join</a></li>
<li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Web UI (Dashboard)</a></li>
<li><a href="https://i4t.com/4400.html">kubeadm部署k8s集群</a></li>
<li><a href="https://www.cnblogs.com/itzgr/p/11982945.html">附012.Kubeadm部署高可用Kubernetes</a></li>
</ol>
]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title>一次go语言内存泄漏的排查过程</title>
    <url>/2018/12/11/%E4%B8%80%E6%AC%A1go%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E7%9A%84%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>线上一个go语言写的服务，在连续跑了10多天后，占用内存从最开始的20M左右，涨到了1个多G，有内存泄漏的问题，将整个排查过程和相应方法进行整理，做个记录。</p>
<img src="/2018/12/11/%E4%B8%80%E6%AC%A1go%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E7%9A%84%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/1_%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F.png" class="" title="image">

<h2 id="pprof"><a href="#pprof" class="headerlink" title="pprof"></a>pprof</h2><p>Go语言自带的pprof工具，是检测Golang开发应用性能的利器。</p>
<p>pprof 采样数据主要有三种获取方式:</p>
<ol>
<li>runtime/pprof: 手动调用runtime.StartCPUProfile或者runtime.StopCPUProfile等 API来生成和写入采样文件，灵活性高。参考文档<a href="https://blog.golang.org/profiling-go-programs">profiling-go-programs</a></li>
</ol>
<ol start="2">
<li><p>net/http/pprof: 通过 http 服务获取Profile采样文件，简单易用，适用于对应用程序的整体监控。通过runtime/pprof实现。这里推荐这种方法</p>
</li>
<li><p>go test: 通过 go test -bench . -cpuprofile prof.cpu生成采样文件 适用对函数进行针对性测试</p>
</li>
</ol>
<h2 id="net-http-pprof"><a href="#net-http-pprof" class="headerlink" title="net/http/pprof"></a>net/http/pprof</h2><p>线上的服务是一直运行的，用过“net/http/pprof”可以看到实时的状态，看到各个信息指标，这里主要介绍这种方法。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import _ &quot;net/http/pprof&quot;</span><br><span class="line">func main() &#123;</span><br><span class="line">	......</span><br><span class="line">	go func() &#123;</span><br><span class="line">		log.Println(http.ListenAndServe(&quot;localhost:8211&quot;, nil))</span><br><span class="line">	&#125;()</span><br><span class="line">	......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<img src="/2018/12/11/%E4%B8%80%E6%AC%A1go%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E7%9A%84%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/3_netpprof%E6%B5%8F%E8%A7%88%E5%99%A8%E6%88%AA%E5%9B%BE2.png" class="" title="image">

<p>之后可以通过 <a href="http://localhost:8211/debug/pprof/CMD%E8%8E%B7%E5%8F%96%E5%AF%B9%E5%BA%94%E7%9A%84%E9%87%87%E6%A0%B7%E6%95%B0%E6%8D%AE">http://localhost:8211/debug/pprof/CMD获取对应的采样数据</a></p>
<p><a href="https://golang.org/src/runtime/pprof/pprof.go?s=23119:23158#L730">pprof</a></p>
<p>Profile翻译为“剖面，侧面等”，可以理解为当前应用的的一个剖面。可以知道哪些地方耗费了cpu、memory。</p>
<p>// Each Profile has a unique name. A few profiles are predefined:</p>
<ol>
<li><pre><code>goroutine    - stack traces of all current goroutines
</code></pre>
</li>
<li>heap         - a sampling of memory allocations of live objects</li>
<li>allocs       - a sampling of all past memory allocations</li>
<li>threadcreate - stack traces that led to the creation of new OS threads</li>
<li>block        - stack traces that led to blocking on synchronization primitives</li>
<li>mutex        - stack traces of holders of contended mutexes</li>
</ol>
<ul>
<li>goroutine: 获取程序当前所有 goroutine 的堆栈信息。</li>
<li>heap: 包含每个 goroutine 分配大小，分配堆栈等。每分配 runtime.MemProfileRate(默认为512K) 个字节进行一次数据采样。</li>
<li>threadcreate: 获取导致创建 OS 线程的 goroutine 堆栈</li>
<li>block: 获取导致阻塞的 goroutine 堆栈(如 channel, mutex 等)，使用前需要先调用 runtime.SetBlockProfileRate</li>
<li>mutex: 获取导致 mutex 争用的 goroutine 堆栈，使用前需要先调用 runtime.SetMutexProfileFraction</li>
</ul>
<p><a href="https://golang.org/src/runtime/pprof/pprof.go?s=23119:23158#L730">pprof_debug</a></p>
<p>The debug parameter enables additional output.</p>
<p>Passing debug=0 prints only the hexadecimal addresses that pprof needs.</p>
<p>Passing debug=1 adds comments translating addresses to function names and line numbers, so that a programmer can read the profile without tools.</p>
<img src="/2018/12/11/%E4%B8%80%E6%AC%A1go%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E7%9A%84%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/4_netpprof_debug_1.png" class="" title="image">


<p>The predefined profiles may assign meaning to other debug values;</p>
<p>for example, when printing the “goroutine” profile, debug=2 means to print the goroutine stacks in the same form that a Go program uses when dying due to an unrecovered panic.</p>
<p>以上几种 Profile 可在 <a href="http://localhost:8211/debug/pprof/">http://localhost:8211/debug/pprof/</a> 中看到，除此之外，go pprof 的 CMD 还包括, </p>
<ul>
<li>cmdline: 获取程序的命令行启动参数</li>
<li>profile: 获取指定时间内(从请求时开始)的cpuprof，倒计时结束后自动返回。参数: seconds, 默认值为30。cpuprofile 每秒钟采样100次，收集当前运行的 goroutine 堆栈信息。</li>
<li>symbol: 用于将地址列表转换为函数名列表，地址通过’+’分隔，如 URL/debug/pprof?0x18d067f+0x17933e7</li>
<li>trace: 对应用程序进行执行追踪，参数: seconds, 默认值1s</li>
</ul>
<p>参数具体信息可以参考<a href="https://golang.org/pkg/net/http/pprof/">http_pprof</a></p>
<p>再回顾来看服务的问题，从图2中看到，goroutine的数量在一直增长；从图3看出，共起了414个goroutine, 其中395个是在调用FetchZkName。<br>然后再查看我们的代码，有哪里在调用go, 哪个goroutine会最终调用FetchZkName,就可以查到出问题的地方。</p>
<p>最终查到原因是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  // 10s去更新下zk节点</span><br><span class="line">ticker2 := time.NewTicker(time.Second * 10)</span><br><span class="line">go func() &#123;</span><br><span class="line">	for &#123;</span><br><span class="line">		select &#123;</span><br><span class="line">		case &lt;-ticker2.C:</span><br><span class="line">			UpdateInstanceFromZk()</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">// 里面的函数调用关系如下：</span><br><span class="line">UpdateInstanceFromZk --&gt; nameservice.InitNameService </span><br><span class="line">--&gt; updateNames --&gt; FetchZkName</span><br><span class="line"></span><br><span class="line">// FetchZkName函数里起了一个goroutine,封装了watcher，自己已实现了更新zk节点的功能</span><br><span class="line">   go func() &#123;</span><br><span class="line">	select &#123;</span><br><span class="line">	case ev := &lt;-ch:</span><br><span class="line">		uflog.INFO(&quot;cat watcher&quot;, ev)</span><br><span class="line">		if ev.Type == zk.EventNotWatching &amp;&amp; ev.State == zk.StateDisconnected &#123;</span><br><span class="line">			reConnect(connStr)</span><br><span class="line">		&#125;</span><br><span class="line">		nc.FetchZkName(connStr, shortName, fullName)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line">// 该goroutine不会退出，会一直watch。定时调用UpdateInstanceFromZk，会导致goroutine不断增加，内存泄漏。</span><br></pre></td></tr></table></figure>

<p>解决方案是：UpdateInstanceFromZk（）调用一次即可。</p>
<p>改进后，占用内存一直保持在20M左右，完美~</p>
<img src="/2018/12/11/%E4%B8%80%E6%AC%A1go%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E7%9A%84%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/6_%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E8%A7%A3%E5%86%B3.png" class="" title="image">

<h2 id="Go-Tool-PProf-分析工具"><a href="#Go-Tool-PProf-分析工具" class="headerlink" title="Go Tool PProf 分析工具"></a>Go Tool PProf 分析工具</h2><p>上面通过浏览器访问http服务的方式，需要有图形界面。而线上服务器通常是没有图形界面的，可以通过命令行方式来分析，或者将profile拷贝到本地，再生成调用关系图</p>
<h3 id="Go-Tool-PProf-命令行"><a href="#Go-Tool-PProf-命令行" class="headerlink" title="Go Tool PProf 命令行"></a>Go Tool PProf 命令行</h3><p>支持的cmd跟net/http/pprof里提到的一样</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">go tool pprof http://localhost:8211/debug/pprof/goroutine</span><br><span class="line">go tool pprof http://localhost:8211/debug/pprof/profile</span><br><span class="line">go tool pprof http://localhost:8211/debug/pprof/heap</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>以采集CPU耗时举例, </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># /root/go/bin/go tool pprof http://localhost:8211/debug/pprof/profile</span><br><span class="line">Fetching profile over HTTP from http://localhost:8211/debug/pprof/profile</span><br><span class="line">Saved profile in /root/pprof/pprof.uhost_server.samples.cpu.003.pb.gz</span><br><span class="line">File: uhost_server</span><br><span class="line">Type: cpu</span><br><span class="line">Time: Dec 11, 2018 at 6:12pm (CST)</span><br><span class="line">Duration: 30s, Total samples = 10ms (0.033%)</span><br><span class="line">Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)</span><br><span class="line">(pprof)</span><br></pre></td></tr></table></figure>

<p>会经历30s的采样时间，生成pprof.uhost_server.samples.cpu.003.pb.gz,这个可以拷贝到本地分析。</p>
<h4 id="最重要的分析命令是top-N"><a href="#最重要的分析命令是top-N" class="headerlink" title="最重要的分析命令是top N"></a>最重要的分析命令是top N</h4><p>topN 命令可以查出程序最耗 CPU 的调用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(pprof) top 5</span><br><span class="line">Showing nodes accounting for 70ms, 38.89% of 180ms total</span><br><span class="line">Showing top 5 nodes out of 83</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">      30ms 16.67% 16.67%       30ms 16.67%  syscall.Syscall</span><br><span class="line">      10ms  5.56% 22.22%       20ms 11.11%  fmt.Sprintf</span><br><span class="line">      10ms  5.56% 27.78%       30ms 16.67%  os.(*File).write</span><br><span class="line">      10ms  5.56% 33.33%       10ms  5.56%  runtime._ExternalCode</span><br><span class="line">      10ms  5.56% 38.89%       10ms  5.56%  runtime.epollwait</span><br></pre></td></tr></table></figure>
<ul>
<li>flat / flat% : 函数占用CPU的运行时间和百分比</li>
<li>sum% : 按列累加的累计占用CPU百分比</li>
<li>cum / cum% : 采样过程中， the function appeared (either running or waiting for a called function to return).</li>
</ul>
<h4 id="top-cum"><a href="#top-cum" class="headerlink" title="top -cum"></a>top -cum</h4><p>以累加的方式，看出消耗cpu的函数运行。可以和后面图形的方式做个对比</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(pprof) top10 -cum</span><br><span class="line">Showing nodes accounting for 40ms, 22.22% of 180ms total</span><br><span class="line">Showing top 10 nodes out of 83</span><br><span class="line">      flat  flat%   sum%        cum   cum%</span><br><span class="line">         0     0%     0%      140ms 77.78%  runtime.goexit</span><br><span class="line">         0     0%     0%       40ms 22.22%  uframework/log.RealWrite</span><br><span class="line">         0     0%     0%       40ms 22.22%  uframework/task.(*TCPTask).Run.func1</span><br><span class="line">         0     0%     0%       40ms 22.22%  uframework/task.TCPTaskFunc.ServeTCP</span><br><span class="line">         0     0%     0%       40ms 22.22%  uhost-go/uhost-scheduler/logic.getSuitableResource</span><br><span class="line">         0     0%     0%       30ms 16.67%  os.(*File).Write</span><br><span class="line">      10ms  5.56%  5.56%       30ms 16.67%  os.(*File).write</span><br><span class="line">      30ms 16.67% 22.22%       30ms 16.67%  syscall.Syscall</span><br><span class="line">         0     0% 22.22%       30ms 16.67%  uframework/message/protobuf/proto.(*Buffer).dec_slice_struct</span><br><span class="line">         0     0% 22.22%       30ms 16.67%  uframework/message/protobuf/proto.(*Buffer).dec_slice_struct_message</span><br></pre></td></tr></table></figure>

<h4 id="list-Func"><a href="#list-Func" class="headerlink" title="list Func"></a>list Func</h4><p>显示函数名以及每行代码的采样分析</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(pprof) list dec_slice_struct_message</span><br><span class="line">Total: 180ms</span><br><span class="line">ROUTINE ======================== uframework/message/protobuf/proto.(*Buffer).dec_slice_struct_message in /Users/patrick.xu/go/src/uframework/message/protobuf/proto/decode.go</span><br><span class="line">         0       30ms (flat, cum) 16.67% of Total</span><br><span class="line">         .          .    822:	return err</span><br><span class="line">         .          .    823:&#125;</span><br><span class="line">         .          .    824:</span><br><span class="line">         .          .    825:// Decode a slice of embedded messages.</span><br><span class="line">         .          .    826:func (o *Buffer) dec_slice_struct_message(p *Properties, base structPointer) error &#123;</span><br><span class="line">         .       30ms    827:	return o.dec_slice_struct(p, false, base)</span><br><span class="line">         .          .    828:&#125;</span><br><span class="line">         .          .    829:</span><br><span class="line">         .          .    830:// Decode a slice of embedded groups.</span><br><span class="line">         .          .    831:func (o *Buffer) dec_slice_struct_group(p *Properties, base structPointer) error &#123;</span><br><span class="line">         .          .    832:	return o.dec_slice_struct(p, true, base)</span><br></pre></td></tr></table></figure>

<h4 id="web-web-Func"><a href="#web-web-Func" class="headerlink" title="web / web Func"></a>web / web Func</h4><p>显示调用图 / 显示某个具体函数的调用图</p>
<p>mac上需要先安装graphviz <a href="http://www.graphviz.org/">http://www.graphviz.org/</a></p>
<p>homebrew安装完毕后运行 brew install graphviz即可</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  uhost-go git:(from_pxu) ✗ go tool pprof uhost_server pprof.uhost_server.samples.cpu.004.pb</span><br><span class="line">Entering interactive mode (type &quot;help&quot; for commands)</span><br><span class="line">(pprof) web</span><br><span class="line">(pprof) web getSuitableResource</span><br></pre></td></tr></table></figure>

<p>但是web命令生成的调用图往往是残缺的，不知道是什么原因。但找到下面更好的方式。</p>
<h2 id="pprof-1"><a href="#pprof-1" class="headerlink" title="pprof"></a>pprof</h2><p>As of Go 1.11, flamegraph visualizations are available in go tool pprof directly!</p>
<p>This will listen on :8081 and open a browser.<br>Change :8081 to a port of your choice.</p>
<p>$ go tool pprof -http=”:8081” [binary] [profile]</p>
<p>If you cannot use Go 1.11, you can get the latest pprof tool and use it instead:</p>
<p>Get the pprof tool directly</p>
<p>$ go get -u github.com/google/pprof</p>
<p>$ pprof -http=”:8081” [binary] [profile]</p>
<p>由于我的Mac上用的是go1.8.1版本，所用pprof命令。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  uhost-go git:(from_pxu) ✗ pprof uhost_server pprof.uhost_server.samples.cpu.004.pb</span><br><span class="line">uhost_server: parsing profile: unrecognized profile format</span><br><span class="line">Fetched 1 source profiles out of 2</span><br><span class="line">File: uhost_server</span><br><span class="line">Type: cpu</span><br><span class="line">Time: Dec 11, 2018 at 6:18pm (CST)</span><br><span class="line">Duration: 30s, Total samples = 180ms (  0.6%)</span><br><span class="line">Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)</span><br><span class="line">(pprof) web</span><br></pre></td></tr></table></figure>

<img src="/2018/12/11/%E4%B8%80%E6%AC%A1go%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E7%9A%84%E6%8E%92%E6%9F%A5%E8%BF%87%E7%A8%8B/5_cpu%E8%80%97%E6%97%B6%E8%B0%83%E5%BA%A6%E5%9B%BE.png" class="" title="image">

<h2 id="其他工具"><a href="#其他工具" class="headerlink" title="其他工具"></a>其他工具</h2><ul>
<li>delve<br><a href="https://github.com/derekparker/delve">https://github.com/derekparker/delve</a></li>
<li>go-torch 生成火焰图<br><a href="https://github.com/uber/go-torch">https://github.com/uber/go-torch</a></li>
</ul>
<p>以后用到了再整理，这里mark下</p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://blog.golang.org/profiling-go-programs">Profiling Go Programs</a></p>
<p><a href="https://golang.org/src/runtime/pprof/pprof.go?s=23119:23158#L730">Source file src/runtime/pprof/pprof.go</a></p>
<p><a href="http://wudaijun.com/2018/04/go-pprof/">go pprof性能分析</a></p>
<p><a href="https://cizixs.com/2017/09/11/profiling-golang-program/">使用pprof和火焰图调试golang应用</a></p>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>一次mongo慢查询的优化过程</title>
    <url>/2019/07/24/%E4%B8%80%E6%AC%A1mongo%E6%85%A2%E6%9F%A5%E8%AF%A2%E7%9A%84%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<p>今天发现查询mongo时容易慢查询，现在将整个排查及优化过程,总结记录如下。</p>
<h1 id="Database-Profiler介绍"><a href="#Database-Profiler介绍" class="headerlink" title="Database Profiler介绍"></a>Database Profiler介绍</h1><p>类似于mysql,Profiler可以实现慢查询日志的功能，它把数据收集在system.profile集合里。该集合是一个capped collection（有上限的集合）。Profiler默认是关闭的，可以在一个database或者一个instance上设置开启，设置不同的profiling levels.<br>具体描述见<a href="https://docs.mongodb.com/manual/tutorial/manage-the-database-profiler/">Database Profiler</a>。</p>
<span id="more"></span>

<h1 id="慢查询分析流程"><a href="#慢查询分析流程" class="headerlink" title="慢查询分析流程"></a>慢查询分析流程</h1><p>慢查询日志一般作为优化步骤里的第一步。通过慢查询日志，定位每一条语句的查询时间。比如超过了200ms，那么查询超过200ms的语句需要优化。然后它通过 .explain() 解析影响行数是不是过大，所以导致查询语句超过200ms。<br>所以优化步骤一般就是：</p>
<ol>
<li>用慢查询日志（system.profile）找到超过200ms的语句</li>
<li>然后再通过.explain()解析影响行数，分析为什么超过200ms </li>
<li>决定是不是需要添加索引</li>
</ol>
<h1 id="开启慢查询"><a href="#开启慢查询" class="headerlink" title="开启慢查询"></a>开启慢查询</h1><h2 id="Profiling-Levels"><a href="#Profiling-Levels" class="headerlink" title="Profiling Levels"></a>Profiling Levels</h2><table>
<thead>
<tr>
<th align="center">Level</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">The profiler is off and does not collect any data. This is the default profiler level.</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">The profiler collects data for operations that take longer than the value of slowms.</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">The profiler collects data for all operations.</td>
</tr>
</tbody></table>
<h2 id="Enable-and-Configure-Database-Profiling"><a href="#Enable-and-Configure-Database-Profiling" class="headerlink" title="Enable and Configure Database Profiling"></a>Enable and Configure Database Profiling</h2><h3 id="方式1-from-the-mongo-shell"><a href="#方式1-from-the-mongo-shell" class="headerlink" title="方式1 - from the mongo shell"></a>方式1 - from the mongo shell</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 查看状态：级别和时间</span><br><span class="line">uhost:PRIMARY&gt; db.getProfilingStatus()</span><br><span class="line">&#123; &quot;was&quot; : 0, &quot;slowms&quot; : 100 &#125;</span><br><span class="line"># 设置Profiling Level，返回的是之前设置的Level,返回&quot;ok&quot;:1表示设置成功</span><br><span class="line">uhost:PRIMARY&gt; db.setProfilingLevel(1)</span><br><span class="line">&#123; &quot;was&quot; : 0, &quot;slowms&quot; : 100, &quot;ok&quot; : 1 &#125;</span><br><span class="line">uhost:PRIMARY&gt; db.getProfilingStatus()</span><br><span class="line">&#123; &quot;was&quot; : 1, &quot;slowms&quot; : 100 &#125;</span><br><span class="line"># 设置Profiling Level和slowms</span><br><span class="line">uhost:PRIMARY&gt; db.setProfilingLevel(1,200)</span><br><span class="line">&#123; &quot;was&quot; : 1, &quot;slowms&quot; : 100, &quot;ok&quot; : 1 &#125;</span><br><span class="line">uhost:PRIMARY&gt; db.getProfilingStatus()</span><br><span class="line">&#123; &quot;was&quot; : 1, &quot;slowms&quot; : 200 &#125;</span><br><span class="line"></span><br><span class="line"># New in version 3.6，支持设置sampleRate。</span><br><span class="line"># 默认是1，以下设置0.42，表示采样42%的慢查询</span><br><span class="line">db.setProfilingLevel(1, &#123; sampleRate: 0.42 &#125;)</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<ol>
<li><font color=#FF0000>每次设置之后，返回的是之前的状态</font></li>
<li><font color=#FF0000>每次设置仅对当前所在的database有效</font></li>
</ol>
<h3 id="方式2-在mongo启动时设置"><a href="#方式2-在mongo启动时设置" class="headerlink" title="方式2 - 在mongo启动时设置"></a>方式2 - 在mongo启动时设置</h3><p>作为命令行参数传入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mongod --profile 1 --slowms 15 --slowOpSampleRate 0.5</span><br></pre></td></tr></table></figure>
<p>还可以在配置文件里设置</p>
<p>具体设置参考<a href="https://docs.mongodb.com/manual/reference/command/profile/#dbcmd.profile">profile</a></p>
<h3 id="关闭Profiling"><a href="#关闭Profiling" class="headerlink" title="关闭Profiling"></a>关闭Profiling</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">uhost:PRIMARY&gt; db.setProfilingLevel(0)</span><br><span class="line">&#123; &quot;was&quot; : 1, &quot;slowms&quot; : 200, &quot;ok&quot; : 1 &#125;</span><br></pre></td></tr></table></figure>

<h2 id="Profiling的效率问题"><a href="#Profiling的效率问题" class="headerlink" title="Profiling的效率问题"></a>Profiling的效率问题</h2><p>这是我能搜到的说法，但是具体原理和数据我还没找到，官网也没看到介绍。</p>
<p>从其他地方复制过来的说法，待论证。”Profiling 功能肯定是会影响效率的，但是不太严重，原因是他使用的是system.profile 来记录，而system.profile 是一个capped collection 这种collection 在操作上有一些限制和特点，但是效率更高“</p>
<h1 id="慢查询结果分析"><a href="#慢查询结果分析" class="headerlink" title="慢查询结果分析"></a>慢查询结果分析</h1><h2 id="各项参数含义"><a href="#各项参数含义" class="headerlink" title="各项参数含义"></a>各项参数含义</h2><p>这里用的MongoDB版本是2.6, 具体可以参考 <a href="https://docs.mongodb.com/v2.6/reference/database-profiler/">https://docs.mongodb.com/v2.6/reference/database-profiler/</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">uhost:PRIMARY&gt; db.system.profile.find(&#123;&quot;ns&quot; : &quot;uhost.DiskAttachInfo&quot;,&quot;ts&quot;:&#123;$gt:new ISODate(&quot;2019-07-23T10:11:00Z&quot;)&#125;&#125;).pretty()</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">	&quot;op&quot; : &quot;query&quot;, #操作类型，有insert、query、update、remove、getmore、command</span><br><span class="line">	&quot;ns&quot; : &quot;uhost.DiskAttachInfo&quot;, #操作的collection</span><br><span class="line">	&quot;query&quot; : &#123;</span><br><span class="line">		&quot;backend_types&quot; : &#123;</span><br><span class="line">			&quot;$in&quot; : [</span><br><span class="line">				&quot;ssd&quot;</span><br><span class="line">			]</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,</span><br><span class="line">	&quot;ntoreturn&quot; : 0, #返回的数量</span><br><span class="line">	&quot;ntoskip&quot; : 0, #The number of documents the skip() method specified to skip.</span><br><span class="line">	&quot;nscanned&quot; : 520144, #MongoDB在索引中扫描以执行操作的文档数。 通常，如果nscanned远高于nredurned，则数据库会扫描许多对象以查找目标对象。 考虑创建一个索引来改善这一点。</span><br><span class="line">	&quot;nscannedObjects&quot; : 520144, # The number of documents that MongoDB scans from the collection in order to carry out the operation.</span><br><span class="line">	&quot;keyUpdates&quot; : 0,</span><br><span class="line">	&quot;numYield&quot; : 6,</span><br><span class="line">	&quot;lockStats&quot; : &#123;</span><br><span class="line">		&quot;timeLockedMicros&quot; : &#123;</span><br><span class="line">			&quot;r&quot; : NumberLong(1536711),</span><br><span class="line">			&quot;w&quot; : NumberLong(0)</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;timeAcquiringMicros&quot; : &#123;</span><br><span class="line">			&quot;r&quot; : NumberLong(913),</span><br><span class="line">			&quot;w&quot; : NumberLong(4)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;,</span><br><span class="line">	&quot;nreturned&quot; : 0, // 返回的文档数量</span><br><span class="line">	&quot;responseLength&quot; : 20, // 返回字节长度，如果这个数字很大，考虑值返回所需字段</span><br><span class="line">	&quot;millis&quot; : 820, #消耗的时间（毫秒）</span><br><span class="line">	&quot;execStats&quot; : &#123;</span><br><span class="line">		&quot;type&quot; : &quot;COLLSCAN&quot;, #”COLLSCAN“：整个集合扫描，效果最差；”IXSCAN“：索引扫描；&quot;Fetch&quot;: ?</span><br><span class="line">		&quot;works&quot; : 520146,</span><br><span class="line">		&quot;yields&quot; : 4063,</span><br><span class="line">		&quot;unyields&quot; : 4063,</span><br><span class="line">		&quot;invalidates&quot; : 0,</span><br><span class="line">		&quot;advanced&quot; : 0,</span><br><span class="line">		&quot;needTime&quot; : 520145,</span><br><span class="line">		&quot;needFetch&quot; : 0,</span><br><span class="line">		&quot;isEOF&quot; : 1,</span><br><span class="line">		&quot;docsTested&quot; : 520144,</span><br><span class="line">		&quot;children&quot; : [ ]</span><br><span class="line">	&#125;,</span><br><span class="line">	&quot;ts&quot; : ISODate(&quot;2019-07-23T10:11:54.468Z&quot;), #该命令在何时执行</span><br><span class="line">	&quot;client&quot; : &quot;127.0.0.1&quot;, #链接ip或则主机</span><br><span class="line">	&quot;allUsers&quot; : [ ],</span><br><span class="line">	&quot;user&quot; : &quot;&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>如果发现 millis 值比较大，那么就需要作优化。</p>
<p>1  如果nscanned数很大，或者接近记录总数（文档数），那么可能没有用到索引查询，而是全表扫描。</p>
<p>2  如果nscanned值高于nreturned的值，说明数据库为了找到目标文档扫描了很多文档。这时可以考虑创建索引来提高效率。</p>
<h2 id="system-profile补充-这部分待验证"><a href="#system-profile补充-这部分待验证" class="headerlink" title="system.profile补充(这部分待验证)"></a>system.profile补充(<font color=#FF0000>这部分待验证</font>)</h2><p>‘type’的返回参数说明：</p>
<ul>
<li>COLLSCAN #全表扫描</li>
<li>IXSCAN #索引扫描</li>
<li>FETCH #根据索引去检索指定document</li>
<li>SHARD_MERGE #将各个分片返回数据进行merge</li>
<li>SORT #表明在内存中进行了排序（与老版本的scanAndOrder:true一致）</li>
<li>LIMIT #使用limit限制返回数</li>
<li>SKIP #使用skip进行跳过</li>
<li>IDHACK #针对_id进行查询</li>
<li>SHARDING_FILTER #通过mongos对分片数据进行查询</li>
<li>COUNT #利用db.coll.explain().count()之类进行count运算</li>
<li>COUNTSCAN #count不使用Index进行count时的stage返回</li>
<li>COUNT_SCAN #count使用了Index进行count时的stage返回</li>
<li>SUBPLA #未使用到索引的$or查询的stage返回</li>
<li>TEXT #使用全文索引进行查询时候的stage返回</li>
<li>PROJECTION #限定返回字段时候stage的返回</li>
</ul>
<p>对于普通查询，我们最希望看到的组合有这些：</p>
<ul>
<li>Fetch+IDHACK</li>
<li>Fetch+ixscan</li>
<li>Limit+（Fetch+ixscan）</li>
<li>PROJECTION+ixscan</li>
<li>SHARDING_FILTER+ixscan</li>
</ul>
<p>不希望看到包含如下的type：</p>
<ul>
<li>COLLSCAN（全表扫）</li>
<li>SORT（使用sort但是无index）</li>
<li>不合理的SKIP，SUBPLA（未用到index的$or）</li>
</ul>
<p>对于count查询，希望看到的有:COUNT_SCAN</p>
<p>不希望看到的有:COUNTSCAN</p>
<h1 id="常见的慢日志-system-profile-查询语句"><a href="#常见的慢日志-system-profile-查询语句" class="headerlink" title="常见的慢日志(system.profile)查询语句"></a>常见的慢日志(system.profile)查询语句</h1><p>返回最近的10条记录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">db.system.profile.find().limit(10).sort( &#123; ts : -1 &#125; ).pretty()</span><br></pre></td></tr></table></figure>

<p>返回所有的操作，除command类型的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">db.system.profile.find( &#123; op: &#123; $ne : ‘command‘ &#125; &#125;).pretty()</span><br></pre></td></tr></table></figure>

<p>返回特定集合</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">db.system.profile.find( &#123; ns: &quot;uhost.DiskAttachInfo&quot;&#125; ).pretty()</span><br></pre></td></tr></table></figure>

<p>To return operations slower than 5 milliseconds, run a query similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">db.system.profile.find( &#123; millis : &#123; $gt : 5 &#125; &#125; ).pretty()</span><br></pre></td></tr></table></figure>

<p>To return information from a certain time range, run a query similar to the following:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">db.system.profile.find(&#123;</span><br><span class="line">  ts : &#123;</span><br><span class="line">    $gt: new ISODate(&quot;2012-12-09T03:00:00Z&quot;),</span><br><span class="line">    $lt: new ISODate(&quot;2012-12-09T03:40:00Z&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;).pretty()</span><br></pre></td></tr></table></figure>

<p>The following example looks at the time range, suppresses the user field from the output to make it easier to read, and sorts the results by how long each operation took to run:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">db.system.profile.find(&#123;</span><br><span class="line">  ts : &#123;</span><br><span class="line">    $gt: new ISODate(&quot;2011-07-12T03:00:00Z&quot;),</span><br><span class="line">    $lt: new ISODate(&quot;2011-07-12T03:40:00Z&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;, &#123; user: 0 &#125;).sort( &#123; millis: -1 &#125; )</span><br></pre></td></tr></table></figure>

<p>Show the Five Most Recent Events</p>
<p>On a database that has profiling enabled, the show profile helper in the mongo shell displays the 5 most recent operations that took at least 1 millisecond to execute. Issue show profile from the mongo shell, as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">show profile</span><br></pre></td></tr></table></figure>

<h1 id="explain分析"><a href="#explain分析" class="headerlink" title="explain分析"></a>explain分析</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">uhost:PRIMARY&gt; db.getCollection(&#x27;DiskAttachInfo&#x27;).find(&#123;&quot;backend_types&quot;:&#123;$in:[&quot;ssd&quot;]&#125;&#125;).explain()</span><br><span class="line">&#123;</span><br><span class="line">	&quot;cursor&quot; : &quot;BasicCursor&quot;, #返回游标类型，有BasicCursor和BtreeCursor，后者意味着使用了索引。</span><br><span class="line">    &quot;isMultiKey&quot; : false,</span><br><span class="line">	&quot;isMultiKey&quot; : false,</span><br><span class="line">	&quot;n&quot; : 0,</span><br><span class="line">	&quot;nscannedObjects&quot; : 522072, #the total number of documents scanned.</span><br><span class="line">	&quot;nscanned&quot; : 522072,  #the total number of index entries scanned (or documents for a collection scan).</span><br><span class="line">	&quot;nscannedObjectsAllPlans&quot; : 522072,</span><br><span class="line">	&quot;nscannedAllPlans&quot; : 522072,</span><br><span class="line">	&quot;scanAndOrder&quot; : false, #MongoDB是否在内存中对结果集进行了排序</span><br><span class="line">	&quot;indexOnly&quot; : false, #MongoDB是否只使用索引就能完成此次查询</span><br><span class="line">	&quot;nYields&quot; : 4078,  #为了让写入请求能够顺利执行，本次查询暂停暂停的次数。如果有写入请求需求处理，查询会周期性的释放他们的锁，以便写入能够顺利执行</span><br><span class="line">	&quot;nChunkSkips&quot; : 0,</span><br><span class="line">	&quot;millis&quot; : 918, #数据库执行本次查询所耗费的毫秒数。这个数字越小，说明效率越高</span><br><span class="line">	&quot;server&quot; : &quot;172-18-38-130:27017&quot;,</span><br><span class="line">	&quot;filterSet&quot; : false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>详细见<a href="https://docs.mongodb.com/v2.6/reference/method/cursor.explain/index.html">cursor.explain()</a></p>
<h1 id="添加索引"><a href="#添加索引" class="headerlink" title="添加索引"></a>添加索引</h1><p>在以上的分析中，”nscanned” 过大，且”type” : “COLLSCAN”为整个集合扫描，”indexOnly” : false表示没有用到索引。考虑创建一个索引来改善这一点。</p>
<p>查看当前索引，在backend_type上并没有索引。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">uhost:PRIMARY&gt; db.getCollection(&#x27;DiskAttachInfo&#x27;).getIndexes()</span><br><span class="line">[</span><br><span class="line">	&#123;</span><br><span class="line">		&quot;v&quot; : 1,</span><br><span class="line">		&quot;key&quot; : &#123;</span><br><span class="line">			&quot;_id&quot; : 1</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;name&quot; : &quot;_id_&quot;,</span><br><span class="line">		&quot;ns&quot; : &quot;uhost.DiskAttachInfo&quot;</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		&quot;v&quot; : 1,</span><br><span class="line">		&quot;key&quot; : &#123;</span><br><span class="line">			&quot;uhost_id&quot; : 1</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;name&quot; : &quot;uhost_id_1&quot;,</span><br><span class="line">		&quot;ns&quot; : &quot;uhost.DiskAttachInfo&quot;,</span><br><span class="line">		&quot;background&quot; : true,</span><br><span class="line">		&quot;safe&quot; : null</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		&quot;v&quot; : 1,</span><br><span class="line">		&quot;key&quot; : &#123;</span><br><span class="line">			&quot;attached&quot; : 1</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;name&quot; : &quot;attached_1&quot;,</span><br><span class="line">		&quot;ns&quot; : &quot;uhost.DiskAttachInfo&quot;,</span><br><span class="line">		&quot;background&quot; : true,</span><br><span class="line">		&quot;safe&quot; : null</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		&quot;v&quot; : 1,</span><br><span class="line">		&quot;key&quot; : &#123;</span><br><span class="line">			&quot;disk_id&quot; : 1</span><br><span class="line">		&#125;,</span><br><span class="line">		&quot;name&quot; : &quot;disk_id_1&quot;,</span><br><span class="line">		&quot;ns&quot; : &quot;uhost.DiskAttachInfo&quot;,</span><br><span class="line">		&quot;background&quot; : true,</span><br><span class="line">		&quot;safe&quot; : null</span><br><span class="line">	&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>添加index</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">uhost:PRIMARY&gt; db.DiskAttachInfo.createIndex(&#123;&quot;backend_type&quot;:1&#125;,&#123;background: true, safe: null&#125;)</span><br><span class="line">&#123;</span><br><span class="line">	&quot;createdCollectionAutomatically&quot; : false,</span><br><span class="line">	&quot;numIndexesBefore&quot; : 4,</span><br><span class="line">	&quot;numIndexesAfter&quot; : 5,</span><br><span class="line">	&quot;ok&quot; : 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="添加索引后测试效果"><a href="#添加索引后测试效果" class="headerlink" title="添加索引后测试效果"></a>添加索引后测试效果</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">uhost:PRIMARY&gt; db.getCollection(&#x27;DiskAttachInfo&#x27;).find(&#123;&quot;backend_type&quot;:&#123;$in:[&quot;ssd&quot;]&#125;&#125;).explain()</span><br><span class="line">&#123;</span><br><span class="line">	&quot;cursor&quot; : &quot;BtreeCursor backend_type_1&quot;,</span><br><span class="line">	&quot;isMultiKey&quot; : false,</span><br><span class="line">	&quot;n&quot; : 98023,</span><br><span class="line">	&quot;nscannedObjects&quot; : 98023,</span><br><span class="line">	&quot;nscanned&quot; : 98023,</span><br><span class="line">	&quot;nscannedObjectsAllPlans&quot; : 98023,</span><br><span class="line">	&quot;nscannedAllPlans&quot; : 98023,</span><br><span class="line">	&quot;scanAndOrder&quot; : false,</span><br><span class="line">	&quot;indexOnly&quot; : false,</span><br><span class="line">	&quot;nYields&quot; : 765,</span><br><span class="line">	&quot;nChunkSkips&quot; : 0,</span><br><span class="line">	&quot;millis&quot; : 169,</span><br><span class="line">	&quot;indexBounds&quot; : &#123;</span><br><span class="line">		&quot;backend_type&quot; : [</span><br><span class="line">			[</span><br><span class="line">				&quot;ssd&quot;,</span><br><span class="line">				&quot;ssd&quot;</span><br><span class="line">			]</span><br><span class="line">		]</span><br><span class="line">	&#125;,</span><br><span class="line">	&quot;server&quot; : &quot;172-18-38-130:27017&quot;,</span><br><span class="line">	&quot;filterSet&quot; : false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>“nscanned” 由原来的52w降低到9w多。<br>时间（”millis” ）由828ms降低到169ms。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://docs.mongodb.com/manual/tutorial/manage-the-database-profiler/">Database Profiler</a></li>
<li><a href="https://docs.mongodb.com/v2.6/reference/method/cursor.explain/index.html">cursor.explain()</a></li>
<li><a href="https://blog.csdn.net/yisun123456/article/details/78274477">转 mongodb Profiling 通过慢查询日志分析查询慢的原因 相应优化</a></li>
<li><a href="https://blog.csdn.net/joy0921/article/details/80131186">MongoDB查询优化：从10s到10ms</a>   </li>
</ol>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mongo</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>一次node进程cpu负载高问题的处理</title>
    <url>/2019/07/13/%E4%B8%80%E6%AC%A1node%E8%BF%9B%E7%A8%8Bcpu%E8%B4%9F%E8%BD%BD100-%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="1-node进程负载高的问题"><a href="#1-node进程负载高的问题" class="headerlink" title="1 node进程负载高的问题"></a>1 node进程负载高的问题</h1><ul>
<li>线上有个用node开发的uhost-manager的服务。每个set内会有多台物理机，每台物理机（宿主机）上存在着1台或者多台虚拟机。uhost-manager里有个定时逻辑：每隔30s,从MongoDB里拉取所有的虚拟机的记录，数量为2000多台；将位于同一台宿主机上的虚拟机资源，包括cpu、内存、磁盘容量等，做个sum，将sum再填入对应宿主机（共200多台）的记录里，表示已经被占用的资源。</li>
<li>由于计算量大，和代码写得差，导致在该定时的计算任务执行时，会把占用的cpu单核打满；如果uhost-manager此时又有其他接口被调用时，就会出现响应慢的问题，导致调用方的服务也响应慢，甚至会因超时导致服务接口不可用。</li>
</ul>
<img src="/2019/07/13/%E4%B8%80%E6%AC%A1node%E8%BF%9B%E7%A8%8Bcpu%E8%B4%9F%E8%BD%BD100-%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/uhost-manager-cpu-100.png" class="" title="image">
<p>这个单核打满持续的时间短，在1s左右。</p>
<span id="more"></span>

<h1 id="2-问题的解决"><a href="#2-问题的解决" class="headerlink" title="2 问题的解决"></a>2 问题的解决</h1><p>为了解决这个问题，带来两个要思考的问题：</p>
<ul>
<li>解决方法 –&gt; 怎么改进代码</li>
<li>怎么衡量改进的效果 –&gt;<br>因为负载高的时间短，统计实时cpu负载，想到的命令只有top，但top命令本身无法做到统计，加入单核打满时间从1秒降到0.9秒，是无法凭肉眼感知的；像ps、vmstat等，也都不满足统计某个进程的实时负载</li>
</ul>
<h2 id="2-1-统计进程占用的cpu运行时间"><a href="#2-1-统计进程占用的cpu运行时间" class="headerlink" title="2.1 统计进程占用的cpu运行时间"></a>2.1 统计进程占用的cpu运行时间</h2><p>/proc/<pid>/stat该文件包含了某一进程所有的活动的信息，该文件中的所有值都是从系统启动开始累计到当前时刻。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># cat /proc/17273/stat</span><br><span class="line">17273 (node) R 1369 17273 17273 0 -1 4218880 12446677 0 2 0 380634 40519 0 0 20 0 6 0 3525562392 964878336 38584 18446744073709551615 4194304 5593643 140724209068064 140400647923040 140400819234755 0 0 4096 16898 0 0 0 17 1 0 0 3 0 0 7692288 7703720 35180544 140724209071315 140724209071434 140724209071434 140724209074154 0</span><br></pre></td></tr></table></figure>
<p>这样看非常不直观，通过网上找的一段小代码procstat[1]，以人类可读的方式显示。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@172-30-38-65 uhost-manager.logs]# procstat 17273</span><br><span class="line">                 pid: 17273</span><br><span class="line">               tcomm: (node)</span><br><span class="line">                ...</span><br><span class="line">               utime: 3818.830000</span><br><span class="line">               stime: 406.260000</span><br><span class="line">              cutime: 0.000000</span><br><span class="line">              cstime: 0.000000</span><br><span class="line">                 ...</span><br><span class="line">              start_time: 07.13 03:00 (77599.65s)</span><br><span class="line">                 ...</span><br></pre></td></tr></table></figure>
<p>进程的总Cpu时间processCpuTime = utime + stime + cutime + cstime，该值包括其所有线程的cpu时间[2]。注：这里统计出来的单位不是s，是相对于clock ticks来说的[3]。</p>
<p>我们的衡量手段为统计一段时间内新增的该进程的cpu时间；该值越小，表示运行时间越短。</p>
<p>具体步骤如下：</p>
<ul>
<li>以其他端口，再起一个uhost-manager服务，防止被其他服务调用；</li>
<li>另外，定时任务里只保留定时更新宿主机占用资源的接口（ smi_update_host_resource ）。</li>
<li>为了使统计的cpu占用时间长一点，smi改成每5秒执行一次；总共统计60s。查看新增的cpu时间，processCpuTime_60s - processCpuTime_0s。</li>
</ul>
<h2 id="2-2-解决方法"><a href="#2-2-解决方法" class="headerlink" title="2.2 解决方法"></a>2.2 解决方法</h2><p>为了解决，需要知道消耗cpu性能的环节在哪些步骤上，好进一步优化。</p>
<h3 id="2-2-1-perf"><a href="#2-2-1-perf" class="headerlink" title="2.2.1 perf"></a>2.2.1 perf</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">perf top -g -p 26642 -d 1  #此时uhost-manager pid为26642</span><br></pre></td></tr></table></figure>
<img src="/2019/07/13/%E4%B8%80%E6%AC%A1node%E8%BF%9B%E7%A8%8Bcpu%E8%B4%9F%E8%BD%BD100-%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/perf_top.png" class="" title="image">
<p>这里看起来不直观，并不能看清楚是哪个操作引起的。</p>
<h3 id="2-2-2-FlameGraph"><a href="#2-2-2-FlameGraph" class="headerlink" title="2.2.2 FlameGraph"></a>2.2.2 FlameGraph</h3><p>火焰图的方式比较直观，这里测试下效果。</p>
<ul>
<li>采集脚本： perf record -F 99 -p 26642 -g – sleep 60</li>
</ul>
<p>参数说明如下：</p>
<table>
<thead>
<tr>
<th align="center">采集频率(ms)</th>
<th align="center">进程pid</th>
<th align="center">调用记录</th>
<th align="center">记录时长</th>
</tr>
</thead>
<tbody><tr>
<td align="center">-F 99</td>
<td align="center">-p 26642</td>
<td align="center">-g</td>
<td align="center">–sleep 60</td>
</tr>
</tbody></table>
<ul>
<li>进程对应的符号表perf-pid.map权限设置：$chown root /tmp/perf-pid.map。</li>
</ul>
<p>踩过的坑！！！是为了消除下面这个问题的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">File /tmp/perf-PID.map not owned by current user or root, ignoring it (use -f to override).</span><br><span class="line">Failed to open /tmp/perf-PID.map, continuing without symbols</span><br></pre></td></tr></table></figure>
<ul>
<li>FlameGraph使用方式如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/brendangregg/FlameGraph.git</span><br><span class="line">cd FlameGraph</span><br><span class="line">perf script -i perf.data &amp;&gt; perf.unfold  # 以下几个命令都是在FlameGraph目录下执行的</span><br><span class="line">perl stackcollapse-perf.pl perf.unfold &amp;&gt; perf.folded</span><br><span class="line">perl flamegraph.pl perf.folded &gt; perf.svg</span><br></pre></td></tr></table></figure>

<ul>
<li>火焰图效果</li>
</ul>
<img src="/2019/07/13/%E4%B8%80%E6%AC%A1node%E8%BF%9B%E7%A8%8Bcpu%E8%B4%9F%E8%BD%BD100-%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/perf_flamegraph.png" class="" title="image">

<p>火焰图中看到libv8.so.3.14.5是消耗cpu最多的，但底下的方块（父函数）是unknown，所以还是无法知晓是哪里调用的。</p>
<p>火焰图形状对应关系</p>
<table>
<thead>
<tr>
<th align="left">形状</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">每一个平面方块</td>
<td align="left">一个函数在栈中的位置（也称一个栈帧）</td>
</tr>
<tr>
<td align="left">Y轴</td>
<td align="left">栈的深度(也叫栈的帧数)</td>
</tr>
<tr>
<td align="left">X轴</td>
<td align="left">表示总的样例，不过它们左右顺序没有特殊含义</td>
</tr>
<tr>
<td align="left">每个平面方块的宽度</td>
<td align="left">方块的宽度标示CPU使用时间或者说相对父函数而言使用CPU的比率，越宽代表占用CPU的时间越长，或者使用CPU很频繁</td>
</tr>
</tbody></table>
<h3 id="2-2-3-原始的分析代码方式（肉眼看）"><a href="#2-2-3-原始的分析代码方式（肉眼看）" class="headerlink" title="2.2.3 原始的分析代码方式（肉眼看）"></a>2.2.3 原始的分析代码方式（肉眼看）</h3><p>以上手段不起作用后，只能逐行分析代码，看哪里可能消耗的cpu多，然后优化掉。</p>
<h4 id="原始的消耗时间"><a href="#原始的消耗时间" class="headerlink" title="原始的消耗时间"></a>原始的消耗时间</h4><p>先按照2.1中的采集uhost-manager进程在60s内消耗的cpu时间，为10.65。这里的时间采集是5次测量，然后取平均值。</p>
<h4 id="lean"><a href="#lean" class="headerlink" title="lean"></a>lean</h4><p>从MongoDB里读取数据时，用到mongoose的模块。使用lean可以加快查询速度。</p>
<img src="/2019/07/13/%E4%B8%80%E6%AC%A1node%E8%BF%9B%E7%A8%8Bcpu%E8%B4%9F%E8%BD%BD100-%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/query%E6%97%B6lean%E7%9A%84%E4%BD%BF%E7%94%A8.png" class="" title="image">

<blockquote>
<p>The lean option tells Mongoose to skip hydrating the result documents. This makes queries faster and less memory intensive, <strong>but the result documents are plain old JavaScript objects (POJOs), not Mongoose documents</strong>.</p>
</blockquote>
<blockquote>
<p>If you’re executing a query and sending the results without modification to, say, an Express response, you should use lean. <font color=#FF0000 >In general, if you do not modify the query results and do not use custom getters, you should use lean(). If you modify the query results or rely on features like getters or transforms, you should not use lean()</font>.</p>
</blockquote>
<p>测试下来结果为6.5，降低了40%左右，效果明显。</p>
<h4 id="减少无用的重复执行的语句"><a href="#减少无用的重复执行的语句" class="headerlink" title="减少无用的重复执行的语句"></a>减少无用的重复执行的语句</h4><p>修改之后，测试下来结果为5.3。</p>
<img src="/2019/07/13/%E4%B8%80%E6%AC%A1node%E8%BF%9B%E7%A8%8Bcpu%E8%B4%9F%E8%BD%BD100-%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/%E9%87%8D%E5%A4%8D%E5%88%A4%E6%96%AD%E8%AE%BE%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%80%BC%E7%9A%84%E4%BB%A3%E7%A0%81.png" class="" title="image">

<h4 id="减少日志打印"><a href="#减少日志打印" class="headerlink" title="减少日志打印"></a>减少日志打印</h4><p>减少量大的无用的日志打印后，测试下来结果为1.9。所以日志打印的cpu消耗在原始基础上占了32%，即（5.3-1.9)/10.65。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 每台虚机打印下vcpu的个数，&gt;2000</span><br><span class="line">GLOBAL.logger.info(_vcpus);</span><br><span class="line"></span><br><span class="line">// 每台宿主机打印下host_id, &gt;200</span><br><span class="line">GLOBAL.logger.info(&quot;[SMI]check:&quot;, host_ids[i]);</span><br></pre></td></tr></table></figure>

<h1 id="3-优化效果"><a href="#3-优化效果" class="headerlink" title="3 优化效果"></a>3 优化效果</h1><p>经过优化后，消耗时间是1.9 clock ticks，相对于原来的10.65 clock ticks，减少了82.16%。</p>
<img src="/2019/07/13/%E4%B8%80%E6%AC%A1node%E8%BF%9B%E7%A8%8Bcpu%E8%B4%9F%E8%BD%BD100-%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%84%E7%90%86/%E8%BF%9B%E7%A8%8B%E7%9A%84cpu%E6%97%B6%E9%97%B4.png" class="" title="image">
<p>单核峰值CPU消耗为70%左右，且持续时间很短，为100ms左右。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="http://brokestream.com/procstat.html">procstat - displays linux proc stat (/proc/pid/stat) in human-readable format
</a></li>
<li><a href="http://www.samirchen.com/linux-cpu-performance/">Linux环境下进程的CPU占用率</a></li>
<li><a href="http://man7.org/linux/man-pages/man5/proc.5.html">Linux Programmer’s Manual PROC(5)</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1005867">NodeJS 性能优化之 CPU 看图篇</a></li>
<li><a href="http://www.cpper.cn/2017/06/04/linux/perf/">Linux性能调优工具perf的使用</a></li>
<li><a href="https://www.yangcs.net/posts/how-to-deal-with-increasing-of-cpu-usage/">某个应用的 CPU 使用率居然达到 100%，我该怎么办？</a></li>
<li><a href="https://mongoosejs.com/docs/tutorials/lean.html#lean-and-populate">Faster Mongoose Queries With Lean</a></li>
</ol>
]]></content>
      <categories>
        <category>linux性能优化</category>
      </categories>
      <tags>
        <tag>cpu</tag>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title>并发控制</title>
    <url>/2018/08/14/%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/</url>
    <content><![CDATA[<h2 id="并发场景"><a href="#并发场景" class="headerlink" title="并发场景"></a>并发场景</h2><p>秒杀场景，多人并发申请购买同一种商品。<br>下面以2人抢购同一件商品举例，商品数量为1。<br>基本流程如下：</p>
<img src="/2018/08/14/%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/%E8%B4%AD%E4%B9%B0%E8%AE%B0%E5%BD%95.png" class="" title="image">
<p>这个流程中存在明显的并发问题，当进程A查看有1个商品，但未开始创建购买记录，此时另一个进程B也进行了资源检查，发现有1个商品，顺利通过，并完成购买操作，此时A继续执行，则1个商品会产生两条购买的记录。（这个并发场景很简单，很普遍）</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>解决方案通常是2类：<br>1 通过将请求分发到不同set，减少并发的可能性，分而治之。这里暂时不讨论。<br>2 当收到并发请求时，如何处理。</p>
<h3 id="加锁操作先占有锁资源，再占有资源"><a href="#加锁操作先占有锁资源，再占有资源" class="headerlink" title="加锁操作先占有锁资源，再占有资源"></a>加锁操作先占有锁资源，再占有资源</h3><p>a. 锁库存<br>b. 插入“秒杀”记录<br>c. 更新库存</p>
<p>“秒杀”系统的设计难点就在这个事务操作上。商品库存在DB中记为一行，大量用户同时“秒杀”同一商品时，第一个到达DB的请求锁住了这行库存记录。在第一个事务完成提交之前这个锁一直被第一个请求占用，其他到达的请求全部失败。</p>
<p>加锁的方案，效率会比较低。实际应用时，通常要配合排队系统，让没有获取到锁的请求排队。</p>
<h3 id="单独开发请求排队调度模块"><a href="#单独开发请求排队调度模块" class="headerlink" title="单独开发请求排队调度模块"></a>单独开发请求排队调度模块</h3><p>排队模块接收用户的抢红包请求，以FIFO模式保存下来，调度模块负责FIFO队列的动态调度，一旦有空闲资源，便从队列头部把用户的访问请求取出后交给真正提供服务的模块处理。优点是，具有中心节点的统一资源管理，对系统的可控性强，可深度定制。缺点是，所有请求流量都会有中心节点参与，效率必然会比分布式无中心系统低，并且，中心节点也很容易成为整个系统的性能瓶颈。</p>
<h3 id="巧用Redis-特性，使其成为分布式序号生成器（我们最终采用的做法）-–-具体还要再看，如何利用redis的特性"><a href="#巧用Redis-特性，使其成为分布式序号生成器（我们最终采用的做法）-–-具体还要再看，如何利用redis的特性" class="headerlink" title="巧用Redis 特性，使其成为分布式序号生成器（我们最终采用的做法） – 具体还要再看，如何利用redis的特性"></a>巧用Redis 特性，使其成为分布式序号生成器（我们最终采用的做法） – 具体还要再看，如何利用redis的特性</h3><p>参考<br><a href="https://www.ibm.com/developerworks/cn/web/wa-design-small-and-good-kill-system/index.html">https://www.ibm.com/developerworks/cn/web/wa-design-small-and-good-kill-system/index.html</a><br><a href="https://blog.csdn.net/zhanjianshinian/article/details/53342730">https://blog.csdn.net/zhanjianshinian/article/details/53342730</a><br>每件商品以一个数字ID来标识，这个ID是全局唯一的，所有围绕商品的操作都使用这个ID作为数据的关联项。</p>
<p>Redis节点内存储的是这个分组可以分发的商品ID号段，利用Redis特性实现红包分发，各服务节点通过Redis原语获取当前拆到的红包。这种做法的思路是，Redis 本身是单进程工作模型，来自分布式系统各个节点的操作请求天然的被 Redis Server 做了一个同步队列，只要每个请求执行的足够快，这个队列就不会引起阻塞及请求超时。而本例中我们使用了 DECR 原语，性能上是可以满足需求的。Redis 在这里相当于是充当一个分布式序号发生器的功能，分发红包 ID。</p>
<h3 id="通过memcached来控制并发数-–-具体还要再看"><a href="#通过memcached来控制并发数-–-具体还要再看" class="headerlink" title="通过memcached来控制并发数 – 具体还要再看"></a>通过memcached来控制并发数 – 具体还要再看</h3><p><a href="https://blog.csdn.net/echo3/article/details/17580347">https://blog.csdn.net/echo3/article/details/17580347</a>  使用memcached进行并发控制（写的非常好）<br><a href="http://www.infoq.com/cn/articles/2017hongbao-weixin#">http://www.infoq.com/cn/articles/2017hongbao-weixin#</a>  百亿级微信红包的高并发资金交易系统设计方案<br><a href="https://blog.csdn.net/yanker1990/article/details/78737626">https://blog.csdn.net/yanker1990/article/details/78737626</a>  Memcache的并发问题和利用CAS的解决方案</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>如何设计一个小而美的秒杀系统？<br><a href="https://www.ibm.com/developerworks/cn/web/wa-design-small-and-good-kill-system/index.html">https://www.ibm.com/developerworks/cn/web/wa-design-small-and-good-kill-system/index.html</a><br>使用memcached进行并发控制（写的非常好）<br><a href="https://blog.csdn.net/echo3/article/details/17580347">https://blog.csdn.net/echo3/article/details/17580347</a><br>电商秒杀系统设计分析<br><a href="https://blog.csdn.net/zhanjianshinian/article/details/53342730">https://blog.csdn.net/zhanjianshinian/article/details/53342730</a></p>
<p><a href="https://memcached.org/about">https://memcached.org/about</a><br>Redis DECR命令<br><a href="https://www.kancloud.cn/thinkphp/redis-quickstart/36180">https://www.kancloud.cn/thinkphp/redis-quickstart/36180</a></p>
<p>云主机的问题:<br>申请时要排队，好像没有意义。<br>防止申请到同一台宿主机，前面做的临时添加资源是否有用。跟定时更新是否有冲突？</p>
]]></content>
      <categories>
        <category>架构设计</category>
      </categories>
      <tags>
        <tag>并发控制</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式系统架构的简介</title>
    <url>/2019/03/18/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%9A%84%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<p>摘自 分布式系统架构的冰与火 左耳听风</p>
<h2 id="分布式系统架构的优缺点"><a href="#分布式系统架构的优缺点" class="headerlink" title="分布式系统架构的优缺点"></a>分布式系统架构的优缺点</h2><p>需要分布式系统架构，来代替传统的单体架构的原因：</p>
<ol>
<li>增大系统容量 –&gt; 提高性能<br>单台机器的性能无法满足要求时，需要垂直或者水平拆分成分布式系统架构，让更多的机器去承担。</li>
<li>加强系统可用 –&gt; 提高服务可用性<br>避免单点故障。</li>
</ol>
<p>分布式系统是一种trade-off</p>
<img src="/2019/03/18/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%9A%84%E7%AE%80%E4%BB%8B/%E5%8D%95%E4%BD%93%E5%BA%94%E7%94%A8%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9.png" class="" title="image">

<h2 id="分布式系统的发展"><a href="#分布式系统的发展" class="headerlink" title="分布式系统的发展"></a>分布式系统的发展</h2><p>SOA——基于服务的架构</p>
<img src="/2019/03/18/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%9A%84%E7%AE%80%E4%BB%8B/SOA%E6%9E%B6%E6%9E%84%E7%9A%84%E6%BC%94%E5%8C%96.png" class="" title="image">

<ol>
<li>20 世纪 90 年代前，是单体架构，软件模块高度耦合。</li>
<li>2000 年左右出现了比较松耦合的 SOA 架构，这个架构需要一个标准的协议或是中间件来联动其它相关联的服务.</li>
<li>而 2010 年后，出现了微服务架构，这个架构更为松耦合。每一个微服务都能独立完整地运行（所谓的自包含），后端单体的数据库也被微服务这样的架构分散到不同的服务中。而它和传统 SOA 的差别在于，服务间的整合需要一个服务编排或是服务整合的引擎。就好像交响乐中需要有一个指挥来把所有乐器编排和组织在一起。</li>
</ol>
<p>一般来说，这个编排和组织引擎可以是工作流引擎，也可以是网关。当然，还需要辅助于像容器化调度这样的技术方式，如 Kubernetes.</p>
]]></content>
      <categories>
        <category>分布式系统</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title>文件删除空间未释放的例子</title>
    <url>/2018/11/10/%E6%96%87%E4%BB%B6%E5%B7%B2%E5%88%A0%E9%99%A4%E7%A9%BA%E9%97%B4%E6%9C%AA%E9%87%8A%E6%94%BE%E7%9A%84%E4%BE%8B%E5%AD%90/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在删除文件后，有时会遇到磁盘空间并未被释放的场景。这里自己尝试复现，并去做相关的说明。</p>
<h2 id="删除后空间未释放"><a href="#删除后空间未释放" class="headerlink" title="删除后空间未释放"></a>删除后空间未释放</h2><p>根目录下当前占用空间是626G。</p>
<img src="/2018/11/10/%E6%96%87%E4%BB%B6%E5%B7%B2%E5%88%A0%E9%99%A4%E7%A9%BA%E9%97%B4%E6%9C%AA%E9%87%8A%E6%94%BE%E7%9A%84%E4%BE%8B%E5%AD%90/1_%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%90%8E%E7%A9%BA%E9%97%B4.png" class="" title="image">

<p>创建的文件/tmp/safedog_windows_2012_new.img占了15G。（ls和du的区别留到后面讲）<br>![image](文件已删除空间未释放的例子/2_ 文件占用空间.png)</p>
<p>这里用vim打开文件，让有个进程一直在使用这个文件。</p>
<img src="/2018/11/10/%E6%96%87%E4%BB%B6%E5%B7%B2%E5%88%A0%E9%99%A4%E7%A9%BA%E9%97%B4%E6%9C%AA%E9%87%8A%E6%94%BE%E7%9A%84%E4%BE%8B%E5%AD%90/3_%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6.png" class="" title="image">

<p>删除文件后，空间并未释放。</p>
<img src="/2018/11/10/%E6%96%87%E4%BB%B6%E5%B7%B2%E5%88%A0%E9%99%A4%E7%A9%BA%E9%97%B4%E6%9C%AA%E9%87%8A%E6%94%BE%E7%9A%84%E4%BE%8B%E5%AD%90/4_%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E5%90%8E%E7%9A%84%E7%A9%BA%E9%97%B4.png" class="" title="image">

<p>原因分析：</p>
<p>一个文件在文件系统中的存放分为两个部分：数据部分和指针部分，指针位于文件系统的meta-data中，数据被删除后，这个指针就从meta-data中清除了，而数据部分存储在磁盘中，数据对应的指针从meta-data中清除后，文件数据部分占用的空间就可以被覆盖并写入新的内容，之所以出现删除文件后，空间还没释放，就是因为有进程还在一直向这个文件写入内容，导致虽然删除了文件，但文件对应的指针部分由于进程锁定，并未从meta-data中清除，而由于指针并未被删除，那么系统内核就认为文件并未被删除，因此通过df命令查询空间并未释放也就不足为奇了。</p>
<img src="/2018/11/10/%E6%96%87%E4%BB%B6%E5%B7%B2%E5%88%A0%E9%99%A4%E7%A9%BA%E9%97%B4%E6%9C%AA%E9%87%8A%E6%94%BE%E7%9A%84%E4%BE%8B%E5%AD%90/5_lsof.png" class="" title="image">

<h2 id="释放的解决措施"><a href="#释放的解决措施" class="headerlink" title="释放的解决措施"></a>释放的解决措施</h2><h3 id="停掉或重启占用文件的进程"><a href="#停掉或重启占用文件的进程" class="headerlink" title="停掉或重启占用文件的进程"></a>停掉或重启占用文件的进程</h3><p>重启或停止进程后，系统会自动回收</p>
<h3 id="Truncate-File-Size"><a href="#Truncate-File-Size" class="headerlink" title="Truncate File Size"></a>Truncate File Size</h3><p>Alternatively, it is possible to force the system to de-allocate the space consumed by an in-use file by forcing the system to truncate the file via the proc file system. This is an advanced technique and should only be carried out when the administrator is certain that this will cause no adverse effects to running processes. Applications may not be designed to deal elegantly with this situation and may produce inconsistent or undefined behavior when files that are in use are abruptly truncated in this manner.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; /proc/$pid/fd/$fd_number   </span><br><span class="line">echo &gt; /proc/$pid/fd/$fd_number </span><br><span class="line">echo &quot;&quot; &gt; /proc/$pid/fd/$fd_number</span><br></pre></td></tr></table></figure>

<img src="/2018/11/10/%E6%96%87%E4%BB%B6%E5%B7%B2%E5%88%A0%E9%99%A4%E7%A9%BA%E9%97%B4%E6%9C%AA%E9%87%8A%E6%94%BE%E7%9A%84%E4%BE%8B%E5%AD%90/6_solution1_space.png" class="" title="image">


<p>// 以上截断命令的区别</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; /proc/$pid/fd/$fd_number   //截断后占据空间为0 </span><br><span class="line"></span><br><span class="line">[root@xxg-uhost1683 2018-01-09]# &gt; /proc/21780/fd/4</span><br><span class="line">[root@xxg-uhost1683 2018-01-09]# lsof -p 21780 | grep delete</span><br><span class="line">less    21780 root    4r   REG  253,0        0 171442340 /opt/data/delay_delete/2018-01-09/1356f3f2-1c93-4c9e-9f9f-0a4cee57f5b7.img_1515479993 (deleted)</span><br><span class="line"></span><br><span class="line">echo &gt; /proc/$pid/fd/$fd_number   //截断后占据空间为1个字节 </span><br><span class="line"></span><br><span class="line">[root@xxg-uhost1683 2018-01-09]# echo &gt; /proc/27052/fd/4</span><br><span class="line">[root@xxg-uhost1683 2018-01-09]# lsof -p 27052 | grep delete</span><br><span class="line">less    27052 root  cwd    DIR  253,0     4096   7995395 /opt/data/delay_delete/2018-01-09</span><br><span class="line">less    27052 root    4r   REG  253,0        1 171442356 /opt/data/delay_delete/2018-01-09/1ede93ec-4424-413f-ad24-9781038abdf7.img_1515479994 (deleted)</span><br><span class="line"></span><br><span class="line">echo &quot;&quot; &gt; /proc/$pid/fd/$fd_number // 占据空间为1个字节 </span><br><span class="line"></span><br><span class="line">[root@xxg-uhost1683 2018-01-09]# echo &quot;&quot; &gt; /proc/7706/fd/4</span><br><span class="line">[root@xxg-uhost1683 2018-01-09]# lsof -p 7706 | grep delete</span><br><span class="line">less    7706 root  cwd    DIR  253,0     4096   7995395 /opt/data/delay_delete/2018-01-09</span><br><span class="line">less    7706 root    4r   REG  253,0        1 284164259 /opt/data/delay_delete/2018-01-09/439e3eb3-0272-44ad-b1ee-5ad248ca2db8.disk_1515479994 (deleted)</span><br><span class="line"></span><br><span class="line">echo &quot;&quot; &gt; /proc/$pid/fd/$fd_number // 占据空间为2个字节 </span><br><span class="line"></span><br><span class="line">[root@xxg-uhost1683 2018-01-09]# echo &quot; &quot; &gt; /proc/7706/fd/4</span><br><span class="line">[root@xxg-uhost1683 2018-01-09]# lsof -p 7706 | grep delete</span><br><span class="line">less    7706 root  cwd    DIR  253,0     4096   7995395 /opt/data/delay_delete/2018-01-09</span><br><span class="line">less    7706 root    4r   REG  253,0        2 284164259 /opt/data/delay_delete/2018-01-09/439e3eb3-0272-44ad-b1ee-5ad248ca2db8.disk_1515479994 (deleted)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>以下直接操作文件名均不起作用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; /tmp/safedog_windows_2012.img //不起作用</span><br><span class="line">echo &gt; /tmp/safedog_windows_2012.img //不起作用</span><br><span class="line">echo &quot;&quot; &gt; /tmp/safedog_windows_2012.img不起作用</span><br></pre></td></tr></table></figure>

<h2 id="删除后文件未释放的恢复"><a href="#删除后文件未释放的恢复" class="headerlink" title="删除后文件未释放的恢复"></a>删除后文件未释放的恢复</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /proc/44848/fd/4 /tmp/safedog_windows_2012_new.img1    //方法1</span><br><span class="line">cat /proc/44848/fd/4 &gt; /tmp/safedog_windows_2012_new.img2 //方法2</span><br></pre></td></tr></table></figure>

<img src="/2018/11/10/%E6%96%87%E4%BB%B6%E5%B7%B2%E5%88%A0%E9%99%A4%E7%A9%BA%E9%97%B4%E6%9C%AA%E9%87%8A%E6%94%BE%E7%9A%84%E4%BE%8B%E5%AD%90/8_%E8%AF%AF%E5%88%A0%E5%90%8E%E6%81%A2%E5%A4%8D%E6%96%87%E4%BB%B6.png" class="" title="image">

<p>恢复的文件占用空间还会更大，是否恢复出来的已经不是稀疏的了？后面待解释</p>
<p>比较文件的一致性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">diff /tmp/safedog_windows_2012_new.img1 /tmp/safedog_windows_2012_new.img2</span><br></pre></td></tr></table></figure>


<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://www.tanglei.name/blog/difference-between-du-and-ls.html">由一次磁盘告警引发的血案 – du 和 ls 的区别</a></p>
<p><a href="https://access.redhat.com/solutions/2316">Why is space not being freed from disk after deleting a file in Red Hat Enterprise Linux?</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title>磁盘IO在文件传输过程中的稳定控制优化</title>
    <url>/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h2 id="问题的描述"><a href="#问题的描述" class="headerlink" title="问题的描述"></a>问题的描述</h2><p>本地镜像在不同物理机之间的传输，这里是通过自研的工具rsc，包含rsc_client和rsc_server。</p>
<p>rsc_server对收到的数据的处理不当，对IO可能产生不理想的结果：<br>（1）当Bps或者iops 过高时，可能使dm-0的util过高，甚至100%，影响上面虚机的IO；<br>（2）当传输过程中Bps过低时，会使得传输镜像过慢，影响效率，甚至导致创建虚机失败。</p>
<p>dm-0是物理机上的数据盘，有通过多快盘做raid。</p>
<p>以下在rsc_server的IO优化过程中，对不同方案测试结果的比较和分析。</p>
<h2 id="无限速方案"><a href="#无限速方案" class="headerlink" title="无限速方案"></a>无限速方案</h2><p>测试环境为:</p>
<p>测试镜像： Windows 2008 64位 EN </p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/2-nolimit-1.png" class="" title="image">
<p>镜像文件为qcow2格式，总共40G，实际占用空间20G。</p>
<p>宿主机: </p>
<p>172.27.162.108 –&gt; 172.27.172.226（V3）</p>
<p>172.28.160.151–&gt; 172.28.161.86（V4）</p>
<p>rsc_server端，接收到数据时，write到cache，就立刻返回。当cache满时，由系统一次性flush到磁盘。会使磁盘util=100%，持续10~20s，导致影响到客户虚机的IO性能。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/2-nolimit-2.png" class="" title="image">

<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/2-nolimit-3.png" class="" title="image">
<h2 id="限速方案"><a href="#限速方案" class="headerlink" title="限速方案"></a>限速方案</h2><p>不同方案的测试和比较：<br>（1）对rsc_server进程的IO限速；<br>（2）对flush进程的IO限速；<br>（3）每次write时都sync；<br>（4）收到一定数量的包，调用fsync<br>其中对进程的限速是通过cgroup。</p>
<h3 id="对rsc-server进程的IO限速"><a href="#对rsc-server进程的IO限速" class="headerlink" title="对rsc_server进程的IO限速"></a>对rsc_server进程的IO限速</h3><p>设置写到磁盘的限速参数：Bps=40MB/s，iops=15000。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-1-limit-1.png" class="" title="image">

<p>把rsc_server进程PID添加到限速的tasks里。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-1-limit-2.png" class="" title="image">

<p>无法限制住磁盘的IO。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-1-limit-3.png" class="" title="image">

<p>原因：将cache刷到磁盘时，是由flush进程执行的，并不是rsc_server本身。所以考虑到将flush进程限速。</p>
<h3 id="对flush-进程的IO限速"><a href="#对flush-进程的IO限速" class="headerlink" title="对flush 进程的IO限速"></a>对flush 进程的IO限速</h3><p>把flush进程的PID加入tasks里，可以限制磁盘到40M/s。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-2-flushlimit-1.png" class="" title="image">

<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-2-flushlimit-2.png" class="" title="image">

<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-2-flushlimit-3.png" class="" title="image">

<p>带来的问题，会影响所有通过flush来刷磁盘的操作：</p>
<p>1 宿主机的磁盘dm-0上，其他会将数据写到cahe就返回的操作都受到影响，如jbd2服务。jbd2(journaling block device 2)是ext4文件系统的服务，可以完成数据备份和恢复。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-2-flushlimit-4.png" class="" title="image">
<p>2 虚机内部</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-2-flushlimit-5.png" class="" title="image">

<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-2-flushlimit-6.png" class="" title="image">
<h3 id="每次write时都sync"><a href="#每次write时都sync" class="headerlink" title="每次write时都sync"></a>每次write时都sync</h3><p>每次write都sync到磁盘，可以绕过从cache flush到磁盘的过程,也就不会产生flush过程中产生的IO过大的问题。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-3-sync-1.png" class="" title="image">

<p>但每次write都sync带来写磁盘性能过低,落盘速度5M/s左右,util为98%~100%。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-3-sync-2.png" class="" title="image">
<h3 id="每收到10个64K的包，调用一次fsync"><a href="#每收到10个64K的包，调用一次fsync" class="headerlink" title="每收到10个64K的包，调用一次fsync"></a>每收到10个64K的包，调用一次fsync</h3><p>考虑到每次write都sync，影响磁盘性能，考虑减少sync的次数。改成累积到一定数量的包后，再调用fsync的方案。</p>
<p>172.27.162.108 –&gt; 172.27.172.226( V3 )</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-4-fsync-1.png" class="" title="image">

<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-4-fsync-2.png" class="" title="image">


<p>172.28.160.151–&gt; 172.28.161.86( V4 )</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-4-fsync-3.png" class="" title="image">

<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-4-fsync-4.png" class="" title="image">

<p>可以结果：</p>
<p>（1）写入IO得到很大提高，在30M/s ~ 200M/s之间，受宿主机影响较大；</p>
<p>（2）传输过程中，IO一直很稳定，util保持在40%或70%，也没有看到flush进程的出现，说明IO落地是由fsync执行；</p>
<p>  使标准镜像（小于40G）都能在20分钟内传完，扩容过的镜像也不影响虚机的创建。</p>
<p>小插曲：</p>
<p>前面由于代码bug， 在每第10个64K包sync了n次。n为该64K的包，在跳过0的空洞后，拆成的小包的个数。sync次数过多导致IO性能过低，测试数据为6M/s左右。</p>
<p>172.28.160.151–&gt; 172.28.161.86（V4）</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-4-fsync-5.png" class="" title="image">

<p>strace的查看结果：</p>
<p>strace -T -p 21712( rsc_server进程PID ) -o test_rsc_server.txt</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-4-fsync-6.png" class="" title="image">


<p>IO性能过低。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-4-fsync-7.png" class="" title="image">

<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/3-4-fsync-8.png" class="" title="image">


<h2 id="最终方案"><a href="#最终方案" class="headerlink" title="最终方案"></a>最终方案</h2><p>“每收到10个64K的包，调用一次fsync”，由于宿主机的磁盘性能不同，不排除依然可能会有IO打满的情况，因为此时cache到磁盘的过程，是由rsc_server自己控制的，所以可以通过cgroup对rsc_server限速。</p>
<p>172.27.162.108 –&gt; 172.27.172.226( V3 )</p>
<p>限速： rsc_server写入dm-0的 Bps=60M/s。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/4-final-1.png" class="" title="image">

<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/4-final-2.png" class="" title="image">

<p>结果：rsc_server的落盘速度稳定在60M/s。</p>
<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/4-final-3.png" class="" title="image">

<img src="/2018/04/18/%E7%A3%81%E7%9B%98IO%E5%9C%A8%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95/4-final-4.png" class="" title="image">

<p>因此最终的方案是“fsync + cgroup限速”。</p>
]]></content>
      <categories>
        <category>linux性能优化</category>
      </categories>
      <tags>
        <tag>限速</tag>
        <tag>cgroup</tag>
        <tag>磁盘IO</tag>
      </tags>
  </entry>
  <entry>
    <title>读书笔记-TCP/IP详解 卷1：协议</title>
    <url>/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/</url>
    <content><![CDATA[<p>不同层的PDU（Protocol Data Unit）</p>
<ul>
<li>传输层：Segment(帧)</li>
<li>网络层： Packet</li>
<li>链路层：Frame</li>
<li>物理层： Bit</li>
</ul>
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E4%BC%A0%E5%85%A5%E7%9A%84%E5%B8%A7%E5%9C%A8%E4%B8%BB%E6%9C%BA%E4%B8%8A%E7%9A%84%E5%88%86%E8%A7%A3.png" class="" title="image">
<span id="more"></span>

<h3 id="链路层"><a href="#链路层" class="headerlink" title="链路层"></a>链路层</h3><img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E4%BB%A5%E5%A4%AA%E7%BD%91%E5%B8%A7%E6%A0%BC%E5%BC%8F.png" class="" title="image">

<ul>
<li>MTU 到底是怎么来的</li>
</ul>
<p>MTU, 是 Maximum Transmission Unit 的缩写, 根据 Wikipedia 的定义, MTU 指的是在 Network Layer (因处 OSI 第三层, 后以 L3 代替)上传输的最大数据报单元, 而 MTU 的大小一般由 Link Layer (因处 OSI 第二层, 后以 L2 代替) 设备决定. 比如生活中使用最广泛的以太网(Ethernet, IEEE 802.3)的帧大小是 1518 字节, 根据 Ethernet Frame 的定义, L2 Frame 由 14 字节 Header 和 4 字节 Trailer 组成, 所以 L3 层(也就是 IP 层)最多只能填充 1500 字节大小, 这就是 MTU 的由来.</p>
<p>MTU：46~1500</p>
<p>物理层扩展以太网：集线器<br>链路层扩展以太网：网桥、交换机</p>
<p>隧道：在高层（或同等层）分组中携带底层数据</p>
<h3 id="Internet协议"><a href="#Internet协议" class="headerlink" title="Internet协议"></a>Internet协议</h3><img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/IPV4%E6%95%B0%E6%8D%AE%E6%8A%A5.png" class="" title="image">

<p><a href="https://www.tutorialspoint.com/ipv4/ipv4_packet_structure.htm">IPv4 - Packet Structure</a></p>
<h3 id="DHCP"><a href="#DHCP" class="headerlink" title="DHCP"></a>DHCP</h3><img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/DHCP%E4%BA%A4%E6%8D%A2.png" class="" title="image">

<h3 id="防火墙、NAT"><a href="#防火墙、NAT" class="headerlink" title="防火墙、NAT"></a>防火墙、NAT</h3><h3 id="ICMP"><a href="#ICMP" class="headerlink" title="ICMP"></a>ICMP</h3><h3 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h3><img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/IPV4_udp%E6%95%B0%E6%8D%AE%E6%8A%A5.jpg" class="" title="image">

<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/udp%E5%A4%B4%E9%83%A8%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%8C%BA.jpg" class="" title="image">

<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97UDPIPV4%E6%95%B0%E6%8D%AE%E6%8A%A5%E7%9A%84%E5%AD%97%E6%AE%B5.jpg" class="" title="image">
<p>UDP校验和覆盖了UDP头部、UDP数据和一个伪头部。<br>它不提供差错纠正、队列管理、流量控制和拥塞控制。</p>
<p>面向数据报：应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。因此，应用程序必须选择合适大小的报文。若报文太长，则IP层需要分片，降低效率。若太短，会是IP太小。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。这也就是说，应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。</p>
<h3 id="TCP-传输控制协议（初步）"><a href="#TCP-传输控制协议（初步）" class="headerlink" title="TCP: 传输控制协议（初步）"></a>TCP: 传输控制协议（初步）</h3><ul>
<li>差错纠正：接收方收报，校验和字段，校验失败时，不回ACK；发送端会重新发送。</li>
<li>流量控制（flow control）：接收方相对发送方太慢时，会窗口更新（window update）,告诉发送方，调整其窗口大小，从而降低发送包的速度</li>
<li>拥塞控制(congestion control)：中间的网络（比如路由器）丢包，减缓TCP发送</li>
<li>面向字节流： 虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序看成是一连串的无结构的字节流。TCP有一个缓冲，当应用程序传送的数据块太长，TCP就可以把它划分短一些再传送。如果应用程序一次只发送一个字节，TCP也可以等待积累有足够多的字节后再构成报文段发送出去。</li>
</ul>
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E5%8F%91%E9%80%81%E6%96%B9%E7%AA%97%E5%8F%A3.png" class="" title="image">

<ul>
<li><p>分组窗口（window）</p>
</li>
<li><p>窗口大小（windows size）</p>
</li>
<li><p>滑动窗口（sliding window）协议</p>
</li>
<li><p>组包（packetization）：把一个发送应用程序的字节流转换成一组IP可以携带的分组。分组的序列号表示了每个分组的第一个字节再整个数据流中的字节偏移，而不是分组号。在传输过程中允许重新组包(repacketization)</p>
</li>
<li><p>校验和：TCP维持了一个较强的校验和，该校验和涉及它的头部、任何相关应用程序数据和IP头部的所有字段。是一个端到端的伪头部，用于检测传送中引入的比特差错。</p>
</li>
<li><p>当TCP收到连接另一端的数据时，会发送一个确认。这个确认可能不会立即发送，一般会延迟片刻。TCP使用的ACK是累积的，从某种意义来讲，一个指示字节好N的ACK暗示着所有直到N的字节（但不包含N）已经成功接收了。</p>
</li>
<li><p>双工服务</p>
</li>
<li><p>使用序列号，TCP丢弃重复报文和记录以杂乱次序到达的报文段，将正确次序的数据交给应用程序。因此，TCP接收端可能会被迫先保持大序列号的数据不交给应用程序，直到确实的小序列号到达报文段（一个“洞“）被填满。</p>
</li>
</ul>
<h4 id="TCP头部和封装"><a href="#TCP头部和封装" class="headerlink" title="TCP头部和封装"></a>TCP头部和封装</h4><img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/TCP%E5%9C%A8IP%E6%95%B0%E6%8D%AE%E6%8A%A5%E4%B8%AD%E7%9A%84%E5%B0%81%E8%A3%85.png" class="" title="image">

<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/TCP%E5%A4%B4%E9%83%A8.png" class="" title="image">

<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/TCP_flags.png" class="" title="image">

<ul>
<li>TCP MSS</li>
</ul>
<p>MSS (Maximum Segment Size)是 TCP Layer (L4) 的属性, MSS 指的是 TCP payload的长度. 当在MTU 1500 的网络上传输时, MSS 为 1460 (即 1500 减去 20 字节 IP 头, 20 字节 TCP 头).</p>
<h3 id="TCP连接管理"><a href="#TCP连接管理" class="headerlink" title="TCP连接管理"></a>TCP连接管理</h3><ul>
<li><p>普通TCP连接的建立与终止</p>
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.png" class="" title="image"></li>
<li><p>TCP半关闭</p>
</li>
</ul>
<p>TCP的半关闭操作是指仅关闭数据流的⼀一个传输⽅方向, ⽽而两个半关闭操作合在⼀一起就能够关闭整个连接。<br>例如, 应⽤用程序表明“ 我 已经完成了了数据的发送⼯工作, 并发送⼀一个F I N 给对⽅方, 但是我仍然希望接收来⾃自对⽅方的数据 直到它发送⼀一个F I N 给我” 。伯克利利套接字的A P I 提供了了半关闭操作。应⽤用程序只需要调⽤用shutdown函数来代替基本的close函数, 就能实现上述操作。然⽽而, 绝⼤大部分应⽤用程序仍然会调⽤用close函数来同时关闭⼀一条连接的两个传输⽅方向。</p>
<p>图中左侧的客户端发起了半关闭</p>
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/TCP%E5%8D%8A%E5%85%B3%E9%97%AD.png" class="" title="image">

<ul>
<li>初始序列号（英文为：Initial Sequence Number，简称ISN）<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E4%BD%BF%E7%94%A8%E5%9B%BA%E5%AE%9A%E5%BA%8F%E5%88%97%E5%8F%B7%E7%9A%84%E9%97%AE%E9%A2%98.png" class="" title="image">
按理来说，B应该把这个数据报丢弃掉才是。但是由于A每次发送报文都使用了相同的序号（SEQ = 1），这可能会让B误认为这200字节是属于新建立的tcp连接的，因此B会对这200字节数据照收不误。这就导致B在收到新tcp连接的数据后，又收到旧tcp连接的数据，从而出现数据乱序的问题。</li>
</ul>
<p>这其实反映了tcp的一些缺点，如果被一些恶意攻击者加以利用tcp的这种缺点：选择合适的序号，ip地址和端口的话，就能伪造出一个tcp报文段，从而打断正常的tcp连接。但是初始化序号的方式（通过算法来随机生成序号）就会使序号难以猜出，也就不容易利用这种缺点来进行一些恶意攻击行为。</p>
<p>一般来说，这个序号的范围是0  ~  2^31 - 1之间，而且序号的生成也是随机的，</p>
<ul>
<li>TCP选项<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/TCP%E9%80%89%E9%A1%B9.jpg" class="" title="image"></li>
</ul>
<p>MSS: 最⼤段⼤小是指TCP协议所允许的从对⽅方接收到的最⼤大报⽂文段。只记录TCP的payload的字节数。典型值1460</p>
<ul>
<li><p>TIME_WAIT状态<br>等待通信另一方重传它的FIN</p>
</li>
<li><p>重置报文段</p>
</li>
</ul>
<p>当发现一个到达的报文段对于相关连接而言是不正确的，TCP就会发送一个重置报文段。TCP头部的RST位字段。重置报文段通常会导致TCP连接的快速拆卸。</p>
<pre><code>1）针对不存在端口的请求连接
2）终止一条连接（终止释放）： 
任何排队的数据都将被抛弃，一个重置报文段会被立即发送出去；重置报文段的接收方会说明通信另一端采用了终止的方式而不是正常关闭。
3）半开连接
如果再未告知另一端的情况下通信的一端关闭或终止连接，那么就认为该TCP连接处于半开状态。
发生在一方的主机崩溃的情况下。只要不同过半开连接传数据，正常工作的一端不会检测出另一端已经崩溃。
</code></pre>
<p>更多的rst情况参考<a href="https://zhuanlan.zhihu.com/p/30791159">tcp rst产生的几种情况</a></p>
<h3 id="TCP超时与重传"><a href="#TCP超时与重传" class="headerlink" title="TCP超时与重传"></a>TCP超时与重传</h3><p>RTO： Retransmission Timeout 重传超时</p>
<p>RTT： Round Trip Time</p>
<h3 id="TCP数据流与窗口管理"><a href="#TCP数据流与窗口管理" class="headerlink" title="TCP数据流与窗口管理"></a>TCP数据流与窗口管理</h3><ul>
<li><p>交互式通信</p>
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E4%BA%A4%E4%BA%92%E5%BC%8F%E9%80%9A%E4%BF%A1.png" class="" title="image"></li>
<li><p>延时确认（Delayed Acknowledgement）</p>
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E5%BB%B6%E6%97%B6%E7%A1%AE%E8%AE%A4.png" class="" title="image"></li>
<li><p>Nagle算法</p>
</li>
</ul>
<p>Nagle算法要求, 当⼀个TCP连接中有在传数据(即那些已发送但还未经确认的数据),小的报文段(长度小于SMSS)就不能被发送, 直到所有的在传数据都收到ACK并且, 在收到ACK后, TCP需要收集这些小数据, 将其整合到⼀个报⽂文段中发送。</p>
<pre><code>迫使TCP遵循停等( stop-and-Wait)规程⼀只有等接收到所有在传数据的ACK后才能继续发送。
该算法的精妙之处在于它实现了⾃时钟( self-clocking) 控制: ACK返回越快, 数据传输也越快。在相对高延迟的广域⽹网中, 更需要减少微型报的数⽬目, 该算法使得单位时间内发送的报⽂文段数目更少。也就是说, RTT控制着发包速率。
</code></pre>
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/Nagle%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94.png" class="" title="image">

<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E5%BB%B6%E6%97%B6ACK%E5%92%8CNagle%E7%AE%97%E6%B3%95%E7%9B%B8%E7%BB%93%E5%90%88.png" class="" title="image">

<ul>
<li><p>流量控制与窗口管理</p>
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E5%8F%91%E9%80%81%E7%AB%AF%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3.png" class="" title="image">
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/%E6%8E%A5%E6%94%B6%E7%AB%AF%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3.png" class="" title="image">

<pre><code>  当TPC通信的接收方的接收速度无法匹配发送速度时，发送方会降低发送速度。
  TCP的流量控制机制完成了对发送速率的调节，它是基于ACK数据包中的通告窗口大小来实现的。
  这种方式提供了明确的接收方返回的状态信息，避免接收方缓存溢出。
</code></pre>
</li>
</ul>
<h3 id="TCP拥塞控制（Congestion-Control）"><a href="#TCP拥塞控制（Congestion-Control）" class="headerlink" title="TCP拥塞控制（Congestion Control）"></a>TCP拥塞控制（Congestion Control）</h3><p>路由器因无法处理高速率到达的流量而被迫丢弃数据信息的现象称为拥塞。</p>
<h3 id="TCP保活机制（KeepAlive）"><a href="#TCP保活机制（KeepAlive）" class="headerlink" title="TCP保活机制（KeepAlive）"></a>TCP保活机制（KeepAlive）</h3><p>TCP KeepAlive是⼀种在不不影响数据流内容的情况下探测对方的⽅方式。它是由⼀个保活计时器实现的。当计时器器被激发, 连接一端将发送一个保活探测(简称保活) 报文, 另⼀一端接收报⽂的同时会发送⼀个ACK作为响应。</p>
<ul>
<li>TCP KeepAlive的作用</li>
</ul>
<p>保活功能一般是为服务器器应用程序提供的, 服务器应用程序希望知道客户主机是否崩溃或离开, 从而决定是否为客户端绑定资源。</p>
<p>利用TCP保活功能来探测离开的主机, 有助于服务器与⾮交互性客户端进行相对短时间的对话, 例如, Web服务器、POP和IMAP电子邮件服务器器。短时间内一般网络稳定，且在相对短时间内一般能操作完。当操作完后，所以即使网络抖动原因，被keepalive断开连接，影响不大。</p>
<p>而更多地实现⻓时间交互服务的服务器可能不希望使用保活功能, 如ssh和Windows远程桌面这样的远程登录系统。需要长时间操作的，不希望断开连接，即使有网络抖动。</p>
<ul>
<li>KeepAlive报文序列号<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/KeepAlive%E6%8A%A5%E6%96%87%E5%BA%8F%E5%88%97%E5%8F%B7.png" class="" title="image"></li>
</ul>
<img src="/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/TCPKeepAlive%E6%8E%A2%E6%B5%8B%E6%9C%BA%E5%88%B6.png" class="" title="image">


]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title>通过delve(dlv)调试golang程序</title>
    <url>/2019/08/25/%E9%80%9A%E8%BF%87delve-dlv-%E8%B0%83%E8%AF%95golang%E7%A8%8B%E5%BA%8F/</url>
    <content><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>gdb是强大的调试工具。但对于golang程序来说，delve是个更好的选择，它能更好地理解Go runtime, data structures, and expressions，尤其是goroutine。</p>
<p>以下描述引用自<a href="https://golang.google.cn/doc/gdb">Debugging Go Code with GDB</a>【1】</p>
<blockquote>
<p>Note that Delve is a better alternative to GDB when debugging Go programs built with the standard toolchain. It understands the Go runtime, data structures, and expressions better than GDB. Delve currently supports Linux, OSX, and Windows on amd64. For the most up-to-date list of supported platforms, please see the Delve documentation.</p>
</blockquote>
<span id="more"></span>
<blockquote>
<p>GDB does not understand Go programs well. The stack management, threading, and runtime contain aspects that differ enough from the execution model GDB expects that they can confuse the debugger and cause incorrect results even when the program is compiled with gccgo. As a consequence, although GDB can be useful in some situations (e.g., debugging Cgo code, or debugging the runtime itself), it is not a reliable debugger for Go programs, particularly heavily concurrent ones. Moreover, it is not a priority for the Go project to address these issues, which are difficult.</p>
</blockquote>
<p>Delve is a debugger for the Go programming language【2】.</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>安装过程参考<a href="https://github.com/go-delve/delve/tree/master/Documentation/installation">https://github.com/go-delve/delve/tree/master/Documentation/installation</a></p>
<p>安装好后，可以查看版本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  _posts dlv version</span><br><span class="line">Delve Debugger</span><br><span class="line">Version: 1.1.0</span><br><span class="line">Build: $Id: 1990ba12450cab9425a2ae62e6ab988725023d5c</span><br></pre></td></tr></table></figure>

<h1 id="调试方法"><a href="#调试方法" class="headerlink" title="调试方法"></a>调试方法</h1><p>dlv提供了多种调试命令，如debug、exec、attach、core等，具体见官网文档或help信息。接下介绍常用的几种。</p>
<h2 id="dlv-debug"><a href="#dlv-debug" class="headerlink" title="dlv debug"></a>dlv debug</h2><p>这是一个代码工程实例, 位于目录GoWorks/GoDbg。</p>
<img src="/2019/08/25/%E9%80%9A%E8%BF%87delve-dlv-%E8%B0%83%E8%AF%95golang%E7%A8%8B%E5%BA%8F/godbg_project.jpg" class="" title="image">

<ul>
<li>如果位于工程目录下，可以dlv debug开始调试；</li>
<li>也可以指定目录，dlv debug GoWorks/GoDbg; </li>
<li>如果要传入参数，添加–后指定， 如dlv debug GoWorks/GoDbg – -arg1 value</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  GoDbg dlv debug GoWorks/GoDbg</span><br><span class="line">Type &#x27;help&#x27; for list of commands.</span><br><span class="line">(dlv) break main.main</span><br><span class="line">Breakpoint 1 set at 0x1089f6b for main.main() ./main.go:7</span><br><span class="line">(dlv) continue</span><br><span class="line">&gt; main.main() ./main.go:7 (hits goroutine(1):1 total:1) (PC: 0x1089f6b)</span><br><span class="line">     2:	import (</span><br><span class="line">     3:		&quot;GoWorks/GoDbg/mylib&quot;</span><br><span class="line">     4:		&quot;fmt&quot;</span><br><span class="line">     5:		&quot;os&quot;</span><br><span class="line">     6:	)</span><br><span class="line">=&gt;   7:	func main() &#123;</span><br><span class="line">     8:		fmt.Println(&quot;Golang dbg test...&quot;)</span><br><span class="line">     9:		var argc = len(os.Args)</span><br><span class="line">    10:		var argv = append([]string&#123;&#125;, os.Args...)</span><br><span class="line">    11:		fmt.Printf(&quot;argc:%d\n&quot;, argc)</span><br><span class="line">    12:		fmt.Printf(&quot;argv:%v\n&quot;, argv)</span><br><span class="line">(dlv) next</span><br><span class="line">&gt; main.main() ./main.go:8 (PC: 0x1089f82)</span><br><span class="line">     3:		&quot;GoWorks/GoDbg/mylib&quot;</span><br><span class="line">     4:		&quot;fmt&quot;</span><br><span class="line">     5:		&quot;os&quot;</span><br><span class="line">     6:	)</span><br><span class="line">     7:	func main() &#123;</span><br><span class="line">=&gt;   8:		fmt.Println(&quot;Golang dbg test...&quot;)</span><br><span class="line">     9:		var argc = len(os.Args)</span><br><span class="line">    10:		var argv = append([]string&#123;&#125;, os.Args...)</span><br><span class="line">    11:		fmt.Printf(&quot;argc:%d\n&quot;, argc)</span><br><span class="line">    12:		fmt.Printf(&quot;argv:%v\n&quot;, argv)</span><br><span class="line">    13:		var var1 = 1</span><br></pre></td></tr></table></figure>

<p>具体有哪些debug命令，可以help来查看，与gdb的很类似，goroutine/goroutines的是dlv中特有的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(dlv) help</span><br><span class="line">The following commands are available:</span><br><span class="line">    args ------------------------ Print function arguments.</span><br><span class="line">    break (alias: b) ------------ Sets a breakpoint.</span><br><span class="line">    breakpoints (alias: bp) ----- Print out info for active breakpoints.</span><br><span class="line">    call ------------------------ Resumes process, injecting a function call (EXPERIMENTAL!!!)</span><br><span class="line">    clear ----------------------- Deletes breakpoint.</span><br><span class="line">    clearall -------------------- Deletes multiple breakpoints.</span><br><span class="line">    condition (alias: cond) ----- Set breakpoint condition.</span><br><span class="line">    config ---------------------- Changes configuration parameters.</span><br><span class="line">    continue (alias: c) --------- Run until breakpoint or program termination.   // 继续运行</span><br><span class="line">    deferred -------------------- Executes command in the context of a deferred call.</span><br><span class="line">    disassemble (alias: disass) - Disassembler.</span><br><span class="line">    down ------------------------ Move the current frame down.</span><br><span class="line">    edit (alias: ed) ------------ Open where you are in $DELVE_EDITOR or $EDITOR</span><br><span class="line">    exit (alias: quit | q) ------ Exit the debugger.</span><br><span class="line">    frame ----------------------- Set the current frame, or execute command on a different frame.</span><br><span class="line">    funcs ----------------------- Print list of functions.</span><br><span class="line">    goroutine (alias: gr) ------- Shows or changes current goroutine</span><br><span class="line">    goroutines (alias: grs) ----- List program goroutines.</span><br><span class="line">    help (alias: h) ------------- Prints the help message.</span><br><span class="line">    libraries ------------------- List loaded dynamic libraries</span><br><span class="line">    list (alias: ls | l) -------- Show source code.    // 显示代码</span><br><span class="line">    locals ---------------------- Print local variables.</span><br><span class="line">    next (alias: n) ------------- Step over to next source line.  // 单步下一句</span><br><span class="line">    on -------------------------- Executes a command when a breakpoint is hit.</span><br><span class="line">    print (alias: p) ------------ Evaluate an expression.</span><br><span class="line">    regs ------------------------ Print contents of CPU registers.</span><br><span class="line">    restart (alias: r) ---------- Restart process.</span><br><span class="line">    set ------------------------- Changes the value of a variable.</span><br><span class="line">    source ---------------------- Executes a file containing a list of delve commands</span><br><span class="line">    sources --------------------- Print list of source files.</span><br><span class="line">    stack (alias: bt) ----------- Print stack trace.</span><br><span class="line">    step (alias: s) ------------- Single step through program.  // 进入函数内,单步下一句</span><br><span class="line">    step-instruction (alias: si)  Single step a single cpu instruction.</span><br><span class="line">    stepout (alias: so) --------- Step out of the current function.</span><br><span class="line">    thread (alias: tr) ---------- Switch to the specified thread.</span><br><span class="line">    threads --------------------- Print out info for every traced thread.</span><br><span class="line">    trace (alias: t) ------------ Set tracepoint.</span><br><span class="line">    types ----------------------- Print list of types</span><br><span class="line">    up -------------------------- Move the current frame up.</span><br><span class="line">    vars ------------------------ Print package variables.</span><br><span class="line">    whatis ---------------------- Prints type of an expression.</span><br><span class="line">Type help followed by a command for full documentation.</span><br></pre></td></tr></table></figure>

<h2 id="dlv-exec"><a href="#dlv-exec" class="headerlink" title="dlv exec"></a>dlv exec</h2><p>Execute a precompiled binary, and begin a debug session.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  file ./main</span><br><span class="line">./main: Mach-O 64-bit executable x86_64</span><br><span class="line">➜  dlv exec ./main</span><br><span class="line">Type &#x27;help&#x27; for list of commands.</span><br><span class="line">(dlv)</span><br></pre></td></tr></table></figure>


<h2 id="dlv-attach"><a href="#dlv-attach" class="headerlink" title="dlv attach"></a>dlv attach</h2><p>Attach to running process and begin debugging.</p>
<p>注意：<strong>在退出时，有可选是否要kill该进程</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@yg-man-uhost-set9-01 ~]# dlv attach 22063</span><br><span class="line">Type &#x27;help&#x27; for list of commands.</span><br><span class="line">(dlv) q</span><br><span class="line">Would you like to kill the process? [Y/n] n</span><br></pre></td></tr></table></figure>

<h2 id="dlv-core"><a href="#dlv-core" class="headerlink" title="dlv core"></a>dlv core</h2><p>Examine a core dump.</p>
<p>注意：<strong>dlv不支持生成core，可以通过gdb attach上去后，执行gcore来生成core；然后再dlv core来调试</strong>。</p>
<h1 id="线上调试的例子"><a href="#线上调试的例子" class="headerlink" title="线上调试的例子"></a>线上调试的例子</h1><h2 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h2><p>线上一个服务出现了问题，看日志是一个接口的程序跑了一半，后面不再执行下去了。</p>
<p>dlv attach上去后，看到有很多.runtime_SemacquireMutex这个的goroutine，一直卡着。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Goroutine 28232679 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28232684 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28232685 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28232686 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28232688 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28232704 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28232705 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28232710 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28232714 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28232715 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28232718 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28232719 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28232721 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28232726 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28232727 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28232939 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28232940 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28239464 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28239465 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28239914 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28239915 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28239938 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28239939 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28239965 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28239966 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28239968 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28239984 - User: /Users/patrick.xu/go/src/uframework/task/tcp_task.go:84 uframework/task.(*TCPTask).Run (0x6ce8cc)</span><br><span class="line">Goroutine 28239985 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28240001 - User: /usr/local/go/src/runtime/sema.go:62 sync.runtime_SemacquireMutex (0x43cb74)</span><br><span class="line">Goroutine 28240961 - User: /Users/patrick.xu/go/src/uframework/log/logext.go:281 uframework/log.RealWrite (0x6bff2d)</span><br><span class="line">Goroutine 28241103 - User: /Users/patrick.xu/go/src/uframework/utils/zookeeper/zk/conn.go:515 uframework/utils/zookeeper/zk.(*Conn).sendLoop (0x7c7835)</span><br><span class="line">Goroutine 28241104 - User: /usr/local/go/src/runtime/netpoll.go:164 net.runtime_pollWait (0x426ec9)</span><br><span class="line">Goroutine 28241126 - User: /usr/local/go/src/runtime/sema.go:47 sync.runtime_Semacquire (0x43ca94)</span><br></pre></td></tr></table></figure>

<p>看这个goroutine卡在了0x000000000046c45d in sync.(*Mutex).Lock，对应业务代码uhost-go/uhost-scheduler/logic.updateBuffer。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(dlv) gr 28239985</span><br><span class="line">Switched from 28241159 to 28239985 (thread 31788)</span><br><span class="line">(dlv) bt</span><br><span class="line">0  0x000000000042c76a in runtime.gopark</span><br><span class="line">   at /usr/local/go/src/runtime/proc.go:272</span><br><span class="line">1  0x000000000042c84e in runtime.goparkunlock</span><br><span class="line">   at /usr/local/go/src/runtime/proc.go:277</span><br><span class="line">2  0x000000000043ce91 in runtime.semacquire</span><br><span class="line">   at /usr/local/go/src/runtime/sema.go:130</span><br><span class="line">3  0x000000000043cb74 in sync.runtime_SemacquireMutex</span><br><span class="line">   at /usr/local/go/src/runtime/sema.go:62</span><br><span class="line">4  0x000000000046c45d in sync.(*Mutex).Lock</span><br><span class="line">   at /usr/local/go/src/sync/mutex.go:87</span><br><span class="line">5  0x0000000000817b1d in uhost-go/uhost-scheduler/logic.updateBuffer</span><br><span class="line">   at /Users/patrick.xu/go/src/uhost-go/uhost-scheduler/logic/get_suitable_resource.go:418</span><br><span class="line">6  0x0000000000814bd7 in uhost-go/uhost-scheduler/logic.getSuitableResource</span><br><span class="line">   at /Users/patrick.xu/go/src/uhost-go/uhost-scheduler/logic/get_suitable_resource.go:328</span><br><span class="line">7  0x00000000006cecd4 in uframework/task.TCPTaskFunc.ServeTCP</span><br><span class="line">   at /Users/patrick.xu/go/src/uframework/task/tcp_task_handle.go:27</span><br><span class="line">8  0x00000000006d02b8 in uframework/task.(*TCPTask).Run.func1</span><br><span class="line">   at /Users/patrick.xu/go/src/uframework/task/tcp_task.go:66</span><br><span class="line">9  0x0000000000458dd1 in runtime.goexit</span><br><span class="line">   at /usr/local/go/src/runtime/asm_amd64.s:2197</span><br></pre></td></tr></table></figure>

<p>再去看业务代码， <strong>frame 5 list</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(dlv) frame 5 list</span><br><span class="line">Goroutine 28239985 frame 5 at /Users/patrick.xu/go/src/uhost-go/uhost-scheduler/logic/get_suitable_resource.go:418 (PC: 0x817b1d)</span><br><span class="line">Command failed: open /Users/patrick.xu/go/src/uhost-go/uhost-scheduler/logic/get_suitable_resource.go: no such file or directory</span><br></pre></td></tr></table></figure>

<p>由于线上没有源代码，提示文件不存在。到本地查看这行的代码。</p>
<img src="/2019/08/25/%E9%80%9A%E8%BF%87delve-dlv-%E8%B0%83%E8%AF%95golang%E7%A8%8B%E5%BA%8F/lock.jpg" class="" title="image">

<p>定位到这行后，再结合业务逻辑去分析，就能定位到是哪里问题，造成了这里的死锁。</p>
<p>最后退出来后，注意向上代码别重启。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(dlv) q</span><br><span class="line">Would you like to kill the process? [Y/n] n</span><br></pre></td></tr></table></figure>

<p>最后查明原因是：</p>
<p>调用updatePUhostSlice时，有多个不同wh.hostId的mapHostIdLock[wh.hostId].Lock()，没执行完时不会unlock；如果有并发调用updatePUhostSlice，且遍历whs的顺序和第一次不一样，有可能陷入死锁。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">func updatePUhostSlice(</span><br><span class="line">        ......</span><br><span class="line">        for _, wh := range whs &#123;</span><br><span class="line"></span><br><span class="line">             if mapHostIdLock[wh.hostId] == nil &#123;</span><br><span class="line">                 mapHostIdLock[wh.hostId] = new(sync.Mutex)</span><br><span class="line">             &#125;</span><br><span class="line">              mapHostIdLock[wh.hostId].Lock()</span><br><span class="line">              defer mapHostIdLock[wh.hostId].Unlock()</span><br><span class="line">              ......</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>改进方法是：<br>把Lock()和defer Unlock()抽到一个函数里。注意：不能在循环里出现直接调用Lock()的代码。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">func updatePUhostSlice(</span><br><span class="line">        ......</span><br><span class="line">        for _, wh := range whs &#123;</span><br><span class="line">               recheckHostResource(xxx......)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func recheckHostResource(</span><br><span class="line">       sessionNo string,</span><br><span class="line">       hostId uint64,</span><br><span class="line">       alreadyMatched *uint32,</span><br><span class="line">       pBHRrs *uhost.HostResourceSurvey,</span><br><span class="line">       uhostParams UhostParams,</span><br><span class="line">       )(isEnough bool) &#123;</span><br><span class="line"></span><br><span class="line">    if mapHostIdLock[hostId] == nil &#123;</span><br><span class="line">       mapHostIdLock[hostId] = new(sync.Mutex)</span><br><span class="line">    &#125;</span><br><span class="line">    mapHostIdLock[hostId].Lock()</span><br><span class="line">    defer mapHostIdLock[hostId].Unlock()</span><br><span class="line">    .......</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h2 id="和gdb的对比"><a href="#和gdb的对比" class="headerlink" title="和gdb的对比"></a>和gdb的对比</h2><p>gdb能看出的信息有限，只能定位到线程这个层面，无法到goroutine这一层，很难定位出问题。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(gdb) info threads</span><br><span class="line">* 1 process 10732  runtime.epollwait () at /usr/local/go/src/runtime/sys_linux_amd64.s:560</span><br><span class="line">(gdb) thread apply all bt</span><br><span class="line"></span><br><span class="line">Thread 1 (process 10732):</span><br><span class="line">#0  runtime.epollwait () at /usr/local/go/src/runtime/sys_linux_amd64.s:560</span><br><span class="line">#1  0x00000000004280d1 in runtime.netpoll (block=true, ~r1=0x1) at /usr/local/go/src/runtime/netpoll_epoll.go:67</span><br><span class="line">#2  0x0000000000430f8f in runtime.findrunnable (gp#10=0xc42001c000, inheritTime=false) at /usr/local/go/src/runtime/proc.go:2084</span><br><span class="line">#3  0x0000000000431aec in runtime.schedule () at /usr/local/go/src/runtime/proc.go:2222</span><br><span class="line">#4  0x0000000000431deb in runtime.park_m (gp=0xc4200011e0) at /usr/local/go/src/runtime/proc.go:2285</span><br><span class="line">#5  0x000000000045626b in runtime.mcall () at /usr/local/go/src/runtime/asm_amd64.s:269</span><br><span class="line">#6  0x0000000000cebd00 in runtime.work ()</span><br><span class="line">#7  0x00007ffe3162bc70 in ?? ()</span><br><span class="line">#8  0x0000000000cebd80 in runtime.work ()</span><br><span class="line">#9  0x00007ffe3162bc60 in ?? ()</span><br><span class="line">#10 0x000000000042f0e4 in runtime.mstart () at /usr/local/go/src/runtime/proc.go:1149</span><br><span class="line">#11 0x00000000004560d9 in runtime.rt0_go () at /usr/local/go/src/runtime/asm_amd64.s:169</span><br><span class="line">#12 0x0000000000000009 in ?? ()</span><br><span class="line">#13 0x00007ffe3162bca8 in ?? ()</span><br><span class="line">#14 0x0000000000000009 in ?? ()</span><br><span class="line">#15 0x00007ffe3162bca8 in ?? ()</span><br><span class="line">#16 0x0000000000000000 in ?? ()</span><br></pre></td></tr></table></figure>

<p>但可以用gdb产生core，采集一些现场信息，然后重启尽快恢复业务，事后再用dlv分析core来定位原因。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://golang.google.cn/doc/gdb">Debugging Go Code with GDB</a></li>
<li><a href="https://github.com/go-delve/delve">go-delve/delve</a></li>
<li><a href="http://lday.me/2017/02/27/0005_gdb-vs-dlv/">Golang程序调试工具介绍(gdb vs dlv)</a><ol start="4">
<li><a href="https://zhuanlan.zhihu.com/p/66228858">实战分析一个运行起来会卡死的Go程序</a></li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>debug</tag>
        <tag>delve</tag>
        <tag>dlv</tag>
      </tags>
  </entry>
  <entry>
    <title>这些年读过的那些书&amp;&amp;待读清单列表</title>
    <url>/2019/08/31/%E8%BF%99%E4%BA%9B%E5%B9%B4%E8%AF%BB%E8%BF%87%E7%9A%84%E9%82%A3%E4%BA%9B%E4%B9%A6-%E5%BE%85%E8%AF%BB%E6%B8%85%E5%8D%95%E5%88%97%E8%A1%A8/</url>
    <content><![CDATA[<p>每年给自己列个计划，要读哪些书，到年尾看看真正看完的有多少。</p>
<h1 id="2020年待读清单列表"><a href="#2020年待读清单列表" class="headerlink" title="2020年待读清单列表"></a>2020年待读清单列表</h1><ol>
<li> 《Designing Data-Intensive Applications》</li>
</ol>
<h1 id="2019年待读清单列表"><a href="#2019年待读清单列表" class="headerlink" title="2019年待读清单列表"></a>2019年待读清单列表</h1><ol>
<li>UNIX环境高级编程（第3版）</li>
<li>重构网络 SDN架构与实现</li>
<li>软件定义网络</li>
<li>全球通史 从史前史到21实际 第7版修订版 上</li>
<li>全球通史 从史前史到21实际 第7版修订版 下<span id="more"></span></li>
</ol>
<h1 id="这些年读过的那些书"><a href="#这些年读过的那些书" class="headerlink" title="这些年读过的那些书"></a>这些年读过的那些书</h1><h2 id="2020年"><a href="#2020年" class="headerlink" title="2020年"></a>2020年</h2><ol>
<li>《深度思考》 深度思考前的盲目勤奋，注定是吃力不讨好的徒劳。香奈儿前全球CEO从普通职员到职场巅峰的人生进阶之路。大脑中走得越远，现实中走得越稳。</li>
</ol>
<h2 id="2019年"><a href="#2019年" class="headerlink" title="2019年"></a>2019年</h2><ol>
<li>《TCP/IP详解 卷1：协议（原书第2版）》 <a href="https://xusenqi.github.io/2019/04/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-TCP-IP%E8%AF%A6%E8%A7%A3-%E5%8D%B71%EF%BC%9A%E5%8D%8F%E8%AE%AE/">读书笔记-TCP/IP详解 卷1：协议</a></li>
<li>《软技能 代码之外的生存指南（Soft Skills - The Software Developer’s Life Manual）》对于程序员是很不错的心灵鸡汤，从职业、自我营销、学习、生产力等方面，讲述了作者的方法和心得，很值得借鉴。</li>
</ol>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>清单</tag>
      </tags>
  </entry>
  <entry>
    <title>c++服务内存持续增长问题的解决</title>
    <url>/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>线上有个用C++写的b服务，随着运行时间的增减，占用内存也会不断增加，最终kernel会触发OOM，导致服务不可用。</p>
<img src="/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/1_%E5%86%85%E5%AD%98%E5%A2%9E%E9%95%BF.png" class="" title="image.png">
<p>所使用的物理机内存为160GB，以上图中一个broker进程就占了大约6.6GB的内存，运行了10天后内存还在持续增长，存在内存泄漏的问题。</p>
<span id="more"></span>

<h1 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h1><h2 id="代码review"><a href="#代码review" class="headerlink" title="代码review"></a>代码review</h2><p>主要以下几个方面：</p>
<ul>
<li>malloc/free、new/delete、new []/delete []是否都配套使用了</li>
<li>构造函数里申请的内存，在析构函数里都有相应的释放</li>
<li>指针的错误使用，没有释放内存。</li>
</ul>
<p>但没有发现有问题的地方。</p>
<h2 id="内存泄漏检查工具"><a href="#内存泄漏检查工具" class="headerlink" title="内存泄漏检查工具"></a>内存泄漏检查工具</h2><h2 id="valgrind"><a href="#valgrind" class="headerlink" title="valgrind"></a>valgrind</h2><h3 id="valgrind介绍"><a href="#valgrind介绍" class="headerlink" title="valgrind介绍"></a>valgrind介绍</h3><p><code>Valgrind</code>是一个开源工具包，提供了许多调试和分析工具。</p>
<ul>
<li><strong>Memcheck</strong>     detects memory-management problems</li>
<li>Cachegrind    a cache profiler</li>
<li>Callgrind        Cachegrind  +  callgraphs（调用关系图）</li>
<li><strong>Massif</strong>           a heap profiler</li>
<li>Helgrind         a thread debugger which finds data races in multithreaded programs</li>
<li>……</li>
</ul>
<h3 id="代码编译"><a href="#代码编译" class="headerlink" title="代码编译"></a>代码编译</h3><p>set(CMAKE_CXX_FLAGS “${CMAKE_CXX_FLAGS} -std=c++11 -g <strong>-O0</strong> -mavx2 -Wall -DDEBUG_RING”)</p>
<ul>
<li>-g  include debugging information so that Memcheck’s error messages include exact line numbers</li>
<li>-O0  禁止编译器的优化，确保valgrind的输出信息的准确性</li>
</ul>
<h3 id="Memcheck的使用"><a href="#Memcheck的使用" class="headerlink" title="Memcheck的使用"></a>Memcheck的使用</h3><p><code>valgrind --leak-check=yes --track-origins=yes /root/ufile/XsqUFileBroker-set7/UFileBroker -c /root/ufile/XsqUFileBroker-set7/config-set7.ini</code></p>
<p>线上不建议开启，能使性能下降10~30倍，线上实测下来最多有下降200倍。</p>
<p>在正常压测下，并不会出现内存泄漏的错误信息。但当把broker依赖的底层服务osd重启后，立刻出现以下的错误信息。</p>
<p>错误信息1：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">==31907== 773,314 (9,216 direct, 764,098 indirect) bytes in 288 blocks are definitely lost in loss record 1,997 of 2,023</span><br><span class="line">==31907==    at 0x4C2A5B3: operator new(unsigned long) (vg_replace_malloc.c:342)</span><br><span class="line">==31907==    by 0x63FD3B: uevent::ConnectionLibevent::Init() (connection_libevent.cc:67)</span><br><span class="line">==31907==    by 0x63AECA: uevent::ConnectorLibevent::Connect() (connector_libevent.cc:30)</span><br><span class="line">==31907==    by 0x5FBAAE: BrokerServerHandle::GetConnection(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;, std::string const&amp;, unsigned int) (broker_server.cc:587)</span><br><span class="line">==31907==    by 0x600644: BrokerManagerHandle::GetConnection(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;, std::string const&amp;, unsigned int) (broker_manager.cc:87)</span><br><span class="line">==31907==    by 0x5E0194: BrokerGetHandle::GetECOnePieceData(int) (broker_get.cc:312)</span><br><span class="line">==31907==    by 0x5E1EBF: BrokerGetHandle::GetECObjectRequest() (broker_get.cc:371)</span><br><span class="line">==31907==    by 0x5E3913: BrokerGetHandle::EntryInit(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;, std::shared_ptr&lt;MessageHeader&gt; const&amp;) (broker_get.cc:110)</span><br><span class="line">==31907==    by 0x5F7AC1: BrokerServerHandle::MessageDispatchHandle(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;, std::shared_ptr&lt;MessageHeader&gt; const&amp;) (broker_server.cc:173)</span><br><span class="line">==31907==    by 0x5FAAA3: BrokerServerHandle::MessageReadHandle(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;) (broker_server.cc:108)</span><br><span class="line">==31907==    by 0x5CE779: std::_Function_handler&lt;void (std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;), void (*)(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;)&gt;::_M_invoke(std::_Any_data const&amp;, std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;) (functional:2071)</span><br><span class="line">==31907==    by 0x640D94: operator() (functional:2471)</span><br><span class="line">==31907==    by 0x640D94: uevent::ConnectionLibevent::HandleReadEvent(int, std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;) (connection_libevent.cc:114)</span><br></pre></td></tr></table></figure>

<p>问题：某个osd进程关闭后，broker对保存长连接而占用的内存，没有释放。<br>修复方法：调用delete释放。</p>
<img src="/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/2_add_delete_it.png" class="" title="image.png">

<p>注意：<br>这里的connectors类型是std::unordered_map&lt;ConnectorKey, uevent::ConnectorUevent*, ConnectorKeyHash&gt; connectors_<br>obj_handle→connectors_.erase(it)只会删除指针本身，并不会销毁指针指向的内容，所以导致内存泄漏。<br>参考<a href="https://stackoverflow.com/questions/39605945/remove-element-from-unordered-map-without-calling-destructor-on-it">https://stackoverflow.com/questions/39605945/remove-element-from-unordered-map-without-calling-destructor-on-it</a></p>
<p>错误信息2：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">==31907== 4,405,094 (507,936 direct, 3,897,158 indirect) bytes in 1,716 blocks are definitely lost in loss record 2,018 of 2,023</span><br><span class="line">==31907== at 0x4C2A5B3: operator new(unsigned long) (vg_replace_malloc.c:342)</span><br><span class="line">==31907== by 0x5FB78C: BrokerServerHandle::GetConnection(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;, std::string const&amp;, unsigned int) (broker_server.cc:578)</span><br><span class="line">==31907== by 0x600644: BrokerManagerHandle::GetConnection(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;, std::string const&amp;, unsigned int) (broker_manager.cc:87)</span><br><span class="line">==31907== by 0x5E0194: BrokerGetHandle::GetECOnePieceData(int) (broker_get.cc:312)</span><br><span class="line">==31907== by 0x5E1EBF: BrokerGetHandle::GetECObjectRequest() (broker_get.cc:371)</span><br><span class="line">==31907== by 0x5E3913: BrokerGetHandle::EntryInit(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;, std::shared_ptr&lt;MessageHeader&gt; const&amp;) (broker_get.cc:110)</span><br><span class="line">==31907== by 0x5F7AC1: BrokerServerHandle::MessageDispatchHandle(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;, std::shared_ptr&lt;MessageHeader&gt; const&amp;) (broker_server.cc:173)</span><br><span class="line">==31907== by 0x5FAAA3: BrokerServerHandle::MessageReadHandle(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;) (broker_server.cc:108)</span><br><span class="line">==31907== by 0x5CE779: std::_Function_handler&lt;void (std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;), void (*)(std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;)&gt;::_M_invoke(std::_Any_data const&amp;, std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;) (functional:2071)</span><br><span class="line">==31907== by 0x640D94: operator() (functional:2471)</span><br><span class="line">==31907== by 0x640D94: uevent::ConnectionLibevent::HandleReadEvent(int, std::shared_ptr&lt;uevent::ConnectionUevent&gt; const&amp;) (connection_libevent.cc:114)</span><br><span class="line">==31907== by 0x4E47A13: event_base_loop (in /usr/lib64/libevent-2.0.so.5.1.9)</span><br><span class="line">==31907== by 0x630C14: uevent::PosixWorker::Running(std::string const&amp;) (posix_stack.cc:192)</span><br></pre></td></tr></table></figure>

<p>问题：某个osd进程关闭后，但数据库里还没有标记成BAD。此时broker还会尝试连接该osd，因连接失败不会发送请求，这部分逻辑正确。但是，失败的连接占据的内存没有释放。<br>修复方法：调用delete释放。</p>
<img src="/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/3_add_delete_connector.png" class="" title="image.png">


<p>修复以上2个错误后，用valgrind已经没有其他错误信息了。但是线上broker的内存，在osd没有重启的情况下，依然还会不断增长。这说明还有其他内存泄漏的地方。</p>
<h2 id="Dump内存并查找问题"><a href="#Dump内存并查找问题" class="headerlink" title="Dump内存并查找问题"></a>Dump内存并查找问题</h2><p>观察线上broker服务内存，在上次版本更新一周后，占用内存分布大致在1~5GB之间。<br>查看broker的虚拟内存分布，有80%的内存都是同一块内存空间，且是不同broker的共同现象。</p>
<img src="/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/4_pmap.png" class="" title="image.png">


<p>自己通过压测环境，也能查到类似现象。<br>压测环境：6块盘；线上环境：72块盘。</p>
<h3 id="dump内存"><a href="#dump内存" class="headerlink" title="dump内存"></a>dump内存</h3><p>所以这块内存是什么呢？可以dump下来查看。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gdb attach 3096</span><br><span class="line"></span><br><span class="line">(gdb) dump memory /tmp/3096_broker.dump 0x7fdf33400000 0x7fdf33410000</span><br><span class="line"></span><br><span class="line">strings /tmp/3096_broker.dump</span><br><span class="line">ZZZZ7_0:4096_broker-32576-25146--7-64115</span><br><span class="line">ZZZ$</span><br><span class="line">ZZZZ7_0:4096_broker-32576-25146--7-64280</span><br><span class="line">ZZZ$</span><br><span class="line">ZZZZ7_0:4096_broker-32576-25146--7-65085</span><br><span class="line">ZZZZ7_0:4096_broker-32576-25146--7-64116</span><br><span class="line">7_0:4096_broker-32576-25146--7-64355</span><br><span class="line">7_0:4096_broker-32576-25146--7-64605</span><br><span class="line">7_0:4096_broker-32576-25146--7-64117</span><br><span class="line">ZZZZ7_0:4096_broker-32576-25146--7-64227</span><br><span class="line">ZZZZ7_0:4096_broker-32576-25146--7-64235</span><br><span class="line">ZZZZ7_0:4096_broker-32576-25146--7-64118</span><br></pre></td></tr></table></figure>
<p>因为压测时上传的数据都是ZZZZ…格式的，可以判断跟上传文件的内容有关。</p>
<h3 id="valgrind-massif的使用"><a href="#valgrind-massif的使用" class="headerlink" title="valgrind massif的使用"></a>valgrind massif的使用</h3><p>再用valgrind massif的工具去剖析heap的占用情况。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">valgrind --tool=massif /root/ufile/XsqUFileBroker-set7/UFileBroker -c /root/ufile/XsqUFileBroker-set7/config-set7.ini</span><br><span class="line">ms_print massif.out.907</span><br></pre></td></tr></table></figure>

<p>发现有lru_cache的部分占用，但为了分析内存占用，lru_cache已经禁止使用了，但为什么还会有呢？</p>
<img src="/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/5_massif.png" class="" title="image.png">

<h3 id="lru-cache的问题"><a href="#lru-cache的问题" class="headerlink" title="lru_cache的问题"></a>lru_cache的问题</h3><p>再去查看跟lru_cache相关的代码，<br>7_0:4096_broker-32576-25146–7-64605的格式正好是定义的BROKER_CACHE_KEY的格式，</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">define BROKER_CACHE_KEY \</span></span><br><span class="line"><span class="bash">    to_string(SET_ID) + <span class="string">&quot;_&quot;</span> + to_string(offset_) + <span class="string">&quot;:&quot;</span> + to_string(data_size_) + <span class="string">&quot;_&quot;</span> + objectid_</span></span><br></pre></td></tr></table></figure>


<p>并进一步分析lru_cache相关的代码，最后发现又是unordered_map的错误使用。<br>每次get数据的时候，代码里都会判断下在lru_cache里是否已经存在；如果不存在，则插入该数据。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lru_cache.h</span><br><span class="line"></span><br><span class="line">private:</span><br><span class="line">  std::unordered_map&lt;K, Node&lt;K, T&gt; *&gt; umap;</span><br><span class="line">  </span><br><span class="line">void Put(K key, T data) &#123; </span><br><span class="line">  ......</span><br><span class="line">  Node&lt;K, T&gt; *node = umap[key]; //放入cache前，都会先判断下是否已经存在</span><br><span class="line">  if (node)&#123;</span><br><span class="line">    ......</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但是std::unordered_map&lt;&gt;::[key]在查不到时，会自动插入该数据。</p>
<blockquote>
<p>Returns a reference to the value that is mapped to a key equivalent to <code>key</code>, performing an insertion if such key does not already exist.</p>
</blockquote>
<p>到这里就真相大白了，罪魁祸首就是std::unordered_map&lt;&gt;::[key]的使用不当，每次get一个没有在cache里的key时，就会插入一条数据，导致内存不断增大。</p>
<h2 id="TCP-Connection的buffer问题"><a href="#TCP-Connection的buffer问题" class="headerlink" title="TCP Connection的buffer问题"></a>TCP Connection的buffer问题</h2><p>再压测时，pmap看时，还是有一块几个GB的内存空间，dump出来也看不出来是什么。</p>
<h3 id="Buffer-问题"><a href="#Buffer-问题" class="headerlink" title="Buffer 问题"></a>Buffer 问题</h3><p>那么到底是哪里占用的呢？<br>想到golang里面的heap profile，搜到C++里也能用类似的工具gperftools。<br>具体使用过程参考<a href="https://xusenqi.github.io/2020/12/06/C++Profile%E7%9A%84%E5%A4%A7%E6%9D%80%E5%99%A8_gperftools%E7%9A%84%E4%BD%BF%E7%94%A8/">c++ profile的大杀器-gperftools的使用</a>。</p>
<p>发现压测后占用3GB的内存中，95%都是每条TCP连接的buffer。</p>
<img src="/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/broker_1003_3GB.jpeg" class="" title="image.png">

<p>网络框架里有一个buffe模块r，维护tcp conncetion占用的Buffer。<br>Buffer底层由std::vector<char>实现，是一块连续的内存；可以自适应增长；保存数据并提供相应的访问函数。<br>上层代码，只要对Buffer发送数据或者接收数据，而不用管理具体的存放和容量等。</p>
<p>然后查看Buffer的代码，发现retrive（回收内存的函数）的实现逻辑有问题。</p>
<img src="/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/6_buffer_retrieve.png" class="" title="image.png">


<p>修改后再次压测，对比结果如下：<br>6磁盘的测试环境，跑压测脚本，证明修改的逻辑有效。</p>
<table>
<thead>
<tr>
<th></th>
<th>改进前</th>
<th>改进后</th>
</tr>
</thead>
<tbody><tr>
<td>retrieve后，&gt;=8Mb的buffer出现的次数</td>
<td>7219</td>
<td>449</td>
</tr>
</tbody></table>
<p>72磁盘的大数据测试环境，连续压测2h:</p>
<table>
<thead>
<tr>
<th></th>
<th>改进前</th>
<th>改进后</th>
</tr>
</thead>
<tbody><tr>
<td>占用内存RSS</td>
<td>4.5GB</td>
<td>3.0GB</td>
</tr>
</tbody></table>
<h3 id="Broker服务的内存模型"><a href="#Broker服务的内存模型" class="headerlink" title="Broker服务的内存模型"></a>Broker服务的内存模型</h3><p>进一步验证：既然是tcp conncetion占用的Buffer，那么把连接断开后，是否会释放buffer呢？</p>
<p>查看跟brorker服务的连接分为两类：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Broker服务的内存模型</span><br><span class="line">进一步验证：既然是tcp conncetion占用的Buffer，那么把连接断开后，是否会释放buffer呢？</span><br><span class="line"></span><br><span class="line">查看跟brorker服务的连接分为两类：</span><br><span class="line"></span><br><span class="line">// 总共的连接数</span><br><span class="line">[root@bj-ufile-storage-set-osd ~]# netstat -antp | grep 8219 | wc -l</span><br><span class="line">986</span><br><span class="line"> </span><br><span class="line">// broker --&gt; osd的连接</span><br><span class="line">[root@bj-ufile-storage-set-osd ~]# netstat -antp | grep 8219 | grep :200 | wc -l</span><br><span class="line">711</span><br><span class="line">[root@bj-ufile-storage-set-osd ~]# netstat -antp | grep 8219 | grep 172.22.18.9:20011 | wc -l</span><br><span class="line">10</span><br><span class="line">[root@bj-ufile-storage-set-osd ~]# netstat -antp | grep 8219 | grep 172.22.18.9:20011</span><br><span class="line">tcp        0      0 172.22.18.9:39996       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line">tcp        0      0 172.22.18.9:37220       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line">tcp        0      0 172.22.18.9:11742       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line">tcp        0      0 172.22.18.9:30720       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line">tcp        0      0 172.22.18.9:58094       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line">tcp        0      0 172.22.18.9:10776       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line">tcp        0      0 172.22.18.9:45502       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line">tcp        0      0 172.22.18.9:40626       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line">tcp        0      0 172.22.18.9:11806       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line">tcp        0      0 172.22.18.9:37098       172.22.18.9:20011       ESTABLISHED 8219/ufile-broker-s</span><br><span class="line"> </span><br><span class="line">// goproxy --&gt; broker的连接</span><br><span class="line">[root@bj-ufile-storage-set-osd ~]# netstat -antp | grep 8219 | egrep &quot;172.22.14.33|172.22.14.35|172.22.14.32|172.22.14.34|172.22.14.36|172.22.14.37|172.22.18.23|172.22.18.18&quot; | wc -l</span><br><span class="line">271</span><br></pre></td></tr></table></figure>

<p>重启大数据set的osd和goproxy后，内存释放。</p>
<img src="/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/7_restart_osd_goproxy.png" class="" title="image.png">

<p>所以衡量broker服务的占用内存可以用公式：</p>
<p>broker_rss = 3MB * (osd连接数+goproxy连接数)  ≈ 4MB * osd连接数</p>
<p>再经过反复压测，内存不再增长，问题得到解决。</p>
<h1 id="一般排查步骤的总结"><a href="#一般排查步骤的总结" class="headerlink" title="一般排查步骤的总结"></a>一般排查步骤的总结</h1><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><h3 id="lsof-列举出正在使用的文件"><a href="#lsof-列举出正在使用的文件" class="headerlink" title="lsof 列举出正在使用的文件"></a>lsof 列举出正在使用的文件</h3><p>lsof，这个工具用于排查是否存在很在很多超出预料的文件的情况，比如打开某文件未关闭，建立很多的socket连接等等。当然，发现问题只能靠眼力劲了。<br>lsof -p <pid> #查看进程打开的文件情况。</p>
<h3 id="pmap-查看进程内存概要"><a href="#pmap-查看进程内存概要" class="headerlink" title="pmap 查看进程内存概要"></a>pmap 查看进程内存概要</h3><p>pmap，用于查看进程的内存映像信息， 发现内存中大块的占用所在，以及分析内存可能存在的异常。<br>pmap {pid}<br>pmap -x {pid}<br>pmap -X {pid}</p>
<p>或者<br>cat /proc/{pid}/maps, 以及cat /proc/{pid}/smaps，都可以查看内存段的具体起始位置</p>
<h3 id="perf"><a href="#perf" class="headerlink" title="perf"></a>perf</h3><p>可以先捕获数据，然后进行性能分析，然后得到可疑的点。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">perf record -g -e cpu-clock -p 5545        # 记录进程 5545 的相关性能信息</span><br><span class="line">perf report -i perf.data                # 读取刚刚记录的数据，可以显示出种操作的占用情况，如下</span><br><span class="line">Samples: 908  of event &#x27;cpu-clock&#x27;, Event count (approx.): 227000000</span><br><span class="line">  Children      Self  Command  Shared Object       Symbol</span><br><span class="line">+   32.27%     0.00%  java     libpthread-2.17.so  [.] start_thread</span><br><span class="line">+   32.27%     0.00%  java     libjvm.so           [.] java_start</span><br><span class="line">+   26.54%     0.00%  java     libjvm.so           [.] ConcurrentG1RefineThread::run</span><br><span class="line">+   26.54%     0.11%  java     libjvm.so           [.] ConcurrentG1RefineThread::run_young_rs_sampling</span><br><span class="line">+   25.77%     5.62%  java     libjvm.so           [.] YoungList::rs_length_sampling_next</span><br><span class="line">+   22.58%     0.55%  java     [kernel.kallsyms]   [k] tracesys</span><br><span class="line">+   11.01%     0.00%  java     perf-5545.map       [.] 0x00007f553ec1e981</span><br><span class="line">+   10.79%     0.44%  java     libjvm.so           [.] JVM_Sleep</span><br><span class="line">+    9.36%     0.55%  java     libjvm.so           [.] G1CollectorPolicy::update_incremental_cset_info</span><br><span class="line">+    8.70%     0.55%  java     libjvm.so           [.] os::sleep</span><br><span class="line">+    8.26%     0.00%  java     [unknown]           [k] 0xee83b0ac00709650</span><br><span class="line">+    8.15%     0.99%  java     libpthread-2.17.so  [.] pthread_cond_timedwait@@GLIBC_2.3.2</span><br><span class="line">+    7.93%     0.00%  java     perf-5545.map       [.] 0x00007f553f7f7d30</span><br></pre></td></tr></table></figure>


<h3 id="gdb-调试工具dump出可疑内存"><a href="#gdb-调试工具dump出可疑内存" class="headerlink" title="gdb 调试工具dump出可疑内存"></a>gdb 调试工具dump出可疑内存</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gdb attach &lt;pid&gt;                    # 先连接到进程中</span><br><span class="line">dump memory /path/dump.bin 0x0011  0x0021    # dump 出内存段的信息,具体要 dump 的内存段地址，可以借助之前pmap 或 cat /proc/&lt;pid&gt;/smaps 或 cat /proc/&lt;pid&gt;/maps 中指示的地址段得出</span><br><span class="line"></span><br><span class="line">更好的方式</span><br><span class="line">gdb --batch --pid &#123;PID&#125; -ex &quot;dump memory native_memory.dump 0x7f7588000000 0x7f7588001A40&quot; </span><br><span class="line"></span><br><span class="line">strings /path/dump.bin | less # 查看内存内容, 相信你能从中发现一些不一样的东西</span><br><span class="line">strings  -n 10 /path/dump.bin | less # 只查看&gt;=10B字符串的行</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>以上这些命令，在不需要改变线上服务的二进制或服务启动方式下，就可以使用，比较方便。如果以上手段都没有效果，再用更复杂的工具。</p>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><h3 id="valgrind-1"><a href="#valgrind-1" class="headerlink" title="valgrind"></a>valgrind</h3><p>以上已经介绍过，这里就不再说明了。<br>需要注意的是10~30倍的性能损耗，在线上一般无法接受使用。</p>
<h3 id="gcc"><a href="#gcc" class="headerlink" title="gcc"></a>gcc</h3><p>gcc 命令行参数 -fsanitize=address -fno-omit-frame-pointer<br>新版本的gcc（gcc49）提供了很好的内存访问检查机制，实践中发现对性能的影响居然比Valgrind小很多。在实践中 Electric Fence 和 Valgrind 严重影响了程序的性能，难以触发内存访问问题，而gcc的-fsanitize=address编译参数解决了大问题，唯一的缺点是gcc高版本才支持，而实践中，生成环境的代码都是老版本编译器编译的。</p>
<img src="/2020/12/07/c++%E6%9C%8D%E5%8A%A1%E5%86%85%E5%AD%98%E6%8C%81%E7%BB%AD%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3/8_gcc_check_mem_leak.png" class="" title="image.png">

<p>这里有gcc版本问题的讨论。<br><a href="https://github.com/Raymo111/i3lock-color/issues/79">https://github.com/Raymo111/i3lock-color/issues/79</a> </p>
<h3 id="bcc"><a href="#bcc" class="headerlink" title="bcc"></a>bcc</h3><p>可以输出内存火焰图.<br>具体参考<br><a href="http://www.brendangregg.com/FlameGraphs/memoryflamegraphs.html">http://www.brendangregg.com/FlameGraphs/memoryflamegraphs.html</a><br><a href="http://www.brendangregg.com/ebpf.html#bcc%22">http://www.brendangregg.com/ebpf.html#bcc%22</a><br>对os有要求，centos仅限于7.6。</p>
<h3 id="pprof-gperftools"><a href="#pprof-gperftools" class="headerlink" title="pprof / gperftools"></a>pprof / gperftools</h3><p><a href="https://github.com/gperftools/gperftools">https://github.com/gperftools/gperftools</a><br>对性能影响小，可以直接压测，但建议有问题时开启，不要一直运行。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://valgrind.org/info/tools.html">Valgrind’s Tool Suite</a></li>
<li><a href="https://valgrind.org/docs/manual/quick-start.html">The Valgrind Quick Start Guide</a></li>
<li><a href="https://simonis.github.io/Memory/">The Memory Layout of a 64-bit Linux Process</a></li>
<li><a href="https://www.cnblogs.com/yougewe/p/11334342.html#_label4">内存泄漏排查攻略之：Show me your Memory</a></li>
<li><a href="https://www.0xffffff.org/2017/01/22/39-multi-thread-memory-bug/">Linux 环境下多线程 C/C++ 程序的内存问题调试</a></li>
</ol>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>内存 - valgrind - gperftools - pprof</tag>
      </tags>
  </entry>
  <entry>
    <title>关于客户端重连ZooKeeper的的那些事儿</title>
    <url>/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/</url>
    <content><![CDATA[<h1 id="ZooKeeper-Sessions"><a href="#ZooKeeper-Sessions" class="headerlink" title="ZooKeeper Sessions"></a>ZooKeeper Sessions</h1><p>![image](关于客户端重连ZooKeeper的的那些事儿/ZooKeeper Sessions.jpg)</p>
<ul>
<li>ZooKeeper客户端初始化后转换到CONNECTING状态，与ZooKeeper服务器（或者ZooKeeper集群中的一台服务器）建立连接后，进入CONNECTED状态</li>
<li>当客户端与ZooKeeper服务器断开连接或者无法收到服务器响应时，就会转回到CONNECTING状态，此时会一直收到CONNECTION_LOSS。</li>
<li>这时ZooKeeper客户端会自动地从列表中逐个选取新的地址进行重连，如果成功，状态又会转回CONNECTED状态</li>
<li>如果一直无法重连，超过会话超时时间(sessionTimeout)后，服务器认为这个session已经结束了，此时客户端无法感知</li>
<li>最后当客户端终于自动重连到ZooKeeper服务器时，会收到Session Expired；这种情况下，需要应用层关闭当前会话，然后重连。</li>
<li>在CONNECTING状态和CONNECTED状态，客户端都可以显示地关闭，进入CLOSED状态</li>
</ul>
<span id="more"></span>

<blockquote>
<p>当建立session时，客户会收到由服务器创建的session id和password。当自动重连时，客户端都会发送session id和password给服务端，重建的还是原来的session。以上的自动重连都是由底层的API库来实现，非应用层面去实现的重连</p>
</blockquote>
<h1 id="客户端自动重连zk的测试"><a href="#客户端自动重连zk的测试" class="headerlink" title="客户端自动重连zk的测试"></a>客户端自动重连zk的测试</h1><p>cons 查看连接到服务端的信息，看到在自动重连之后，session id保持不变。</p>
<img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%87%AA%E5%8A%A8%E9%87%8D%E8%BF%9Ezk%E7%9A%84%E6%BC%94%E7%A4%BA.png" class="" title="image">

<h1 id="Session-Expired后的重连实践"><a href="#Session-Expired后的重连实践" class="headerlink" title="Session Expired后的重连实践"></a>Session Expired后的重连实践</h1><h2 id="Session-Expired产生的过程"><a href="#Session-Expired产生的过程" class="headerlink" title="Session Expired产生的过程"></a>Session Expired产生的过程</h2><p>再来回顾下产生session expired的过程，官网上的说明很清楚。</p>
<p>Session expiration is managed by the ZooKeeper cluster itself, not by the client. When the ZK client establishes a session with the cluster it provides a “timeout” value detailed above. This value is used by the cluster to determine when the client’s session expires. Expirations happens when the cluster does not hear from the client within the specified session timeout period (i.e. no heartbeat). At session expiration the cluster will delete any/all ephemeral nodes owned by that session and immediately notify any/all connected clients of the change (anyone watching those znodes).* At this point the client of the expired session is still disconnected from the cluster, it will not be notified of the session expiration until/unless it is able to re-establish a connection to the cluster.* The client will stay in disconnected state until the TCP connection is re-established with the cluster, at which point the watcher of the expired session will receive the “session expired” notification.</p>
<p>Example state transitions for an expired session as seen by the expired session’s watcher:</p>
<ul>
<li>‘connected’ : session is established and client is communicating with cluster (client/server communication is operating properly)</li>
<li>…. client is partitioned from the cluster</li>
<li>‘disconnected’ : client has lost connectivity with the cluster</li>
<li>…. time elapses, after ‘timeout’ period the cluster expires the session, nothing is seen by client as it is disconnected from cluster</li>
<li>…. time elapses, the client regains network level connectivity with the cluster</li>
<li>‘expired’ : eventually the client reconnects to the cluster, it is then notified of the expiration</li>
</ul>
<p>所以需要在客户端收到expiration后，进行重连。</p>
<h2 id="Session-Expired后的重连的测试"><a href="#Session-Expired后的重连的测试" class="headerlink" title="Session Expired后的重连的测试"></a>Session Expired后的重连的测试</h2><p>由于生产环境中用的语言是node，选用的客户端是<a href="https://github.com/yfinkelstein/node-zookeeper">node-zookeeper</a>。这是一个封装了ZooKeeper C API的模块.</p>
<p>后面实验的测试环境如下:</p>
<ul>
<li>zk集群：192.168.154.103:2181,192.168.154.104:2181,192.168.154.105:2181</li>
<li>客户端：192.168.154.106 uhost-access</li>
</ul>
<p>重连的过程如下：</p>
<p>（1）客户端拉起uhost-access服务:<br>   临时节点 /NS/region666888/set12/uhost/access/0注册成功，session建立成功<br>   <img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/ephemeral_node_access_1.png" class="" title="image"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@192-168-153-51 node-common]# echo cons | nc 192.168.154.103 2181 | grep &quot;192.168.154.106&quot;</span><br><span class="line"> /192.168.154.106:48146[1](queued=0,recved=99,sent=102,sid=0x6b364768a90001,lop=NA,est=1560000679478,to=40000,lcxid=0x5cfbb93f,lzxid=0x1300013dd4,lresp=1560000745134,llat=1,minlat=0,avglat=1,maxlat=6)</span><br><span class="line"> /192.168.154.106:48149[1](queued=0,recved=7,sent=7,sid=0x6b364768a90000,lop=PING,est=1560000679478,to=40000,lcxid=0x5cfbb8b6,lzxid=0xffffffffffffffff,lresp=1560000732987,llat=0,minlat=0,avglat=0,maxlat=4)</span><br><span class="line">[root@192-168-153-51 node-common]# echo cons | nc 192.168.154.104 2181 | grep &quot;192.168.154.106&quot;</span><br><span class="line">[root@192-168-153-51 node-common]# echo cons | nc 192.168.154.105 2181 | grep &quot;192.168.154.106&quot;</span><br></pre></td></tr></table></figure>

<p>（2）客户端添加防火墙规则，模拟网络分区</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables -A OUTPUT -d 192.168.154.103 -p tcp --dport 2181 -j DROP</span><br><span class="line">iptables -A OUTPUT -d 192.168.154.104 -p tcp --dport 2181 -j DROP</span><br><span class="line">iptables -A OUTPUT -d 192.168.154.105 -p tcp --dport 2181 -j DROP</span><br><span class="line">iptables -A INPUT -s 192.168.154.103 -p tcp --sport 2181 -j DROP</span><br><span class="line">iptables -A INPUT -s 192.168.154.104 -p tcp --sport 2181 -j DROP</span><br><span class="line">iptables -A INPUT -s 192.168.154.105 -p tcp --sport 2181 -j DROP   </span><br></pre></td></tr></table></figure>

<p>此时客户端和zk自动重连失败，一直处于CONNECTING状态</p>
<p>这部分错误在module zookeeper： node-zk.cpp里</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2019-06-08 21:38:52,739:22724:ZOO_ERROR@handle_socket_error_msg@1666: Socket [192.168.154.103:2181] zk retcode=-7, errno=110(Connection timed out): connection to 192.168.154.103:2181 timed out (exceeded timeout by 1ms)</span><br><span class="line">2019-06-08 21:38:52,742:22724:ZOO_ERROR@yield@234: yield:zookeeper_interest returned error: -7 - operation timeout</span><br></pre></td></tr></table></figure>

<p>zk.js（在node-zookeeper上层封装的代码）打印on connecting的日志</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[2019-06-08 21:38:52.739] [WARN] uhost-access - zk connection 192.168.154.103:2181,192.168.154.104:2181,192.168.154.105:2181 connecting</span><br><span class="line"></span><br><span class="line">[2019-06-08 21:38:53.016] [WARN] uhost-access - zk connection 192.168.154.103:2181,192.168.154.104:2181,192.168.154.105:2181 connecting</span><br></pre></td></tr></table></figure>

   <img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/connecting_state_1.jpg" class="" title="image">

<p>/NS/region666888/set12/uhost/access/0临时节点和会话被清理</p>
<p>（3）删除防火墙规则，模拟网络恢复</p>
<p>清楚防火墙规则：iptables -F</p>
<p>（4）重连</p>
<p>此时，客户端自动重连到zk时，会收到session expiration的错误。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2019-06-08 22:14:20,118:22724:ZOO_ERROR@handle_socket_error_msg@1764: Socket [192.168.154.104:2181] zk retcode=-112, errno=116(Stale file handle): sessionId=0x6b364768a90000 has expired.</span><br><span class="line">2019-06-08 22:14:20,118:22724:ZOO_ERROR@main_watcher@380: Session expired. Shutting down...</span><br><span class="line"></span><br><span class="line">2019-06-08 22:14:20,124:22724:ZOO_ERROR@zk_io_cb@278: yield:zookeeper_process returned error: -112 - session expired</span><br><span class="line"></span><br><span class="line">2019-06-08 22:14:39,701:22724:ZOO_ERROR@handle_socket_error_msg@1764: Socket [192.168.154.103:2181] zk retcode=-112, errno=116(Stale file handle): sessionId=0x6b364768a90001 has expired.</span><br><span class="line">2019-06-08 22:14:39,702:22724:ZOO_ERROR@main_watcher@380: Session expired. Shutting down...</span><br><span class="line"></span><br><span class="line">2019-06-08 22:14:39,707:22724:ZOO_ERROR@zk_io_cb@278: yield:zookeeper_process returned error: -112 - session expired</span><br></pre></td></tr></table></figure>


<ul>
<li>在node-zookeeper里，会有检测ZOO_ EXPIRED_ SESSION_STATE的watcher。当watch到后，会emit一个closed的event。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> static void main_watcher (zhandle_t *zzh, int type, int state, const char *path, void* context) &#123;</span><br><span class="line">     Nan::HandleScope scope;</span><br><span class="line">     LOG_DEBUG((&quot;main watcher event: type=%d, state=%d, path=%s&quot;, type, state, (path ? path: &quot;null&quot;)));</span><br><span class="line">     ZooKeeper *zk = static_cast&lt;ZooKeeper *&gt;(context);</span><br><span class="line"></span><br><span class="line">     if (type == ZOO_SESSION_EVENT) &#123;</span><br><span class="line">         if (state == ZOO_CONNECTED_STATE) &#123;</span><br><span class="line">             zk-&gt;myid = *(zoo_client_id(zzh));</span><br><span class="line">             zk-&gt;DoEmitPath(Nan::New(on_connected), path);</span><br><span class="line">         &#125; else if (state == ZOO_CONNECTING_STATE) &#123;</span><br><span class="line">             zk-&gt;DoEmitPath (Nan::New(on_connecting), path);</span><br><span class="line">         &#125; else if (state == ZOO_AUTH_FAILED_STATE) &#123;</span><br><span class="line">             LOG_ERROR((&quot;Authentication failure. Shutting down...\n&quot;));</span><br><span class="line">             zk-&gt;realClose(ZOO_AUTH_FAILED_STATE);</span><br><span class="line">         &#125; else if (state == ZOO_EXPIRED_SESSION_STATE) &#123;</span><br><span class="line">             LOG_ERROR((&quot;Session expired. Shutting down...\n&quot;));</span><br><span class="line">             zk-&gt;realClose(ZOO_EXPIRED_SESSION_STATE);</span><br><span class="line">         &#125;</span><br><span class="line">     &#125; </span><br><span class="line">     ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void realClose (int code) &#123;</span><br><span class="line">    ......</span><br><span class="line">       </span><br><span class="line">    if (zhandle) &#123;</span><br><span class="line">        ......</span><br><span class="line">           </span><br><span class="line">        // Close the timer and finally Unref the ZooKeeper instance when it&#x27;s done</span><br><span class="line">        // Unrefing after is important to avoid memory being freed too early.</span><br><span class="line">        uv_close((uv_handle_t*) &amp;zk_timer, timer_closed); </span><br><span class="line"></span><br><span class="line">        Nan::HandleScope scope;</span><br><span class="line">        DoEmitClose (Nan::New(on_closed), code);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<ul>
<li>在自己封装的上层业务库里，捕获到这个close event，并重连到zk<img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/zkjs_on_close.jpg" class="" title="image">
<img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/zkjs_reconnect.jpg" class="" title="image"></li>
</ul>
<p>重连之后，session id改变，建立了新的session</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@192-168-153-51 node-common]# echo cons | nc 192.168.154.103 2181 | grep &quot;192.168.154.106&quot;</span><br><span class="line">[root@192-168-153-51 node-common]# echo cons | nc 192.168.154.104 2181 | grep &quot;192.168.154.106&quot;</span><br><span class="line">[root@192-168-153-51 node-common]# echo cons | nc 192.168.154.105 2181 | grep &quot;192.168.154.106&quot;</span><br><span class="line"> /192.168.154.106:40100[1](queued=0,recved=56,sent=56,sid=0x26b3647662b0002,lop=PING,est=1560003260125,to=40000,lcxid=0x5cfbc659,lzxid=0x1300016fae,lresp=1560003948592,llat=0,minlat=0,avglat=0,maxlat=1)</span><br><span class="line"> /192.168.154.106:40106[1](queued=0,recved=837,sent=837,sid=0x26b3647662b0003,lop=NA,est=1560003279705,to=40000,lcxid=0x5cfbc841,lzxid=0x1300016fde,lresp=1560003957541,llat=1,minlat=0,avglat=1,maxlat=17)</span><br></pre></td></tr></table></figure>

<h2 id="新加坡机房重连失败的分析"><a href="#新加坡机房重连失败的分析" class="headerlink" title="新加坡机房重连失败的分析"></a>新加坡机房重连失败的分析</h2><p>线上问题：5月2日新加坡机房set11出现的问题：zk异常，有2分钟处于不工作状态，主机服务（客户端）收到session expired错误。</p>
<img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/zk%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8E%89.jpg" class="" title="image">
<img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/singapore_session_expire.png" class="" title="image">

<p>当时并未重连成功，但测试下来确实是有重连的逻辑的。虽然当时zk的具体问题原因无法查清，也无法复现问题。但可以猜测是重连的时候，zk服务没有恢复，导致重连没有成功。<br>虽然2min后，zk恢复正常了，但此时客户端也不会再重试了。所以需要更可靠的重连机制。</p>
<h1 id="在zk删除数据后的重连实践"><a href="#在zk删除数据后的重连实践" class="headerlink" title="在zk删除数据后的重连实践"></a>在zk删除数据后的重连实践</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>关闭zk, 删除节点数据，拉起zk  → 无法自动注册成功，需要重启服务</p>
<p>测试步骤<br>zookeeper集群关闭删除数据再启动：</p>
<ul>
<li>zk集群：service zookeeper stop </li>
<li>zk集群里删除快照和日志数据：rm -f /data/zookeeper/data/version-2/* rm -f /data/zookeeper/log/version-2/*  </li>
<li>zk集群拉起：service zookeeper start</li>
</ul>
<p>自动重连还会连到原来的session，但因为数据已经删除了，无法以原来session连到zk，一直失败。</p>
<p>客户端：192.168.154.106 uhost-access的错误日志</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2019-06-09 22:59:33,339:27610:ZOO_ERROR@handle_socket_error_msg@1746: Socket [192.168.154.105:2181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response</span><br><span class="line">2019-06-09 22:59:33,339:27610:ZOO_ERROR@zk_io_cb@278: yield:zookeeper_process returned error: -4 - connection loss</span><br><span class="line"></span><br><span class="line">2019-06-09 22:59:33,340:27610:ZOO_ERROR@handle_socket_error_msg@1746: Socket [192.168.154.105:2181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response</span><br><span class="line">2019-06-09 22:59:33,341:27610:ZOO_ERROR@zk_io_cb@278: yield:zookeeper_process returned error: -4 - connection loss</span><br></pre></td></tr></table></figure>

<p>结果： node服务的节点数据都没有自动注册上去。相当于复现了线上香港公共zk升级后数据丢失后，业务代码没有再次注册的话，需要服务重启的问题。</p>
<h2 id="go框架uframework里的处理"><a href="#go框架uframework里的处理" class="headerlink" title="go框架uframework里的处理"></a>go框架uframework里的处理</h2><p>go框架uframework里用了golang版本的zk 客户端（<a href="https://github.com/samuel/go-zookeeper">go-zookeeper</a>），并又封装了name_container.go的文件，里面实现了上层重连的逻辑。在zookeeper集群数据清掉之后再被拉起，服务的节点还可以重新注册上去。</p>
<p>它的实现有两处重连的逻辑：</p>
<p>（1）name_container.go → func FetchZkName里，注册了所有关注节点的子节点变化的watcher。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">childs, _, ch, err := zkConn.GetChildrenWatcher(fullName)</span><br></pre></td></tr></table></figure>

<p>当连接断开时，触发一个event，然后重连。又调用FetchZkName, 保证一直有个 goroutine在等待这个事件的触发。</p>
<img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/uframework_reconnect_zk_1.png" class="" title="image">
<img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/uframework_reconnect_zk_2.png" class="" title="image">
<img src="/2019/06/06/%E5%85%B3%E4%BA%8E%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%87%8D%E8%BF%9EZooKeeper%E7%9A%84%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/uframework_reconnect_zk_3.png" class="" title="image">

<p>重复4.1中的测试后，可以重连(相当于新建session)</p>
<p>在我们的服务里，关注的节点有三个uhost-access、uimage3-access、uimage3-manager，所有会有三个reconnect的动作，但只会有一个成功。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[2019-06-09 23:57:21.471566] [INFO]cat watcher &#123;EventNotWatching StateDisconnected /NS/region666888/set12/uhost/access zk: zookeeper is closing &#125;</span><br><span class="line">[2019-06-09 23:57:21.471724] [ERROR]********  ev.Type == zk.EventNotWatching &amp;&amp; ev.State == zk.StateDisconnected, begin to reconnect</span><br><span class="line">[2019-06-09 23:57:21.471742] [INFO]cat watcher &#123;EventNotWatching StateDisconnected /NS/region666888/set12/uimage3/access zk: zookeeper is closing &#125;</span><br><span class="line">[2019-06-09 23:57:21.471781] [INFO]cat watcher &#123;EventNotWatching StateDisconnected /NS/region666888/set12/uimage3/manager zk: zookeeper is closing &#125;</span><br><span class="line">[2019-06-09 23:57:21.471786] [ERROR]********  ev.Type == zk.EventNotWatching &amp;&amp; ev.State == zk.StateDisconnected, begin to reconnect</span><br><span class="line">[2019-06-09 23:57:21.471867] [ERROR]********  ev.Type == zk.EventNotWatching &amp;&amp; ev.State == zk.StateDisconnected, begin to reconnect</span><br><span class="line">[2019-06-09 23:57:23.472008] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, before initZkConn</span><br><span class="line">[2019-06-09 23:57:24.481209] [ERROR]Connect zk Servers [192.168.154.103:2181 192.168.154.104:2181 192.168.154.105:2181] failed:can not connect remote servers</span><br><span class="line">[2019-06-09 23:57:24.481379] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, after initZkConn fail</span><br><span class="line">[2019-06-09 23:57:24.481427] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, before initZkConn</span><br><span class="line">[2019-06-09 23:57:25.483472] [ERROR]Connect zk Servers [192.168.154.103:2181 192.168.154.104:2181 192.168.154.105:2181] failed:can not connect remote servers</span><br><span class="line">[2019-06-09 23:57:25.483658] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, after initZkConn fail</span><br><span class="line">[2019-06-09 23:57:25.483674] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, before initZkConn</span><br><span class="line">[2019-06-09 23:57:26.492263] [ERROR]Connect zk Servers [192.168.154.103:2181 192.168.154.104:2181 192.168.154.105:2181] failed:can not connect remote servers</span><br><span class="line">[2019-06-09 23:57:26.492371] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, after initZkConn fail</span><br><span class="line">[2019-06-09 23:57:26.492390] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, before initZkConn</span><br><span class="line">[2019-06-09 23:57:26.515713] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, after initZkConn success</span><br></pre></td></tr></table></figure>

<p>（2）周期性注册自身节点到zk时触发。<br>registerNode（每隔5秒） → zookeeper.GetZkInstance(zk_server) → initZkConn(负责重新连接)。 </p>
<p>注：这是在把（1）里面所有的reconnect注释的情况下测试的，不然会混淆。</p>
<p>第42秒时，zk还没恢复，所以连接不上；第47秒，连到zk后，注册自身节点成功。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">019-06-10 00:19:42.225063] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, before initZkConn</span><br><span class="line">[2019-06-10 00:19:43.232714] [ERROR]Connect zk Servers [192.168.154.103:2181 192.168.154.104:2181 192.168.154.105:2181] failed:can not connect remote servers</span><br><span class="line">[2019-06-10 00:19:43.232775] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, after initZkConn fail</span><br><span class="line">[2019-06-10 00:19:43.232877] [ERROR][register_myself]connect zk server error</span><br><span class="line">[2019-06-10 00:19:47.224898] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, before initZkConn</span><br><span class="line">[2019-06-10 00:19:47.254394] [ERROR]in GetZkInstance, zkConn.conn.State() != zk.StateHasSession, after initZkConn success</span><br><span class="line">[2019-06-10 00:19:47.302364] [INFO][register_myself]complte register, /NS/region666888/set12/uhost/scheduler/0</span><br></pre></td></tr></table></figure>

<p>题外话：<br>zookeeper官方是不推荐加上这种重连机制的。个人感觉现实中有这种机制都是被迫加上的，因为难以避免操作不当而造成数据丢失，底层自动重连失败的情况。</p>
<p>When a client (session) becomes partitioned from the ZK serving cluster it will begin searching the list of servers that were specified during session creation. Eventually, when connectivity between the client and at least one of the servers is re-established, the session will either again transition to the “connected” state (if reconnected within the session timeout value) or it will transition to the “expired” state (if reconnected after the session timeout). It is not advisable to create a new session object (a new ZooKeeper.class or zookeeper handle in the c binding) for disconnection. The ZK client library will handle reconnect for you. In particular we have heuristics built into the client library to handle things like “herd effect”, etc… Only create a new session when you are notified of session expiration (mandatory).</p>
<h2 id="node-common下zk-js的改进"><a href="#node-common下zk-js的改进" class="headerlink" title="node-common下zk.js的改进"></a>node-common下zk.js的改进</h2><p>目前的逻辑, 只有在收到close的event，有一次重试。缺少定期的检查和重试机制。</p>
<p>模仿4.2中的重连逻辑，在node中也能很快地实现。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ZkClass.prototype.connect = function() &#123;</span><br><span class="line">    var self = this;</span><br><span class="line">    var f = new Future;</span><br><span class="line"></span><br><span class="line">    function reconnect() &#123;</span><br><span class="line">        self.close();</span><br><span class="line">        init();</span><br><span class="line">        self.conn.connect(function(err, zk_conn)&#123;</span><br><span class="line">            self.emit(&quot;reconnect&quot;);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    function init() &#123;</span><br><span class="line">        self.conn = new ZooKeeper(&#123;</span><br><span class="line">            connect: self.options.connect,</span><br><span class="line">            timeout: self.options.timeout || 200000,</span><br><span class="line">            debug_level: self.options.debug_level || ZooKeeper.ZOO_LOG_LEVEL_ERROR,</span><br><span class="line">            host_order_deterministic: self.options.host_order_deterministic || false</span><br><span class="line">        &#125;)</span><br><span class="line">        ......</span><br><span class="line">        .once(&quot;close&quot;, reconnect);</span><br><span class="line">        ......</span><br><span class="line">    &#125;</span><br><span class="line">    init();</span><br><span class="line"></span><br><span class="line">    self.conn.connect(function(err, zk_conn)&#123;</span><br><span class="line">        if (err) &#123;</span><br><span class="line">            f.throw(err);</span><br><span class="line">        &#125;</span><br><span class="line">        f.return(zk_conn);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    var res = f.wait();</span><br><span class="line"></span><br><span class="line">    FunctionPool.register(&quot;zk_&quot; + uuid.v4() + &quot;_worker&quot;, function() &#123;</span><br><span class="line">        if (self.conn.state != ZkClass.ZOO_CONNECTED_STATE) &#123;</span><br><span class="line">            GLOBAL.logger.info( &quot;self.conn.state: %d, not ZOO_CONNECTED_STATE(3), begin to reconnet&quot;, self.conn.state );</span><br><span class="line">            reconnect();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, 10000);</span><br><span class="line"></span><br><span class="line">    return res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="http://zookeeper.apache.org/doc/r3.4.5/zookeeperProgrammers.html">ZooKeeper Programmer’s Guide</a></li>
<li><a href="http://zookeeper.apache.org/doc/r3.4.5/zookeeperAdmin.html">ZooKeeper Administrator’s Guide</a></li>
<li><a href="https://wiki.apache.org/hadoop/ZooKeeper/FAQ#A3">Hadoop Wiki ZooKeeper/FAQ</a></li>
<li><a href="https://juejin.im/post/5b13b9e86fb9a01e7b4de21e">Zookeeper系列一：Zookeeper基础命令操作</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>网卡多队列总结（转）</title>
    <url>/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="中断是什么"><a href="#中断是什么" class="headerlink" title="中断是什么"></a>中断是什么</h2><p>中断使得硬件可以发松通知给处理器。例如敲击键盘时，键盘就会产生一个中断，通知操作系统有键被按下。</p>
<p>中断本质上是一种电信号，由硬件设备生成，送入中断控制器的输入引脚中。中断控制器(如8259A)是个简单的电子芯片，将多路中断管线，采用复用技术只通过一个和处理器连接的管线和处理器通信。当产生一个中断后，处理器会检测到一个电信号，中断自己的当前正在运行的程序，通知内核。内核调用一个称为中断处理程序（interrupt handler)或中断服务例程(interrupt service routine）的特定程序。中断处理程序或中断服务例程可以在中断向量表中找到，而这个中断向量表位于内存中的固定地址中。CPU处理中断后，就会恢复执行之前被中断的程序，整个流程如下图所示。</p>
<p>不同设备同时中断如何知道哪个中断是来自硬盘、哪个来自网卡呢?这个很容易，系统上的每个硬件设备都会被分配一个 IRQ 号，通过这个唯一的IRQ号就能区别不同硬件设备了。</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/%E4%B8%AD%E6%96%AD%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B.png" class="" title="image">

<h3 id="中断控制器"><a href="#中断控制器" class="headerlink" title="中断控制器"></a>中断控制器</h3><p>常见的中断控制器有两种：可编程中断控制器 8259A 和高级可编程中断控制器(APIC, Advanced Programmable Interrupt Controller)，中断控制器应该在大学的硬件接口和计算机体系结构的相关课程中都学过。传统的 8259A 只适合单 CPU 的情况，现在都是多CPU多核的SMP体系，所以为了充分利用SMP体系结构、把中断传递给系统上的每个CPU 以便更好实现并行和提高性能，Intel 引入了高级可编程中断控制器(APIC)。</p>
<h3 id="网卡收发包过程"><a href="#网卡收发包过程" class="headerlink" title="网卡收发包过程"></a>网卡收发包过程</h3><p>由于中断会频繁发生，因此要求中断处理程序执行要快速。为了实现快速执行，必须要将一些繁重且不非常紧急的任务从中断处理程序中剥离出来，这一部分Linux中称为下半部，有三种方法处理下半部——软中断、tasklet和工作队列。</p>
<p>内核如何从网卡接受数据，传统的经典过程：</p>
<ol>
<li>数据到达网卡；</li>
<li>网卡产生一个中断给内核；</li>
<li>内核使用I/O指令，从网卡I/O区域中去读取数据；</li>
</ol>
<p>但是，这一种方法，有一种重要的问题，就是大流量的数据来到，网卡会产生大量的中断，内核在中断上下文中，会浪费大量的资源来处理中断本身。所以，一个问题是，“可不可以不使用中断”，这就是轮询技术，所谓NAPI技术，说来也不神秘，就是说，内核屏蔽中断，然后隔一会儿就去问网卡，“你有没有数据啊？”</p>
<p>从这个描述本身可以看到，哪果数据量少，轮询同样占用大量的不必要的CPU资源，大家各有所长吧，呵呵……</p>
<p>OK，另一个问题，就是从网卡的I/O区域，包括I/O寄存器或I/O内存中去读取数据，这都要CPU去读，也要占用CPU资源，“CPU从I/O区域读，然后把它放到内存（这个内存指的是系统本身的物理内存，跟外设的内存不相干，也叫主内存）中”。于是自然地，就想到了DMA技术——让网卡直接从主内存之间读写它们的I/O数据，CPU，这儿不干你事，自己找乐子去：</p>
<ol>
<li>首先，内核在主内存中为收发数据建立一个环形的缓冲队列（通常叫DMA环形缓冲区）。</li>
<li>内核将这个缓冲区通过DMA映射，把这个队列交给网卡；</li>
<li>网卡收到数据，就直接放进这个环形缓冲区了——也就是直接放进主内存了；然后，向系统产生一个中断；</li>
<li>内核收到这个中断，就取消DMA映射，这样，内核就直接从主内存中读取数据；</li>
</ol>
<p>剩下的处理和操作数据包的工作就会交给软中断。高负载的网卡是软中断产生的大户，很容易形成瓶颈。</p>
<p>在相当长的时间内，网卡的中断都是通过CPU 0来处理的，造成CPU 0的压力很高、其他CPU相对空闲的情况。直到网卡多队列技术的出现，网卡多队列实际就是网卡的数据请求可以通过多个CPU处理。</p>
<h2 id="多队列网卡的实现"><a href="#多队列网卡的实现" class="headerlink" title="多队列网卡的实现"></a>多队列网卡的实现</h2><h3 id="多队列网卡硬件实现"><a href="#多队列网卡硬件实现" class="headerlink" title="多队列网卡硬件实现"></a>多队列网卡硬件实现</h3><p>常见的有Intel的82575、82576，Boardcom的57711等，下面以公司的服务器使用较多的Intel 82575网卡为例，分析一下多队列网卡的硬件的实现以及Linux内核软件的支持。</p>
<p>Intel 82575硬件逻辑图，有四个硬件队列。当收到报文时，通过hash包头的SIP、Sport、DIP、Dport四元组，将一条流总是收到相同的队列。同时触发与该队列绑定的中断。</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/82575%E7%BD%91%E5%8D%A1%E7%A1%AC%E4%BB%B6%E9%80%BB%E8%BE%91%E5%9B%BE.gif" class="" title="image">

<p>RSS(Receive-Side Scaling, also known as multi-queue receive),是网卡的硬件特性，实现多队列，将不同的流分发到不同的CPU上，同一数据流始终在同一CPU上，避免TCP的顺序性和CPU的并行性发生冲突。</p>
<h3 id="Linux-kernel-2-6-21前网卡驱动的实现"><a href="#Linux-kernel-2-6-21前网卡驱动的实现" class="headerlink" title="Linux kernel 2.6.21前网卡驱动的实现"></a>Linux kernel 2.6.21前网卡驱动的实现</h3><p>Linux kernel从2.6.21之前不支持多队列特性，一个网卡只能申请一个中断号，因此同一个时刻只有一个核在处理网卡收到的包。如图2.1，协议栈通过NAPI轮询收取各个硬件queue中的报文到图2.2的net_device数据结构中，通过QDisc队列将报文发送到网卡。</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/2.6.21%E4%B9%8B%E5%89%8D%E5%86%85%E6%A0%B8%E5%8D%8F%E8%AE%AE%E6%A0%88.png" class="" title="image">

<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/2.6.21%E4%B9%8B%E5%89%8Dnet_device.png" class="" title="image">

<h3 id="Linux-kernel-2-6-21后网卡驱动的实现"><a href="#Linux-kernel-2-6-21后网卡驱动的实现" class="headerlink" title="Linux kernel 2.6.21后网卡驱动的实现"></a>Linux kernel 2.6.21后网卡驱动的实现</h3><p>Linux kernel 2.6.21开始支持多队列特性，当网卡驱动加载时，通过获取的网卡型号，得到网卡的硬件queue的数量，并结合CPU核的数量，最终通过Sum=Min（网卡queue，CPU core）得出所要激活的网卡queue数量（Sum），并申请Sum个中断号，分配给激活的各个queue。</p>
<p>如图3.1，当某个queue收到报文时，触发相应的中断，收到中断的核，将该任务加入到协议栈负责收包的该核的NET_RX_SOFTIRQ队列中（NET_RX_SOFTIRQ在每个核上都有一个实例），在NET_RX_SOFTIRQ中，调用NAPI的收包接口，将报文收到CPU中如图3.2的有多个netdev_queue的net_device数据结构中。这样，CPU的各个核可以并发的收包，就不会因为一个核不能满足需求，导致网络IO性能下降。</p>
<p>但当CPU可以平行收包时，就会出现不同的核收取了同一个queue的报文，这就会产生报文乱序的问题，解决方法是将一个queue的中断绑定到唯一的一个核上去，从而避免了乱序的问题。</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/2.6.21%E4%B9%8B%E5%90%8E%E5%86%85%E6%A0%B8%E5%8D%8F%E8%AE%AE%E6%A0%88.png" class="" title="image">
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/2.6.21%E4%B9%8B%E5%90%8Enet_device.png" class="" title="image">

<h2 id="查看网卡是否支持多队列"><a href="#查看网卡是否支持多队列" class="headerlink" title="查看网卡是否支持多队列"></a>查看网卡是否支持多队列</h2><p>查看网卡是否支持多队列，使用lspci -vvv命令，找到Ethernet controller项：</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/%E7%BD%91%E5%8D%A1%E6%94%AF%E6%8C%81%E5%A4%9A%E9%98%9F%E5%88%97.jpg" class="" title="image">            

<p>如果有MSI-X， Enable+ 并且Count &gt; 1，则该网卡是多队列网卡。</p>
<p>Message Signaled Interrupts（MSI）是PCI规范的一个实现，可以突破CPU 256条interrupt的限制，使每个设备具有多个中断线变成可能，多队列网卡驱动给每个queue申请了MSI。MSI-X是MSI数组，实际应用场景中，MSI方式的中断对多核cpu的利用情况不佳，网卡中断全部落在某一个cpu上，即使设置cpu affinity也没有作用，而MSI-X中断方式可以自动在多个cpu上分担中断</p>
<p>然后可以查看是否打开了网卡多队列，使用命令cat /proc/interrupts，如果看到如下图信息表明多队列支持已经打开：</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/interrupts.jpg" class="" title="image">

<h3 id="是不是某个CPU在一直忙着处理IRQ？"><a href="#是不是某个CPU在一直忙着处理IRQ？" class="headerlink" title="是不是某个CPU在一直忙着处理IRQ？"></a>是不是某个CPU在一直忙着处理IRQ？</h3><p>这个问题我们可以从 mpstat -P ALL 1 的输出中查明：里面的 %irq一列即说明了CPU忙于处理中断的时间占比</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/cpu%E6%B6%88%E8%80%97.jpg" class="" title="image">

<p>上面的例子中，第四个CPU有25.63%时间在忙于处理中断（这个数值还不算高，如果高达80%（而同时其它CPU这个数值很低）以上就说明有问题了），后面那个 intr/s 也说明了CPU每秒处理的中断数（从上面的数据也可以看出，其它几个CPU都不怎么处理中断）。</p>
<p>然后我们就要接着查另外一个问题：这个忙于处理中断的CPU都在处理哪个（些）中断？这要看/proc/interrupts文件</p>
<h3 id="proc-interrupts文件"><a href="#proc-interrupts文件" class="headerlink" title="/proc/interrupts文件"></a>/proc/interrupts文件</h3><p>这里记录的是自启动以来，每个CPU处理各类中断的数量（第一列是中断号，最后一列是对应的设备名）</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/interrupts%E8%AF%B4%E6%98%8E.jpg" class="" title="image">

<p>对上面文件的输出，解释如下：</p>
<p>● 第一列表示IRQ号。</p>
<p>● 第二、三、四列表示相应的CPU核心被中断的次数。在上面的例子中，timer表示中断名称（为系统时钟）。1825291229表示CPU0被中断了1825291229次。i8042表示控制键盘和鼠标的键盘控制器。</p>
<p>● 对于像rtc（real time clock）这样的中断，CPU是不会被中断的。因为RTC存在于电子设备中，是用于追踪时间的。</p>
<p>● NMI和LOC是系统所使用的驱动，用户无法访问和配置。</p>
<p>IRQ号决定了需要被CPU处理的优先级，IRQ号越小意味着优先级越高。</p>
<p>例如，如果CPU同时接收了来自键盘和系统时钟的中断，那么CPU首先会服务于系统时钟，因为他的IRQ号是0。</p>
<p>● IRQ0 ：系统时钟（不能改变）。</p>
<p>● IRQ1 ：键盘控制器（不能改变）。</p>
<p>● IRQ3 ：串口2的串口控制器（如有串口4，则其也使用这个中断）。</p>
<p>● IRQ4 ：串口1的串口控制器（如有串口3，则其也使用这个中断）。</p>
<p>● IRQ5 ：并口2和3或声卡。</p>
<p>● IRQ6 ：软盘控制器。</p>
<p>● IRQ7 : 并口1，它被用于打印机或若是没有打印机，可以用于任何的并口。</p>
<p>而对于像操作杆（或称为游戏手柄）上的CPU，它并不会等待设备发送中断。因为操作杆主要用于游戏，操作杆的移动必须非常快，因此使用轮询的方式检测设备是否需要CPU的关注还是比较理想的。使用轮询方式的缺点是CPU就处于了忙等状态，因为CPU会不停的多次检查设备。但是需要注意的是在Linux中，这种处理信号的方式也是必不可少的。</p>
<p>最后确认每个队列是否绑定到不同的CPU核心上，cat /proc/interrupts查询到每个队列的中断号，对应的文件/proc/irq/${IRQ_NUM}/smp_affinity为中断号IRQ_NUM绑定的CPU核的情况。以十六进制表示，每一位代表一个CPU核：</p>
<p>（00000001）代表CPU0</p>
<p>（00000010）代表CPU1</p>
<p>（00000011）代表CPU0和CPU1</p>
<p>SMP是指”对称多处理器”，smp_affinity文件主要用于某个特定IRQ要绑定到哪个CPU核心上。在 /proc/irq/IRQ_NUMBER/目录下都有一个smp_affinity文件，例如，网卡的中断号是：</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/eth_interrupts.jpg" class="" title="image">

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/108/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000001</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/109/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000002</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/110/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000004</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/111/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000008</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/112/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000010</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/113/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000020</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/114/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000040</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/115/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000080</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/116/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000100</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/117/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000200</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/118/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000001</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/119/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000002</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/120/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000004</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/121/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000008</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/122/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000010</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/123/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000020</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/124/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000040</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/125/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000080</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/126/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000100</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/127/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000200</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/128/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000001</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/129/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000002</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/130/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000004</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/131/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,00000008</span><br><span class="line">[root@192-168-152-52 ~]# cat /proc/irq/132/smp_affinity</span><br><span class="line">0000,00000000,00000000,00000000,00000000,00000000,00000000,0003f03f</span><br></pre></td></tr></table></figure>

<p>上面说明网卡队列绑定在CPU0~CPU0上。我们可以通过手动改变smp_affinity文件中的值来将IRQ绑定到指定的CPU核心上，或者启用irqbalance服务来自动绑定IRQ到CPU核心上。</p>
<h3 id="IRQ-Balance"><a href="#IRQ-Balance" class="headerlink" title="IRQ Balance"></a>IRQ Balance</h3><p>Irqbalance是一个Linux的实用程序，它主要是用于分发中断请求到CPU核心上，有助于性能的提升。它的目的是寻求省电和性能优化之间的平衡。你可以使用yum进行安装（CentOS系统一般默认安装）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install irqbalance</span><br><span class="line">/etc/init.d/irqbalance start</span><br></pre></td></tr></table></figure>

<p>Irqbalance对于包含多个核心的系统来说是非常有用的，因为通常中断只被第一个CPU核心服务。</p>
<h3 id="手动绑定亲和性"><a href="#手动绑定亲和性" class="headerlink" title="手动绑定亲和性"></a>手动绑定亲和性</h3><p>● 动态监控CPU中断情况，观察中断变化</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">watch -d -n 1 cat /proc/interrupts</span><br></pre></td></tr></table></figure>

<p>● 查看网卡中断相关信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /proc/interrupts | grep -E “eth|CPU”</span><br></pre></td></tr></table></figure>

<p>● 网卡亲和性设置<br>修改proc/irq/irq_number/smp_affinity之前，先停掉irq自动调节服务，不然修改的值就会被覆盖。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/etc/init.d/irqbalance stop</span><br></pre></td></tr></table></figure>

<p>通过查看网卡中断相关信息，得到网卡中断为19</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cd /proc/irq/19</span><br><span class="line">[root@master 19]# cat smp_affinity</span><br><span class="line">00000000,00000000,00000000,00000001</span><br><span class="line">[root@master 19]# cat smp_affinity_list </span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<p>修改值，将19号中断绑定在cpu2上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master 19]# echo 4 &gt; smp_affinity</span><br><span class="line">[root@master 19]# cat smp_affinity</span><br><span class="line">00000000,00000000,00000000,00000004</span><br><span class="line">[root@master 19]# cat smp_affinity_list </span><br><span class="line">2</span><br></pre></td></tr></table></figure>

<p>如果是要将网卡中断绑定在cpu0和cpu2上怎么做了？请先参照上文中的CPU列表。cpu0和2的十六进制值分别为1,4。那么如果要同时绑定在cpu0和cpu2上，则十六进制值为5，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master 19]# echo 5 &gt; smp_affinity</span><br><span class="line">[root@master 19]# cat smp_affinity</span><br><span class="line">00000000,00000000,00000000,00000005</span><br><span class="line">[root@master 19]# cat smp_affinity_list </span><br><span class="line">0,2</span><br></pre></td></tr></table></figure>

<h3 id="taskset为系统进程PID设置CPU亲和性"><a href="#taskset为系统进程PID设置CPU亲和性" class="headerlink" title="taskset为系统进程PID设置CPU亲和性"></a>taskset为系统进程PID设置CPU亲和性</h3><p>查看某个进程的CPU亲和性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># taskset -p 30011</span><br><span class="line">pid 30011&#x27;s current affinity mask: ff</span><br></pre></td></tr></table></figure>

<p>设置某个进程的CPU亲和性</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># taskset -p 1 30011</span><br><span class="line">pid 30011&#x27;s current affinity mask: ff</span><br><span class="line">pid 30011&#x27;s new affinity mask: 1</span><br></pre></td></tr></table></figure>

<p>使用-c选项可以将一个进程对应到多个CPU上去</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># taskset -p -c 1,3 30011</span><br><span class="line">pid 30011&#x27;s current affinity list: 0</span><br><span class="line">pid 30011&#x27;s new affinity list: 1,3</span><br><span class="line"></span><br><span class="line"># taskset -p -c 1-7 30011</span><br><span class="line">pid 30011&#x27;s current affinity list: 1,3</span><br><span class="line">pid 30011&#x27;s new affinity list: 1-7</span><br></pre></td></tr></table></figure>

<h3 id="RPS-RFS"><a href="#RPS-RFS" class="headerlink" title="RPS/RFS"></a>RPS/RFS</h3><p>前面大量介绍了多队列网卡及中断绑定，但是在单网卡单队列的情况下要想负载做网卡软中断绑定怎么办呢？RPS/RFS就是为此而生的，RPS/RFS功能出现在Linux kernel 2.6.35中，由google的工程师提交的两个补丁，这两个补丁的出现主要功能是在单队列网卡的情况下，在系统层用模拟了多队列的情况，以便达到CPU的均衡。</p>
<p>RPS（Receive Packet Steering）主要是把软中断的负载均衡到各个cpu，简单来说，是网卡驱动对每个流生成一个hash标识，这个HASH值得计算可以通过四元组来计算（SIP，SPORT，DIP，DPORT），然后由中断处理的地方根据这个hash标识分配到相应的CPU上去，这样就可以比较充分的发挥多核的能力了。通俗点来说就是在软件层面模拟实现硬件的多队列网卡功能，如果网卡本身支持多队列功能的话RPS就不会有任何的作用。该功能主要针对单队列网卡多CPU环境，如网卡支持多队列则可使用SMP irq affinity直接绑定硬中断。</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/RPS.png" class="" title="image">

<p>由于RPS只是单纯把数据包均衡到不同的cpu，这个时候如果应用程序所在的cpu和软中断处理的cpu不是同一个，此时对于cpu cache的影响会很大，那么RFS（Receive flow steering）确保应用程序处理的cpu跟软中断处理的cpu是同一个，这样就充分利用cpu的cache，这两个补丁往往都是一起设置，来达到最好的优化效果, 主要是针对单队列网卡多CPU环境。</p>
<img src="/2018/11/23/%E7%BD%91%E5%8D%A1%E5%A4%9A%E9%98%9F%E5%88%97%E6%80%BB%E7%BB%93/RFS.png" class="" title="image">

<p>网卡软中断分发的软件解决方法RPS/RFS<br>RSS需要网卡硬件的支持，在使用不支持RSS的网卡时，为了充分利用多核cpu，centos6.1开始提供了RPS（Receive Packet Steering）和RFS（Receive Flow Steering）。<br>RPS使网卡可以把一个rx队列的软中断分发到多个cpu核上，从而达到负载均衡的目的。RFS是RPS的扩展，RPS只依靠hash来控制数据包，提供了好的负载平衡，但是它没有考虑应用程序的位置(注：这个位置是指程序在哪个cpu上执行)。RFS则考虑到了应用程序的位置。RFS的目标是通过指派应用线程正在运行的CPU来进行数据包处理，以此来增加数据缓存的命中率。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="http://www.ywnds.com/?p=4380">多队列网卡及网卡中断绑定阐述</a></p>
<p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/performance_tuning_guide/network-rss">RECEIVE-SIDE SCALING (RSS)</a></p>
<p><a href="https://blog.csdn.net/hankerzero/article/details/55093897">多队列网卡CPU中断均衡</a></p>
<p><a href="http://blog.sina.com.cn/s/blog_3d5517850100fnaj.html">[精] Linux内核数据包处理流程－数据包接收(2)</a></p>
<p><a href="https://tech.meituan.com/Redis_High_Concurrency_Optimization.html">Redis 高负载下的中断优化</a></p>
]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网卡</tag>
        <tag>多队列</tag>
      </tags>
  </entry>
</search>
